<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on TomMago</title>
    <link>https://tommago.com/posts/</link>
    <description>Recent content in Posts on TomMago</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 15 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://tommago.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data re-uploading</title>
      <link>https://tommago.com/posts/drc/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tommago.com/posts/drc/</guid>
      <description>An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically aproximate any function. When it comes to quantum machine learning, a similar statement can be made. Suprisingly a single qubit is sufficient, to perform classification of arbitrary data distributions.
Universial Approximation Theorem The Universial Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continous functions.</description>
    </item>
    
    <item>
      <title>Quantum Natural Gradient Descent</title>
      <link>https://tommago.com/posts/qng/</link>
      <pubDate>Sat, 27 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tommago.com/posts/qng/</guid>
      <description>When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).</description>
    </item>
    
    <item>
      <title>Quantum Autoencoder</title>
      <link>https://tommago.com/posts/qae/</link>
      <pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tommago.com/posts/qae/</guid>
      <description>In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analogue of a classical autoencoder equally aims to learn a smaller representation of data.
A naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure.</description>
    </item>
    
    <item>
      <title>Hello World! | Hello GSoC!</title>
      <link>https://tommago.com/posts/hello/</link>
      <pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://tommago.com/posts/hello/</guid>
      <description>This year I&amp;rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,&amp;#32;Nachman &amp;amp; al.,&amp;#32;2021 Kasieczka,&amp;#32; G.,&amp;#32; Nachman,&amp;#32; B.,&amp;#32; Shih,&amp;#32; D.,&amp;#32; Amram,&amp;#32; O.,&amp;#32; Andreassen,&amp;#32; A.,&amp;#32; Benkendorfer,&amp;#32; K.,&amp;#32; Bortolato,&amp;#32; B.,&amp;#32; Brooijmans,&amp;#32; G.,&amp;#32; Canelli,&amp;#32; F.,&amp;#32; Collins,&amp;#32; J.</description>
    </item>
    
  </channel>
</rss>
