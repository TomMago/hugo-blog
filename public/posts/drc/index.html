<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Data re-uploading | TomMago</title>
<meta name="keywords" content="GSoC">
<meta name="description" content="An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.
Universal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions.">
<meta name="author" content="Tom Magorsch">
<link rel="canonical" href="https://tommago.com/posts/drc/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.18c187d9a47d1bb26b9343ff3062c17d3ec3fafc4f83bd2fd8c235ccdb100f6d.css" integrity="sha256-GMGH2aR9G7Jrk0P/MGLBfT7D&#43;vxPg70v2MI1zNsQD20=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://tommago.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tommago.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tommago.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tommago.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://tommago.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tommago.com/posts/drc/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js" integrity="sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
onload="renderMathInElement(document.body,
        {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
              ],
              throwOnError : false
          });"></script>



<link rel="stylesheet" type="text/css" href="/hugo-cite.css" />

  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-9MSJDZKGWH"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-9MSJDZKGWH');
        }
      </script>
    
  

<meta property="og:title" content="Data re-uploading" />
<meta property="og:description" content="An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.
Universal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tommago.com/posts/drc/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-15T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-09-15T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Data re-uploading"/>
<meta name="twitter:description" content="An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.
Universal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://tommago.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Data re-uploading",
      "item": "https://tommago.com/posts/drc/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Data re-uploading",
  "name": "Data re-uploading",
  "description": "An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.\nUniversal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions.",
  "keywords": [
    "GSoC"
  ],
  "articleBody": "An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.\nUniversal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions. Considering a classification problem, we might have functions $f: I_m \\to \\Reals$, where $I_m = [0,1]^m$. The output of a neural network with a single hidden layer may be written as $$h(\\vec{x}) = \\sum^{N}_{i=1}\\alpha_i\\sigma(\\vec{w}_i\\vec{x} + b_i), \\tag{1}$$ where $\\vec{w}_i$ and $b_i$ are the weights and biases of the hidden layer and $\\alpha_i$ the weights of the output layer. The function $\\sigma$ is the non-linear activation function. The function $h$ being dense in the continuous functions $f$ means, that for every $\\epsilon$ we can choose the parameters in Eq. $(1)$ so that $$|h(\\vec{x}) - f(\\vec{x})| \u003c \\epsilon \\ \\ \\text{for all} \\ \\ \\vec{x}.$$ This is a very powerful statement and enables neural networks to tackle very complex problems.\nA proof for this theorem if $\\sigma$ is a sigmoidal function, which means $\\lim_{x\\to\\infty}\\sigma(x)=1$ and $\\lim_{x\\to -\\infty}\\sigma(x)=0$, can be found in ( Citation: Cybenko, 1989 Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4). 303–314. https://doi.org/10.1007/BF02551274 ) . The proof basically works by contradiction: If the space of all functions $h$ denoted by $S$ is not all of the continuous functions $f: I_m \\to \\Reals$ denoted by $C(I_m)$, then there is a linear functional $L: C(I_m) \\to \\Reals$ with $L(S)=0$ (or more accurately the closure of $S$). The functional can be written as integral over a function $h$ with respect to some measure $\\mu$. Since $L(S)=0$, which follows from the Hanh-Banach theorem, the integral over our neural network function $h$ with respect to the measure $\\mu$ would vanish. However one can show that for a sigmoidal function $\\sigma$ integrals over terms of the form Eq. $(1)$ are non-zero for all non-zero measures, leading to the contradiction.\nThere are many variations of this theorem, especially ( Citation: Hornik, 1991 Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2). 251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T ) provides a proof dropping the sigmoidal requirement. This generalization applies to any nonconstant continuous activation function, which is bounded.\nUniversal Quantum Circuit Approximation In their paper ( Citation: Pérez-Salinas, Cervera-Lierta \u0026 al., 2019 Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E. \u0026 Latorre, J. (2019). Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) the authors show that a similar proof can be done for the approximation capabilities of a PQC with a single qubit. Let’s consider some data $\\vec{x}\\in\\Reals^n$ we want to classify. The data follows some classification function $f: \\Reals^n \\to O$ we want to approximate. In the simple case of binary classification, we might have $O=\\{0,1\\}$. The idea proposed in ( Citation: Pérez-Salinas, Cervera-Lierta \u0026 al., 2019 Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E. \u0026 Latorre, J. (2019). Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) is to subsequently apply parametrized gates with trainable weights and data uploads, effectively uploading the data many times. Hence the name data re-uploading. In their paper, the authors describe the motivation for the data re-uploading to be that classical neural networks effectively copy the input data when processing it. An example of a neural network with a single hidden layer is shown below.\nWhen e.g. passing the input data to the first hidden layer, the data is effectively passed to every unit of the hidden layer separately, thus “copied”. In Quantum Mechanics, however, there is the No-Cloning-Theorem, which states that there is no unitarity $U$ that clones arbitrary input states. This can be seen when assuming two states $\\ket{\\phi}$ and $\\ket{\\psi}$ which should be copied independently to a state $\\ket{c}$. The unitarity $U$ should therefore fulfill $$ \\begin{align*} \u0026\\ket{S_1} = U(\\ket{\\phi,c}) = \\ket{\\phi,\\phi},\\\\ \u0026\\ket{S_2} = U(\\ket{\\psi,c}) = \\ket{\\psi,\\psi}, \\end{align*} $$ in order to universally clone the input states. Here we denote $\\ket{\\phi}\\otimes\\ket{c} = \\ket{\\phi,c}$. The scalar product of the cloned states $\\ket{S_1}$ and $\\ket{S_2}$ can be written as $$ \\begin{align*} \\braket{S_1|S_2} =\u0026 \\bra{\\phi,c}U^\\dagger U \\ket{\\psi,c} = \\braket{\\phi,c|\\psi,c} = \\braket{\\phi,\\phi|\\psi,\\psi} \\\\ =\u0026 \\braket{\\phi|\\psi}\\braket{\\phi|\\psi} = \\braket{\\phi|\\psi}\\braket{k|k}. \\end{align*} $$ Since $\\braket{k|k}=1$ we have $$ \\begin{align*} \\braket{\\phi|\\psi}^2 = \\braket{\\phi|\\psi}, \\end{align*} $$ which is solved by $\\braket{\\phi|\\psi}=1$ and $\\braket{\\phi|\\psi}=0$. In the first case, the two states $\\phi$ and $\\psi$ are identical, which we don’t want, since we want to clone different states with the same unitarity. In the second case, the two states are orthogonal. The unitarity $U$ is therefore only able to clone orthogonal states. Non-orthogonal states can’t be copied without some information loss.\nTo mimic the copying of input data to hidden notes, as it happens in classical neural networks, the authors, therefore, propose to upload the input data multiple times to a single qubit. An example of a DRC is sketched below.\nA single gate can be understood as a single perceptron, with the unitarity as activation function.\nTo investigate the capabilities of this circuit, we consider a general unitary transformation $U(\\phi_1, \\phi_2, \\phi_3)\\in\\text{SU}(2)$. We can use this unitarity to upload data with $U(\\vec{x})$ or apply transformations with trainable parameters $\\vec{\\phi}$. In the case of data with only three features $\\vec{x}\\in\\Reals^3$, we would construct the data re-uploading circuit (DRC) with depth $N$ as $$\\ket{m} = U(\\vec{\\phi}_N)U(\\vec{x}) \\cdots U(\\vec{\\phi}_1)U(\\vec{x})\\ket{0}.$$ After applying the gates, the qubit can be measured to access the state which arises from the PQC. To reduce the number of gates and thus the depth of the circuit, we can incorporate the data upload and the parameters in a single gate. A single processing unit as analogy to a single unit in a neural network is then written as $$L_i = U(\\vec{b}_i + \\vec{w}_i\\odot\\vec{x}),$$ where $w$ are the weights, $b$ the biases and $\\odot$ denotes the elementwise Hadamard product. This already looks like the output of a single neuron! The classifier then becomes $$\\ket{m} = L_N \\cdots L_1\\ket{0}.$$\nData with an arbitrary number of features can be treated by padding the data with zeros, so that the number of features is a multiple of three and then uploading the three-dimensional feature vectors $\\vec{x}_j$ successively. In this case, a single processing unit $L_i$ is given as $$L_i = U(\\vec{b}^{(k)}_i + \\vec{w}^{(k)}_i \\odot \\vec{x}^{(k)}) \\cdots U(\\vec{b}^{(1)}_i + \\vec{w}^{(1)}_i \\odot \\vec{x}^{(1)}).$$\nTo see that this expression can approximate any function, we insert an explicit representation of $U(\\vec{\\phi})$ and summarize all transformations. For the general unitarity, we use $$U(\\vec{\\phi}) = \\mathrm{e}^{i\\phi_2\\sigma_z} \\mathrm{e}^{i\\phi_1\\sigma_y} \\mathrm{e}^{i\\phi_3\\sigma_z},$$ where we abbreviate $\\vec{\\phi} = \\vec{b} + \\vec{w}\\odot\\vec{x}$. This is equal to $$U(\\vec{\\phi}) = \\mathrm{e}^{i(w_1(\\vec{\\phi})\\sigma_x + w_2(\\vec{\\phi})\\sigma_y + w_3(\\vec{\\phi})\\sigma_z)},$$ with $$ \\begin{align} w_1(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_2(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\cos\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_3(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right), \\end{align} $$ where $d = \\arccos\\left(\\cos\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right)\\right)$.\nA DRC with $N$ processing units can now be written as $$\\mathcal{U} := \\prod^N_{j=1} \\mathrm{e}^{i(w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z)}.$$ Here the index of the $\\phi$ denotes the index of the different weights and biases. The product of Pauli-matrix exponentials can be simplified using the Baker-Campbell-Hausdorff formula $$\\mathcal{U} = \\exp\\left(i\\sum^N_{j_1}\\left[w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z\\right] + \\mathcal{O}_\\text{corr}\\right).$$\nThe correction term $\\mathcal{O}_\\text{corr}$ proportional to commutators of Pauli-matrices. The sum of the $w(\\vec{\\phi})$ terms can now be rewritten. Since all $w_i(\\vec{\\phi})$ are trigonometric functions, which are bounded to $[-1,1]$ and continuous, we can use the general version of the Universal Approximation Theorem and use the sum over $w_i(\\vec{b}_i + \\vec{w}_i \\odot \\vec{x})$ to approximate some continuous function $f_i(\\vec{x})$ just like in the classical case $$\\sum^N_{j=1}w_i(\\vec{b}_j + \\vec{w}_j \\odot \\vec{x}) = f_i(\\vec{x}).$$\nSince the correction terms are proportional to pauli matrices, ( Citation: Pérez-Salinas, Cervera-Lierta \u0026 al., 2019 Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E. \u0026 Latorre, J. (2019). Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) argues, that they can be absorbed into the functions $f(\\vec{x})$.\nBy optimizing the parameters with a classical optimization scheme, we can therefore approximate any function in terms of the final state (theoretically only with an infinite number of data re-uploads of course). To perform the optimization and classification, we can measure the qubit at the end of the circuit and compare the outcome with states which are predefined for the different classes. This way, it is possible to perform binary classification, but also multiclass problems can be treated by defining a label state for every class.\nThis approach can be extended to multi-qubit classifiers by entangling the different qubits. I think, the data re-uploading approach definitely looks quite promising, since it proves the capabilities of QML. In addition, it enables us to upload larger amounts of data on fewer qubits. Furthermore, it seems to improve the robustness against noise see e.g. ( Citation: Easom-Mccaldin, Bouridane \u0026 al., 2021 Easom-Mccaldin, P., Bouridane, A., Belatreche, A. \u0026 Jiang, R. (2021). On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access, 9. 65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 ) . Currently, I am using DRCs in my autoencoders. In the future, I aim to explore different entangle schemes for multi-qubit DRCs.\nEasom-Mccaldin, Bouridane, Belatreche \u0026 Jiang (2021) Easom-Mccaldin, P., Bouridane, A., Belatreche, A. \u0026 Jiang, R. (2021). On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access, 9. 65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 Cybenko (1989) Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4). 303–314. https://doi.org/10.1007/BF02551274 Hornik (1991) Hornik, K. (1991). Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2). 251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T Pérez-Salinas, Cervera-Lierta, Gil-Fuster \u0026 Latorre (2019) Pérez-Salinas, A., Cervera-Lierta, A., Gil-Fuster, E. \u0026 Latorre, J. (2019). Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ",
  "wordCount" : "1619",
  "inLanguage": "en",
  "datePublished": "2022-09-15T00:00:00Z",
  "dateModified": "2022-09-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Tom Magorsch"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tommago.com/posts/drc/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TomMago",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tommago.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tommago.com/" accesskey="h" title="TomMago (Alt + H)">TomMago</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://notes.tommago.com" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://tommago.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tommago.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tommago.com/">Home</a>&nbsp;»&nbsp;<a href="https://tommago.com/posts/">Posts</a></div>
    <h1 class="post-title">
      Data re-uploading
    </h1>
    <div class="post-meta"><span title='2022-09-15 00:00:00 +0000 UTC'>September 15, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Tom Magorsch

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#universal-approximation-theorem" aria-label="Universal Approximation Theorem">Universal Approximation Theorem</a></li>
                <li>
                    <a href="#universal-quantum-circuit-approximation" aria-label="Universal Quantum Circuit Approximation">Universal Quantum Circuit Approximation</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.</p>
<h1 id="universal-approximation-theorem">Universal Approximation Theorem<a hidden class="anchor" aria-hidden="true" href="#universal-approximation-theorem">#</a></h1>
<p>The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions.
Considering a classification problem, we might have functions $f: I_m \to \Reals$, where $I_m = [0,1]^m$. The output of a neural network with a single hidden layer may be written as
$$h(\vec{x}) = \sum^{N}_{i=1}\alpha_i\sigma(\vec{w}_i\vec{x} + b_i), \tag{1}$$
where $\vec{w}_i$ and $b_i$ are the weights and biases of the hidden layer and $\alpha_i$ the weights of the output layer.
The function $\sigma$ is the non-linear activation function.
The function $h$ being dense in the continuous functions $f$ means, that for every $\epsilon$ we can choose the parameters in Eq. $(1)$ so that
$$|h(\vec{x}) - f(\vec{x})| &lt; \epsilon \ \ \text{for all} \ \ \vec{x}.$$
This is a very powerful statement and enables neural networks to tackle very complex problems.</p>
<p>A proof for this theorem if $\sigma$ is a sigmoidal function, which means $\lim_{x\to\infty}\sigma(x)=1$ and $\lim_{x\to -\infty}\sigma(x)=0$, can be found in 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#cybenko1989"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="G."><span itemprop="familyName">Cybenko</span></span>,&#32;<span itemprop="datePublished">1989</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cybenko</span>,&#32;
    <meta itemprop="givenName" content="G." />
    G.</span>
  &#32;
    (<span itemprop="datePublished">1989</span>).
  &#32;<span itemprop="name">Approximation by superpositions of a sigmoidal function</span>.<i>
    <span itemprop="about">Mathematics of Control, Signals and Systems</span>,&#32;2(4)</i>.&#32;<span itemprop="pagination">303–314</span>.
  <a href="https://doi.org/10.1007/BF02551274"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.1007/BF02551274</a></span>




</span></span>)</span>
.
The proof basically works by contradiction: If the space of all functions $h$ denoted by $S$ is not all of the continuous functions $f: I_m \to \Reals$ denoted by $C(I_m)$, then there is a linear functional $L: C(I_m) \to \Reals$ with $L(S)=0$ (or more accurately the closure of $S$).
The functional can be written as integral over a function $h$ with respect to some measure $\mu$. Since $L(S)=0$, which follows from the Hanh-Banach theorem, the integral over our neural network function $h$ with respect to the measure $\mu$ would vanish.
However one can show that for a sigmoidal function $\sigma$ integrals over terms of the form Eq. $(1)$ are non-zero for all non-zero measures, leading to the contradiction.</p>
<p>There are many variations of this theorem, especially 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#hornik1991"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Kurt"><span itemprop="familyName">Hornik</span></span>,&#32;<span itemprop="datePublished">1991</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hornik</span>,&#32;
    <meta itemprop="givenName" content="Kurt" />
    K.</span>
  &#32;
    (<span itemprop="datePublished">1991</span>).
  &#32;<span itemprop="name">Approximation capabilities of multilayer feedforward networks</span>.<i>
    <span itemprop="about">Neural Networks</span>,&#32;4(2)</i>.&#32;<span itemprop="pagination">251–257</span>.
  <a href="https://doi.org/https://doi.org/10.1016/0893-6080%2891%2990009-T"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T</a></span>




</span></span>)</span>
 provides a proof dropping the sigmoidal requirement.
This generalization applies to any nonconstant continuous activation function, which is bounded.</p>
<h1 id="universal-quantum-circuit-approximation">Universal Quantum Circuit Approximation<a hidden class="anchor" aria-hidden="true" href="#universal-quantum-circuit-approximation">#</a></h1>
<p>In their paper 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#perezsalinas2019"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Adrián"><span itemprop="familyName">Pérez-Salinas</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Alba"><span itemprop="familyName">Cervera-Lierta</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2019</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pérez-Salinas</span>,&#32;
    <meta itemprop="givenName" content="Adrián" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cervera-Lierta</span>,&#32;
    <meta itemprop="givenName" content="Alba" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gil-Fuster</span>,&#32;
    <meta itemprop="givenName" content="Elies" />
    E.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Latorre</span>,&#32;
    <meta itemprop="givenName" content="José I." />
    J.</span>
  &#32;
    (<span itemprop="datePublished">2019</span>).
  &#32;<span itemprop="name">Data re-uploading for a universal quantum classifier</span>.<i>
    <span itemprop="about">Quantum 4, 226 (2020)</span></i>.
  <a href="https://doi.org/10.22331/q-2020-02-06-226"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.22331/q-2020-02-06-226</a></span>




</span></span>)</span>
 the authors show that a similar proof can be done for the approximation capabilities of a PQC with a single qubit.
Let&rsquo;s consider some data $\vec{x}\in\Reals^n$ we want to classify. The data follows some classification function $f: \Reals^n \to O$ we want to approximate. In the simple case of binary classification, we might have $O=\{0,1\}$.
The idea proposed in 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#perezsalinas2019"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Adrián"><span itemprop="familyName">Pérez-Salinas</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Alba"><span itemprop="familyName">Cervera-Lierta</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2019</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pérez-Salinas</span>,&#32;
    <meta itemprop="givenName" content="Adrián" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cervera-Lierta</span>,&#32;
    <meta itemprop="givenName" content="Alba" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gil-Fuster</span>,&#32;
    <meta itemprop="givenName" content="Elies" />
    E.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Latorre</span>,&#32;
    <meta itemprop="givenName" content="José I." />
    J.</span>
  &#32;
    (<span itemprop="datePublished">2019</span>).
  &#32;<span itemprop="name">Data re-uploading for a universal quantum classifier</span>.<i>
    <span itemprop="about">Quantum 4, 226 (2020)</span></i>.
  <a href="https://doi.org/10.22331/q-2020-02-06-226"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.22331/q-2020-02-06-226</a></span>




</span></span>)</span>
 is to subsequently apply parametrized gates with trainable weights and data uploads, effectively uploading the data many times. Hence the name data re-uploading. In their paper, the authors describe the motivation for the data re-uploading to be that classical neural networks effectively copy the input data when processing it. An example of a neural network with a single hidden layer is shown below.</p>
<p><img loading="lazy" src="../drc_3.png#center" alt="Neural network with one hidden layer"  />
</p>
<p>When e.g. passing the input data to the first hidden layer, the data is effectively passed to every unit of the hidden layer separately, thus &ldquo;copied&rdquo;. In Quantum Mechanics, however, there is the No-Cloning-Theorem, which states that there is no unitarity $U$ that clones arbitrary input states.
This can be seen when assuming two states $\ket{\phi}$ and $\ket{\psi}$ which should be copied independently to a state $\ket{c}$.
The unitarity $U$ should therefore fulfill
$$
\begin{align*}
&amp;\ket{S_1} = U(\ket{\phi,c}) = \ket{\phi,\phi},\\
&amp;\ket{S_2} = U(\ket{\psi,c}) = \ket{\psi,\psi},
\end{align*}
$$
in order to universally clone the input states. Here we denote $\ket{\phi}\otimes\ket{c} = \ket{\phi,c}$.
The scalar product of the cloned states $\ket{S_1}$ and $\ket{S_2}$ can be written as
$$
\begin{align*}
\braket{S_1|S_2} =&amp; \bra{\phi,c}U^\dagger U \ket{\psi,c} = \braket{\phi,c|\psi,c} = \braket{\phi,\phi|\psi,\psi} \\
=&amp; \braket{\phi|\psi}\braket{\phi|\psi} = \braket{\phi|\psi}\braket{k|k}.
\end{align*}
$$
Since $\braket{k|k}=1$ we have
$$
\begin{align*}
\braket{\phi|\psi}^2 = \braket{\phi|\psi},
\end{align*}
$$
which is solved by $\braket{\phi|\psi}=1$ and $\braket{\phi|\psi}=0$. In the first case, the two states $\phi$ and $\psi$ are identical, which we don&rsquo;t want, since we want to clone different states with the same unitarity. In the second case, the two states are orthogonal.
The unitarity $U$ is therefore only able to clone orthogonal states. Non-orthogonal states can&rsquo;t be copied without some information loss.</p>
<p>To mimic the copying of input data to hidden notes, as it happens in classical neural networks, the authors, therefore, propose to upload the input data multiple times to a single qubit. An example of a DRC is sketched below.</p>
<p><img loading="lazy" src="../drc_2.png#center" alt="DRC example"  />
</p>
<p>A single gate can be understood as a single perceptron, with the unitarity as activation function.</p>
<p>To investigate the capabilities of this circuit, we consider a general unitary transformation $U(\phi_1, \phi_2, \phi_3)\in\text{SU}(2)$. We can use this unitarity to upload data with $U(\vec{x})$ or apply transformations with trainable parameters $\vec{\phi}$.
In the case of data with only three features $\vec{x}\in\Reals^3$, we would construct the data re-uploading circuit (DRC) with depth $N$ as
$$\ket{m} = U(\vec{\phi}_N)U(\vec{x}) \cdots U(\vec{\phi}_1)U(\vec{x})\ket{0}.$$
After applying the gates, the qubit can be measured to access the state which arises from the PQC.
To reduce the number of gates and thus the depth of the circuit, we can incorporate the data upload and the parameters in a single gate.
A single processing unit as analogy to a single unit in a neural network is then written as
$$L_i = U(\vec{b}_i + \vec{w}_i\odot\vec{x}),$$
where $w$ are the weights, $b$ the biases and $\odot$ denotes the elementwise Hadamard product.
This already looks like the output of a single neuron!
The classifier then becomes
$$\ket{m} = L_N \cdots L_1\ket{0}.$$</p>
<p>Data with an arbitrary number of features can be treated by padding the data with zeros, so that the number of features is a multiple of three and then uploading the three-dimensional feature vectors $\vec{x}_j$ successively. In this case, a single processing unit $L_i$ is given as
$$L_i = U(\vec{b}^{(k)}_i + \vec{w}^{(k)}_i \odot \vec{x}^{(k)}) \cdots U(\vec{b}^{(1)}_i + \vec{w}^{(1)}_i \odot \vec{x}^{(1)}).$$</p>
<p>To see that this expression can approximate any function, we insert an explicit representation of $U(\vec{\phi})$ and summarize all transformations. For the general unitarity, we use
$$U(\vec{\phi}) = \mathrm{e}^{i\phi_2\sigma_z} \mathrm{e}^{i\phi_1\sigma_y} \mathrm{e}^{i\phi_3\sigma_z},$$
where we abbreviate $\vec{\phi} = \vec{b} + \vec{w}\odot\vec{x}$.
This is equal to
$$U(\vec{\phi}) = \mathrm{e}^{i(w_1(\vec{\phi})\sigma_x + w_2(\vec{\phi})\sigma_y + w_3(\vec{\phi})\sigma_z)},$$
with
$$
\begin{align}
w_1(\vec{\phi}) = \frac{d}{\sqrt{1 - \cos^2d}} \sin\left(\frac{\phi_2 - \phi_3}{2}\right)\sin\left(\frac{\phi_1}{2}\right),\\
w_2(\vec{\phi}) = \frac{d}{\sqrt{1 - \cos^2d}} \cos\left(\frac{\phi_2 - \phi_3}{2}\right)\sin\left(\frac{\phi_1}{2}\right),\\
w_3(\vec{\phi}) = \frac{d}{\sqrt{1 - \cos^2d}} \sin\left(\frac{\phi_2 + \phi_3}{2}\right)\cos\left(\frac{\phi_1}{2}\right),
\end{align}
$$
where $d = \arccos\left(\cos\left(\frac{\phi_2 + \phi_3}{2}\right)\cos\left(\frac{\phi_1}{2}\right)\right)$.</p>
<p>A DRC with $N$ processing units can now be written as
$$\mathcal{U} := \prod^N_{j=1} \mathrm{e}^{i(w_1(\vec{\phi}_j)\sigma_x + w_2(\vec{\phi}_j)\sigma_y + w_3(\vec{\phi}_j)\sigma_z)}.$$
Here the index of the $\phi$ denotes the index of the different weights and biases.
The product of Pauli-matrix exponentials can be simplified using the Baker-Campbell-Hausdorff formula
$$\mathcal{U} = \exp\left(i\sum^N_{j_1}\left[w_1(\vec{\phi}_j)\sigma_x + w_2(\vec{\phi}_j)\sigma_y + w_3(\vec{\phi}_j)\sigma_z\right] + \mathcal{O}_\text{corr}\right).$$</p>
<p>The correction term $\mathcal{O}_\text{corr}$ proportional to commutators of Pauli-matrices.
The sum of the $w(\vec{\phi})$ terms can now be rewritten. Since all $w_i(\vec{\phi})$ are trigonometric functions, which are bounded to $[-1,1]$ and continuous, we can use the general version of the Universal Approximation Theorem and use the sum over $w_i(\vec{b}_i + \vec{w}_i \odot \vec{x})$ to approximate some continuous function $f_i(\vec{x})$ just like in the classical case
$$\sum^N_{j=1}w_i(\vec{b}_j + \vec{w}_j \odot \vec{x}) = f_i(\vec{x}).$$</p>
<p>Since the correction terms are proportional to pauli matrices, 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#perezsalinas2019"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Adrián"><span itemprop="familyName">Pérez-Salinas</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Alba"><span itemprop="familyName">Cervera-Lierta</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2019</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pérez-Salinas</span>,&#32;
    <meta itemprop="givenName" content="Adrián" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cervera-Lierta</span>,&#32;
    <meta itemprop="givenName" content="Alba" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gil-Fuster</span>,&#32;
    <meta itemprop="givenName" content="Elies" />
    E.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Latorre</span>,&#32;
    <meta itemprop="givenName" content="José I." />
    J.</span>
  &#32;
    (<span itemprop="datePublished">2019</span>).
  &#32;<span itemprop="name">Data re-uploading for a universal quantum classifier</span>.<i>
    <span itemprop="about">Quantum 4, 226 (2020)</span></i>.
  <a href="https://doi.org/10.22331/q-2020-02-06-226"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.22331/q-2020-02-06-226</a></span>




</span></span>)</span>
 argues, that they can be absorbed into the functions $f(\vec{x})$.</p>
<p>By optimizing the parameters with a classical optimization scheme, we can therefore approximate any function in terms of the final state (theoretically only with an infinite number of data re-uploads of course). To perform the optimization and classification, we can measure the qubit at the end of the circuit and compare the outcome with states which are predefined for the different classes. This way, it is possible to perform binary classification, but also multiclass problems can be treated by defining a label state for every class.</p>
<p>This approach can be extended to multi-qubit classifiers by entangling the different qubits.
I think, the data re-uploading approach definitely looks quite promising, since it proves the capabilities of QML.
In addition, it enables us to upload larger amounts of data on fewer qubits.
Furthermore, it seems to improve the robustness against noise see e.g. 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#9415627"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Philip"><span itemprop="familyName">Easom-Mccaldin</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Ahmed"><span itemprop="familyName">Bouridane</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2021</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Easom-Mccaldin</span>,&#32;
    <meta itemprop="givenName" content="Philip" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bouridane</span>,&#32;
    <meta itemprop="givenName" content="Ahmed" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Belatreche</span>,&#32;
    <meta itemprop="givenName" content="Ammar" />
    A.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jiang</span>,&#32;
    <meta itemprop="givenName" content="Richard" />
    R.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier</span>.<i>
    <span itemprop="about">IEEE Access</span>,&#32;9</i>.&#32;<span itemprop="pagination">65127–65139</span>.
  <a href="https://doi.org/10.1109/ACCESS.2021.3075492"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.1109/ACCESS.2021.3075492</a></span>




</span></span>)</span>
.
Currently, I am using DRCs in my autoencoders. In the future, I aim to explore different entangle schemes for multi-qubit DRCs.</p>

  

  










<section class="hugo-cite-bibliography">
  <dl>
    

      <div id="9415627">
        <dt>
          Easom-Mccaldin,&#32;
          Bouridane,&#32;
          Belatreche&#32;&amp;&#32;Jiang

          
          (2021)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Easom-Mccaldin</span>,&#32;
    <meta itemprop="givenName" content="Philip" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Bouridane</span>,&#32;
    <meta itemprop="givenName" content="Ahmed" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Belatreche</span>,&#32;
    <meta itemprop="givenName" content="Ammar" />
    A.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Jiang</span>,&#32;
    <meta itemprop="givenName" content="Richard" />
    R.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier</span>.<i>
    <span itemprop="about">IEEE Access</span>,&#32;9</i>.&#32;<span itemprop="pagination">65127–65139</span>.
  <a href="https://doi.org/10.1109/ACCESS.2021.3075492"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.1109/ACCESS.2021.3075492</a></span>




</dd>

      </div>

      <div id="cybenko1989">
        <dt>
          Cybenko

          
          (1989)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cybenko</span>,&#32;
    <meta itemprop="givenName" content="G." />
    G.</span>
  &#32;
    (<span itemprop="datePublished">1989</span>).
  &#32;<span itemprop="name">Approximation by superpositions of a sigmoidal function</span>.<i>
    <span itemprop="about">Mathematics of Control, Signals and Systems</span>,&#32;2(4)</i>.&#32;<span itemprop="pagination">303–314</span>.
  <a href="https://doi.org/10.1007/BF02551274"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.1007/BF02551274</a></span>




</dd>

      </div>

      <div id="hornik1991">
        <dt>
          Hornik

          
          (1991)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hornik</span>,&#32;
    <meta itemprop="givenName" content="Kurt" />
    K.</span>
  &#32;
    (<span itemprop="datePublished">1991</span>).
  &#32;<span itemprop="name">Approximation capabilities of multilayer feedforward networks</span>.<i>
    <span itemprop="about">Neural Networks</span>,&#32;4(2)</i>.&#32;<span itemprop="pagination">251–257</span>.
  <a href="https://doi.org/https://doi.org/10.1016/0893-6080%2891%2990009-T"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T</a></span>




</dd>

      </div>

      <div id="perezsalinas2019">
        <dt>
          Pérez-Salinas,&#32;
          Cervera-Lierta,&#32;
          Gil-Fuster&#32;&amp;&#32;Latorre

          
          (2019)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Pérez-Salinas</span>,&#32;
    <meta itemprop="givenName" content="Adrián" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Cervera-Lierta</span>,&#32;
    <meta itemprop="givenName" content="Alba" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Gil-Fuster</span>,&#32;
    <meta itemprop="givenName" content="Elies" />
    E.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Latorre</span>,&#32;
    <meta itemprop="givenName" content="José I." />
    J.</span>
  &#32;
    (<span itemprop="datePublished">2019</span>).
  &#32;<span itemprop="name">Data re-uploading for a universal quantum classifier</span>.<i>
    <span itemprop="about">Quantum 4, 226 (2020)</span></i>.
  <a href="https://doi.org/10.22331/q-2020-02-06-226"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.22331/q-2020-02-06-226</a></span>




</dd>

      </div>
  </dl>
</section>





  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tommago.com/tags/gsoc/">GSoC</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://tommago.com/">TomMago</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
