<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC | TomMago</title>
<meta name="keywords" content="GSoC">
<meta name="description" content="This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data.
The full code for all my work can be found on Github.
In the following post I will outline my work and describe some parts of the implementation and the results
Event generation in HEP experiments
In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics.


Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. 




(No matching key was found for `Oliveira2017` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Butter2019` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Hariri2021` in the references. Please make sure to provide an available ID in your `bib.json` file.)
. The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from.">
<meta name="author" content="Tom Magorsch">
<link rel="canonical" href="https://tommago.com/posts/gsoc23/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2215a03600b60f1af3cfb8faa798da315a17443436f4572cbbbec618c5735bd8.css" integrity="sha256-IhWgNgC2Dxrzz7j6p5jaMVoXRDQ29Fcsu77GGMVzW9g=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://tommago.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://tommago.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://tommago.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://tommago.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://tommago.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://tommago.com/posts/gsoc23/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js" integrity="sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
onload="renderMathInElement(document.body,
        {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '$', right: '$', display: false},
              ],
              throwOnError : false
          });"></script>



<link rel="stylesheet" type="text/css" href="/hugo-cite.css" />

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-9MSJDZKGWH"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-9MSJDZKGWH');
        }
      </script><meta property="og:title" content="GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC" />
<meta property="og:description" content="This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data.
The full code for all my work can be found on Github.
In the following post I will outline my work and describe some parts of the implementation and the results
Event generation in HEP experiments
In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics.


Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. 




(No matching key was found for `Oliveira2017` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Butter2019` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Hariri2021` in the references. Please make sure to provide an available ID in your `bib.json` file.)
. The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tommago.com/posts/gsoc23/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-12T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-10-12T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC"/>
<meta name="twitter:description" content="This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data.
The full code for all my work can be found on Github.
In the following post I will outline my work and describe some parts of the implementation and the results
Event generation in HEP experiments
In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics.


Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. 




(No matching key was found for `Oliveira2017` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Butter2019` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Hariri2021` in the references. Please make sure to provide an available ID in your `bib.json` file.)
. The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://tommago.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC",
      "item": "https://tommago.com/posts/gsoc23/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC",
  "name": "GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC",
  "description": "This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data. The full code for all my work can be found on Github. In the following post I will outline my work and describe some parts of the implementation and the results\nEvent generation in HEP experiments In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics. Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. (No matching key was found for `Oliveira2017` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Butter2019` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Hariri2021` in the references. Please make sure to provide an available ID in your `bib.json` file.) . The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from.\n",
  "keywords": [
    "GSoC"
  ],
  "articleBody": "This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data. The full code for all my work can be found on Github. In the following post I will outline my work and describe some parts of the implementation and the results\nEvent generation in HEP experiments In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics. Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. (No matching key was found for `Oliveira2017` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Butter2019` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Hariri2021` in the references. Please make sure to provide an available ID in your `bib.json` file.) . The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from.\nClassical GANs Generative Adversarial Networks (GANs) are a class of unsupervised machine learning models proposed in (No matching key was found for `Goodfellow2014` in the references. Please make sure to provide an available ID in your `bib.json` file.) . GANs aim to train a generator $G(z,\\Theta_g)$ with a latent space $z$ and parameters $\\Theta_g$ to replicate a reference probability distribution when sampling from the latent space $z$.\nA GAN consists of two networks, the generator $G$ and a discriminator $D$. The networks are trained by playing a zero sum game, where the generator tries to generate samples which are as realistic as possible, while the discriminator tries to classify real data samples and tag samples generated by the generator as fake.\nA more detailed description of classical GANs is given in this post.\nA Quantum GAN for event generation There have been several different proposals for Quantum GANs (No matching key was found for `Lloyd2018` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `DallaireDemers2018` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Niu2021` in the references. Please make sure to provide an available ID in your `bib.json` file.) including proposals for applications in HEP simulation (No matching key was found for `Rehm2023` in the references. Please make sure to provide an available ID in your `bib.json` file.No matching key was found for `Rehm2023a` in the references. Please make sure to provide an available ID in your `bib.json` file.) .\nIn this project I focused on a Hybrid quantum classical model based on (No matching key was found for `Zoufal2019` in the references. Please make sure to provide an available ID in your `bib.json` file.) . It consists of a quantum circuit acting as generator and a classical discriminator, which recives measured samples as input. Notably, this approach differs from many other approches in the literature, as it does not try to embed the continous data into quantum states through an embedding. We rather try to systematically discretize the data and develop a mapping to basis states of quantum system. The generator can then learn to prepare a quantum state whose distribution of basis states when measured resembles the discretized data.\nAs a toy example, I take a dataset of detector images originating from Quark initiated Jets (No matching key was found for `Andrews2019` in the references. Please make sure to provide an available ID in your `bib.json` file.) and stepwise scale this down to the size of $3\\times 3$ with $M=2$ possible values per pixel. Below I show exemplary, how the average of all data samples scales down from a full $125\\times 125$ detector image.\nAs every possible state needs to be represented by a unique basis state, I represent every pixel with $\\log_2M$ qubits, where $M$ is the number of discrete values a single pixel can take. The total qubits needed to represent an image of size $N\\times N$ with $M$ discrete values per pixel is then $n = N^2\\log_2M$.\nFor our toy example we therefore have $n=9$ pixels. Of course this is a very simplified example, however it can work as a simple benchmark to test our model.\nSampling from the generator we will obtain a measured basis stat which is a list of $0$s and $1$s representing the measured value for every qubit.\nTo map such a basis state $\\ket{q_1q_2\\dots}, q_i = 0,1$ to an image and vice versa, we proceed as following:\nReshape the list $[q_1,q_2,\\dots]$ to $(N,N,\\log_2 M)$ Convert the third dimension from an binary list to an integer Divide by $2^M - 1$ to normalize These images can then be passed into a classical deep neural network of your choice, meaning it is possible to utilize convolutional or graph neural networks as discriminator, as has been done in jet phyics before. Since the generator is trained on adversarially against the discriminator, we expect the generator to be able to generalize, given that the discriminator generalizes well on the data.\nThe training of the hybrid QGAN proceeds as following, for every epoch:\nWe draw $N$ samples $s_i$ of basis states We evaluate the discriminator $D$ at theses samples giving $D(s_i)$ We get the probabilites for every sample beeing drawn by the generator $G$ as $p_i = G(s_i)$ We calculate the generator Loss $\\mathcal{L}_G=\\sum_i p_i \\log D(s_i)$ We perform a step updating the generator parameters using $\\mathcal{L}_G$ We evaluate the discriminator on a batch of real data samples $D(x_i)$ We calculate the discriminator loss as $\\mathcal{L}_D=\\frac{1}{N}\\sum^N_i\\log D(x_i) - \\sum_i p_i \\log D(s_i)$. We perform a step updating the discriminator parameters using $\\mathcal{L}_D$. In the discriminator loss $\\mathcal{L}_D$ the first term corresponds to the discriminator learning the real samples and the second term to learning to spot the fake samples generated by the generator.\nI implemented this learning procedure using pennylane and pytorch. For that, I use a qnode with the pennylane pytorch interface\ndev = qml.device(\"default.qubit.torch\", wires=num_qubits) @qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\", cachesize=1000000) def circuit(inputs, weights): for wire in range(num_qubits): qml.Hadamard(wires=wire) qml.StronglyEntanglingLayers(weights=weights, wires=list(range(num_qubits))) return qml.probs() In this example, I return as measurement the probabilites of the basis states to use for the generator loss function. Note that I have to increase the cachesize for larger ciruits. As a unitarity in this example I use pennylane StronglyEntanglingLayer, though a more hardware efficient ansatz would be advantageous for execution on real hardware.\nTo perform the hybrid training with pytorch, we have to convert the qnode to a torch layer\nweight_shapes = {\"weights\": (n_layers, num_qubits,3)} qlayer = qml.qnn.TorchLayer(circuit, weight_shapes) As discriminator we can build a feed forward neural network with the pixel number as input size.\nclass Discriminator(nn.Module): def __init__(self, input_size): super(Discriminator, self).__init__() self.linear_input = nn.Linear(input_size, 50) self.leaky_relu = nn.LeakyReLU(0.2) self.linear1 = nn.Linear(50, 20) self.linear2 = nn.Linear(20, 1) self.sigmoid = nn.Sigmoid() self.flatten = nn.Flatten() def forward(self, inputs): x = self.flatten(inputs) x = self.linear_input(x) x = self.leaky_relu(x) x = self.linear1(x) x = self.leaky_relu(x) x = self.linear2(x) x = self.sigmoid(x) return x generator = qlayer discriminator = Discriminator(N**2) We can now optimize the parameters of these two models using pytorchs standard optimizers and the loss functions described above.\nTraining results The training results of the Qgan are shown below\nWe can see, that the KL divergence and the MSE between the average generated and data image decrease and converge in a controlled manner. Therefore, the learned generator can generate the data distribution fairly well. It is worthwile to note that especially, there is no mode collapse here, as we are, in contrast to many other QGAN proposals, sampling from a quantum state, therefore mode collapse would mean learning a unitarity, which creates exactly a basis state. This is very unlikely and especially would not lead to a minimum in the loss function.\nScaling and prospects I think this is an interesting model for generative tasks on classical data. Of course we need more qubits than alternative models that embed continous data, however since the discretization scales with $\\log_2 M$ in the long run this should not be the limiting factor. I tried applying this model to larger images, however on a classical simulator the requried qubits were too many to simulate reasonably.\nThe main thing I would like to understand now is what advantages Quantum assisted machine learning can bring over classical methods. My main motivation for this model was the complexity theory based argument that by measurement quantum computers can efficiently sample from distributions, which are hard to sample for classical algorithms. However, I would like to understand this point better, develop an insight to what kind of data distribtuions this applies and especially make sure that the classical discriminator is able to learn these distribtuions.\nReferences Bibliography called, but no references ",
  "wordCount" : "1522",
  "inLanguage": "en",
  "datePublished": "2023-10-12T00:00:00Z",
  "dateModified": "2023-10-12T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Tom Magorsch"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tommago.com/posts/gsoc23/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "TomMago",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tommago.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://tommago.com/" accesskey="h" title="TomMago (Alt + H)">TomMago</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://tommago.com/talks/" title="Talks">
                    <span>Talks</span>
                </a>
            </li>
            <li>
                <a href="https://notes.tommago.com" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://tommago.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://tommago.com/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://tommago.com/">Home</a>&nbsp;»&nbsp;<a href="https://tommago.com/posts/">Posts</a></div>
    <h1 class="post-title">
      GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC
    </h1>
    <div class="post-meta"><span title='2023-10-12 00:00:00 +0000 UTC'>October 12, 2023</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Tom Magorsch

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul><ul>
                <li>
                    <a href="#event-generation-in-hep-experiments" aria-label="Event generation in HEP experiments">Event generation in HEP experiments</a></li>
                <li>
                    <a href="#classical-gans" aria-label="Classical GANs">Classical GANs</a></li>
                <li>
                    <a href="#a-quantum-gan-for-event-generation" aria-label="A Quantum GAN for event generation">A Quantum GAN for event generation</a></li>
                <li>
                    <a href="#training-results" aria-label="Training results">Training results</a></li>
                <li>
                    <a href="#scaling-and-prospects" aria-label="Scaling and prospects">Scaling and prospects</a></li></ul>
                    </ul>
                    
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This is a summary of my 2023 GSoC <a href="https://summerofcode.withgoogle.com/programs/2023/projects/ggoiGDQ5">project</a> with the <a href="https://ml4sci.org">ML4SCI</a>-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data.
The full code for all my work can be found on <a href="https://github.com/ML4SCI/QMLHEP/tree/main/Quantum_GAN_for_HEP_Tom_Magorsch">Github</a>.
In the following post I will outline my work and describe some parts of the implementation and the results</p>
<h3 id="event-generation-in-hep-experiments">Event generation in HEP experiments<a hidden class="anchor" aria-hidden="true" href="#event-generation-in-hep-experiments">#</a></h3>
<p>In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics.
<img loading="lazy" src="../analysis_pipeline.png#center" alt="HEP experiment pipeline"  />

Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. 




<span class="hugo-cite-intext"
        itemprop="citation">(<span style="background-color: #f00; color: #fff;">No matching key was found for `Oliveira2017` in the references. Please make sure to provide an available ID in your `bib.json` file.</span><span style="background-color: #f00; color: #fff;">No matching key was found for `Butter2019` in the references. Please make sure to provide an available ID in your `bib.json` file.</span><span style="background-color: #f00; color: #fff;">No matching key was found for `Hariri2021` in the references. Please make sure to provide an available ID in your `bib.json` file.</span>)</span>
. The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from.</p>
<h3 id="classical-gans">Classical GANs<a hidden class="anchor" aria-hidden="true" href="#classical-gans">#</a></h3>
<p>Generative Adversarial Networks (GANs) are a class of unsupervised machine learning models proposed in 




<span class="hugo-cite-intext"
        itemprop="citation">(<span style="background-color: #f00; color: #fff;">No matching key was found for `Goodfellow2014` in the references. Please make sure to provide an available ID in your `bib.json` file.</span>)</span>
. GANs aim to train a generator $G(z,\Theta_g)$ with a latent space $z$ and parameters $\Theta_g$ to replicate a reference probability distribution when sampling from the latent space $z$.</p>
<p>A GAN consists of two networks, the generator $G$ and a discriminator $D$. The networks are trained by playing a zero sum game, where the generator tries to generate samples which are as realistic as possible, while the discriminator tries to classify real data samples and tag samples generated by the generator as fake.</p>
<p>A more detailed description of classical GANs is given in <a href="https://www.tommago.com/posts/qgan/">this post</a>.</p>
<h3 id="a-quantum-gan-for-event-generation">A Quantum GAN for event generation<a hidden class="anchor" aria-hidden="true" href="#a-quantum-gan-for-event-generation">#</a></h3>
<p>There have been several different proposals for Quantum GANs 




<span class="hugo-cite-intext"
        itemprop="citation">(<span style="background-color: #f00; color: #fff;">No matching key was found for `Lloyd2018` in the references. Please make sure to provide an available ID in your `bib.json` file.</span><span style="background-color: #f00; color: #fff;">No matching key was found for `DallaireDemers2018` in the references. Please make sure to provide an available ID in your `bib.json` file.</span><span style="background-color: #f00; color: #fff;">No matching key was found for `Niu2021` in the references. Please make sure to provide an available ID in your `bib.json` file.</span>)</span>
 including proposals for applications in HEP simulation 




<span class="hugo-cite-intext"
        itemprop="citation">(<span style="background-color: #f00; color: #fff;">No matching key was found for `Rehm2023` in the references. Please make sure to provide an available ID in your `bib.json` file.</span><span style="background-color: #f00; color: #fff;">No matching key was found for `Rehm2023a` in the references. Please make sure to provide an available ID in your `bib.json` file.</span>)</span>
.</p>
<p>In this project I focused on a Hybrid quantum classical model based on 




<span class="hugo-cite-intext"
        itemprop="citation">(<span style="background-color: #f00; color: #fff;">No matching key was found for `Zoufal2019` in the references. Please make sure to provide an available ID in your `bib.json` file.</span>)</span>
. It consists of a quantum circuit acting as generator and a classical discriminator, which recives measured samples as input. Notably, this approach differs from many other approches in the literature, as it does not try to embed the continous data into quantum states through an embedding. We rather try to systematically discretize the data and develop a mapping to basis states of quantum system. The generator can then learn to prepare a quantum state whose distribution of basis states when measured resembles the discretized data.</p>
<p>As a toy example, I take a dataset of detector images originating from Quark initiated Jets 




<span class="hugo-cite-intext"
        itemprop="citation">(<span style="background-color: #f00; color: #fff;">No matching key was found for `Andrews2019` in the references. Please make sure to provide an available ID in your `bib.json` file.</span>)</span>
 and stepwise scale this down to the size of $3\times 3$ with $M=2$ possible values per pixel. Below I show exemplary, how the average of all data samples scales down from a full $125\times 125$ detector image.</p>
<p><img loading="lazy" src="../qg_scaling.png#center" alt="data scaling"  />
</p>
<p>As every possible state needs to be represented by a unique basis state, I represent every pixel with $\log_2M$ qubits, where $M$ is the number of discrete values a single pixel can take. The total qubits needed to represent an image of size $N\times N$ with $M$ discrete values per pixel is then $n = N^2\log_2M$.</p>
<p>For our toy example we therefore have $n=9$ pixels. Of course this is a very simplified example, however it can work as a simple benchmark to test our model.</p>
<p>Sampling from the generator we will obtain a measured basis stat which is a list of $0$s and $1$s representing the measured value for every qubit.</p>
<p>To map such a basis state $\ket{q_1q_2\dots}, q_i = 0,1$ to an image and vice versa, we proceed as following:</p>
<ol>
<li>Reshape the list $[q_1,q_2,\dots]$ to $(N,N,\log_2 M)$</li>
<li>Convert the third dimension from an binary list to an integer</li>
<li>Divide by $2^M - 1$ to normalize</li>
</ol>
<p>These images can then be passed into a classical deep neural network of your choice, meaning it is possible to utilize convolutional or graph neural networks as discriminator, as has been done in jet phyics before. Since the generator is trained on adversarially against the discriminator, we expect the generator to be able to generalize, given that the discriminator generalizes well on the data.</p>
<p>The training of the hybrid QGAN proceeds as following, for every epoch:</p>
<ol>
<li>We draw $N$ samples $s_i$ of basis states</li>
<li>We evaluate the discriminator $D$ at theses samples giving $D(s_i)$</li>
<li>We get the probabilites for every sample beeing drawn by the generator $G$ as $p_i = G(s_i)$</li>
<li>We calculate the generator Loss $\mathcal{L}_G=\sum_i p_i \log D(s_i)$</li>
<li>We perform a step updating the generator parameters using $\mathcal{L}_G$</li>
<li>We evaluate the discriminator on a batch of real data samples $D(x_i)$</li>
<li>We calculate the discriminator loss as $\mathcal{L}_D=\frac{1}{N}\sum^N_i\log D(x_i) - \sum_i p_i \log D(s_i)$.</li>
<li>We perform a step updating the discriminator parameters using $\mathcal{L}_D$.</li>
</ol>
<p>In the discriminator loss $\mathcal{L}_D$ the first term corresponds to the discriminator learning the real samples and the second term to learning to spot the fake samples generated by the generator.</p>
<p>I implemented this learning procedure using <a href="https://pennylane.ai/">pennylane</a> and <a href="https://pytorch.org/">pytorch</a>. For that, I use a <code>qnode</code> with the pennylane pytorch interface</p>
<pre tabindex="0"><code>dev = qml.device(&#34;default.qubit.torch&#34;, wires=num_qubits)

@qml.qnode(dev, interface=&#34;torch&#34;, diff_method=&#34;backprop&#34;, cachesize=1000000)
def circuit(inputs, weights):
    for wire in range(num_qubits): qml.Hadamard(wires=wire)
    qml.StronglyEntanglingLayers(weights=weights, wires=list(range(num_qubits)))
    return qml.probs()
</code></pre><p>In this example, I return as measurement the probabilites of the basis states to use for the generator loss function. Note that I have to increase the <code>cachesize</code> for larger ciruits. As a unitarity in this example I use pennylane <code>StronglyEntanglingLayer</code>, though a more hardware efficient ansatz would be advantageous for execution on real hardware.</p>
<p>To perform the hybrid training with pytorch, we have to convert the <code>qnode</code> to a torch layer</p>
<pre tabindex="0"><code>weight_shapes = {&#34;weights&#34;: (n_layers, num_qubits,3)}
qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)
</code></pre><p>As discriminator we can build a feed forward neural network with the pixel number as input size.</p>
<pre tabindex="0"><code>class Discriminator(nn.Module):
    def __init__(self, input_size):
        super(Discriminator, self).__init__()

        self.linear_input = nn.Linear(input_size, 50)
        self.leaky_relu = nn.LeakyReLU(0.2)
        self.linear1 = nn.Linear(50, 20)
        self.linear2 = nn.Linear(20, 1)
        self.sigmoid = nn.Sigmoid()
        self.flatten = nn.Flatten()

    def forward(self, inputs):
        x = self.flatten(inputs)
        x = self.linear_input(x)
        x = self.leaky_relu(x)
        x = self.linear1(x)
        x = self.leaky_relu(x)
        x = self.linear2(x)
        x = self.sigmoid(x)
        return x

generator = qlayer
discriminator = Discriminator(N**2)
</code></pre><p>We can now optimize the parameters of these two models using pytorchs standard optimizers and the loss functions described above.</p>
<h3 id="training-results">Training results<a hidden class="anchor" aria-hidden="true" href="#training-results">#</a></h3>
<p>The training results of the Qgan are shown below</p>
<p><img loading="lazy" src="../qgan_training.png#center" alt="training progress of the qgan"  />
</p>
<p>We can see, that the KL divergence and the MSE between the average generated and data image decrease and converge in a controlled manner.
Therefore, the learned generator can generate the data distribution fairly well. It is worthwile to note that especially, there is no mode collapse here, as we are, in contrast to many other QGAN proposals, sampling from a quantum state, therefore mode collapse would mean learning a unitarity, which creates exactly a basis state. This is very unlikely and especially would not lead to a minimum in the loss function.</p>
<h3 id="scaling-and-prospects">Scaling and prospects<a hidden class="anchor" aria-hidden="true" href="#scaling-and-prospects">#</a></h3>
<p>I think this is an interesting model for generative tasks on classical data. Of course we need more qubits than alternative models that embed continous data, however since the discretization scales with $\log_2 M$ in the long run this should not be the limiting factor.
I tried applying this model to larger images, however on a classical simulator the requried qubits were too many to simulate reasonably.</p>
<p>The main thing I would like to understand now is what advantages Quantum assisted machine learning can bring over classical methods. My main motivation for this model was the complexity theory based argument that by measurement quantum computers can efficiently sample from distributions, which are hard to sample for classical algorithms. However, I would like to understand this point better, develop an insight to what kind of data distribtuions this applies and especially make sure that the classical discriminator is able to learn these distribtuions.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>

  

  










Bibliography called, but no references





  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://tommago.com/tags/gsoc/">GSoC</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2026 <a href="https://tommago.com/">TomMago</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
