[{"content":"This is a summary of my 2022 GSoC project with ML4SCI. The ML4SCI organization accustoms different projects of machine learning applied to scientific problems, many connected to high-energy physics. A big thank you to Sergei Gleyzer for the supervision and support.\nAbstract The Standard Model of particle physics is a theory that describes the fundamental particles and the interactions between them. While it has extensively been tested and was able to correctly predict experiments to an impressive degree, there are multiple reasons to believe that it cannot be a complete description of nature. In the search for physics beyond the Standard Model (BSM), even though the large hadron collider (LHC) produced a large amount of data, no conclusive evidence of new physics could be found yet. A promising method to uncover new physics in the large amount of data is the use of anomaly detection techniques, which can be used to tag anomalous events. A well-known method of deep anomaly detection is the use of autoencoders, which have been applied to the task of anomaly tagging in HEP data before. In my project study the use of quantum machine learning models for the task of anomaly tagging, to investigate if quantum computers can enhance these analyses.\nThe project In order to apply to GSoC with ML4SCI you have to complete some preliminary screening tasks, and write a proposal for your project. Solving the tasks took some time, but they were very interesting and already prepared well for the upcoming project. You can see my proposal here, if you are interested. Of course, the project deviated a bit from the initial plan, but all in all, I followed the plan outlined in the proposal.\nAnomaly tagging With the large amounts of data produced by the LHC and potentially produced in the future by the HL-LHC, new analysis techniques pose interesting tools to detect new physics. I think the BSM physics is certainly hiding somewhere in the data, uncovering it is just a question of finding the needle in a haystack, due to its elusive nature. I especially like the idea of unsupervised techniques, since it is a way to conduct a model-independent search. Since there are many different BSM models I think model-independent searches make a lot of sense. There has previously been a good amount of research on the application of unsupervised methods to new physics searches, e.g. ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 ;\u0026#32; Citation: Fraser,\u0026#32;Homiller \u0026amp; al.,\u0026#32;2021 Fraser,\u0026#32; K.,\u0026#32; Homiller,\u0026#32; S.,\u0026#32; Mishra,\u0026#32; R.,\u0026#32; Ostdiek,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Schwartz,\u0026#32; M. \u0026#32; (2021). \u0026#32;Challenges for Unsupervised Anomaly Detection in Particle Physics. https://doi.org/10.1007/JHEP03(2022)066 ) . Many studies apply anomaly detection to detector images. Since Autoencoders are one of the most prominent deep anomaly detection models, they have been applied to these anomaly studies as well. I specifically want to highlight ( Citation: Finke,\u0026#32;Krämer \u0026amp; al.,\u0026#32;2021 Finke,\u0026#32; T.,\u0026#32; Krämer,\u0026#32; M.,\u0026#32; Morandini,\u0026#32; A.,\u0026#32; Mück,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Oleksiyuk,\u0026#32; I. \u0026#32; (2021). \u0026#32;Autoencoders for unsupervised anomaly detection in high energy physics. JHEP 06 (2021) 161. https://doi.org/10.1007/JHEP06(2021)161 ) . In their study, the authors use a convolutional autoencoder to tag top quark-initiated jets as anomalous, in samples of QCD-initiated jets. In particular, they highlight, that there is a complexity bias between the QCD and top samples. This refers to the fact, that a convolutional Autoencoder trained on top jets can not tag QCD jets, since the top jets have a higher intrinsic dimensionality, which enables the Autoencoder to work on the \u0026ldquo;easier to reconstruct\u0026rdquo; QCD samples. I think this can definitely be a problem as it sets constraints on the type of new physics we are able to uncover and thus it\u0026rsquo;s an interesting matter to investigate when working on this type of anomaly detection.\nDatasets In the project, I used different datasets, e.g. MNIST for validating ideas and code samples or ECAL images of electrons and photons. However, my main focus was on a dataset of detector images of quark and gluon-initiated jets, which is described in ( Citation: Andrews,\u0026#32;Alison \u0026amp; al.,\u0026#32;2019 Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 ) . The dataset only contains jets of light quarks ($u$, $d$, $s$). The original dataset contains 125x125 images of the electromagnetic calorimeter, hadronic calorimeter and the tracks for every event respectively. Below I show the average images of the three channels for quarks and gluons respectively. Note that I normalized the images by dividing them by the value on the largest pixel respectively. Now I\u0026rsquo;m not completely sure if this is the best way to do it, but since I need the pixel values to be properly distributed in $[0,1]$ this was the most obvious way for me. The HCAL channel was generated at a lower resolution and upscaled to 125x125 resulting in a coarser pixeling than the other channels.\nWe can see, that the gluon-initiated jets show a slightly wider cone, however, the differences are quite small, which makes the differentiation of these jets a very hard task. It becomes even harder if you take a look at the images of individual example events.\nApparently, the images are very sparse, which means only a couple of pixels are activated. In addition, considering the logarithmic scale, most pixels are activated only very weakly, meaning that the majority of the energy in the calorimeters is deposited only in a hand full of pixels, which gives small room for distinctive features. In addition, convolutional networks can struggle with very sparse structures, which can pose difficulties when building robust Autoencoders. Since the quantum circuit simulations are very demanding and my hardware access was limited, I mainly focus on a very reduces version of the dataset. I produced it by center cropping the ECAL images to about 80% and then rescaling it to 12x12. The code for this preprocessing can be found here.\nClassical methods As a very simple benchmark model, I consider a convolutional autoencoder. To get a feeling for the model and training, I first tried an Autoencoder on the 40x40 ECAL dataset. I build the model similar to the one proposed in ( Citation: Finke,\u0026#32;Krämer \u0026amp; al.,\u0026#32;2021 Finke,\u0026#32; T.,\u0026#32; Krämer,\u0026#32; M.,\u0026#32; Morandini,\u0026#32; A.,\u0026#32; Mück,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Oleksiyuk,\u0026#32; I. \u0026#32; (2021). \u0026#32;Autoencoders for unsupervised anomaly detection in high energy physics. JHEP 06 (2021) 161. https://doi.org/10.1007/JHEP06(2021)161 ) , which resulted in about 900k parameters. I Trained the Autoencoder on the quark jet images using AdamW, minimizing the binary cross-entropy. In general, I found the training to be not as straightforward as with other image datasets. The usage of PReLU activation and AdamW seemed to help stabilize the training. As latent space size, I found everything between 25-35 to be suitable. Below are given some quark jet examples from the test set to demonstrate the reconstruction.\nWe can now use the loss (binary cross-entropy) as a discriminating variable to tag gluon jets in the test set. Since the AE was trained on the quark jets, the average loss of a gluon jet image is expected to be higher, which labels it as an anomaly. We can therefore compute the ROC curve of the anomaly tagging. Here I obtained an AUC of about $71$%. Considering that ( Citation: Andrews,\u0026#32;Alison \u0026amp; al.,\u0026#32;2019 Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 ) achieved an AUC of $76$% when training on the full 125x125 ECAL images in a supervised manner, this performance seems reasonably good. However, if we train the Autoencoder on the Gluon jets instead and try to tag quark jets as anomalies, I only obtain an AUC of $29$%. This reflects the complexity bias mentioned before.\nTo have a realistic comparison with the quantum models, I also trained an Autoencoder on the 12x12 dataset. In this case, I also only used 10k images for training and especially constrained the Autoencoder to 3k parameters. Training this Autoencoder until convergence took a couple of hundret epochs and resulted in an AUC of $60$%. In the process of optimizing the models, I used the EMD from ( Citation: Komiske,\u0026#32;Metodiev \u0026amp; al.,\u0026#32;2019 Komiske,\u0026#32; P.,\u0026#32; Metodiev,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Thaler,\u0026#32; J. \u0026#32; (2019). \u0026#32;The Metric Space of Collider Events. Phys. Rev. Lett. 123, 041801 (2019). https://doi.org/10.1103/PhysRevLett.123.041801 ) to judge the reconstruction performance as it seems to capture it a little better than just the loss. Using the EMD as a loss does not seem possible as the implementation as a loss is not differentiable for TensorFlow.\nModels I implemented different models, mainly focusing on a fully quantum model, only consisting of a single parametrized quantum circuit (PQC) and Hybrid models with several classical and quantum layers.\nFully quantum autoencoder My implementaion of the fully quantum autoencoder is based on ( Citation: Ngairangbam,\u0026#32;Spannowsky \u0026amp; al.,\u0026#32;2021 Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 ) and ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) . The architecture consists of some data encoding and trainable PQC which acts as an encoder, followed by a swap test, which fixes the non-latent qubits to the value of some reference bits. This compresses the quantum state to the latent space. A detailed description of the functionality of the Quantum Autoencoder can be found in my post about it.\nOf course, the main question is, how to upload the data and what structure of circuit to use as a trainable layer. I started with a basic angle embedding, where one feature is encoded per qubit. In this encoding the $j$th feature is embedded, by rotating the $j$th qubit with $e^{-i x_j\\sigma_x/2}\\ket{0}$. While this encoding is simple, it only allows a single feature per qubit, which limits the method to datasets with a small number of features or makes it necessary to apply other dimensionality reduction techniques first. Another alternative would be Amplitude Encoding, however, this requires very deep circuits and in prototype implementations I found its performance in the Autoencoder to be very limited. Therefore I drew inspiration from the data re-uploading technique proposed in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) . I discuss the idea in detail in my data re-uploading post, however, in a nutshell, arbitrary dimensional data is uploaded to a single qubit multiple times to mimic a deep neural network with a hidden layer. In order to upload a whole image to a couple of qubits, I chose to upload the image in patches.\nA patch is uploaded to a single qubit, where the pixels $x_i$ of the patch are uploaded with a parametrized unitarity $U(b_i + w_i + x_i)$ with weights and biases as introduced in the data re-uploading. So a 12x12 image can e.g. be divided into $3\\times 3=9$ patches of the size 4x4. In this case, we would upload 16-pixel values on 9 qubits respectively. In the data reuploading spirit, this circuit can be entangled and repeated multiple times to add parameters and build a deeper circuit.\nI trained the model with Adam, maximizing the fidelity of the swap test. Currently, the maximum AUC I achieved is $56.5$%. This result uses 5 data re-uploads which leads to 1440 parameters. I would expect this to improve with more parameters. One question I\u0026rsquo;m still investigating is the best choice of reference states. In the derivation of the Autoencoder, the specific choice of reference states does not matter. However together with the data reuploading as an encoder I think one needs to be careful, because if the reference states are initialized as $\\ket{0}$ the fidelity is maximized if all parameters are zero, creating a pseudo solution.\nHybrid model The hybrid models I build are basically classical layers reducing the dimension of the data and feeding it into a PQC. The qubits get measured to obtain a classical latent space, which is reconstructed into an image by classical layers. Similar models have been proposed before, e.g. in ( Citation: Rivas,\u0026#32;Zhao \u0026amp; al.,\u0026#32;2021 Rivas,\u0026#32; P.,\u0026#32; Zhao,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Orduz,\u0026#32; J. \u0026#32; (2021). \u0026#32; Hybrid Quantum Variational Autoencoders for Representation Learning. https://doi.org/10.1109/CSCI54926.2021.00085 ) .\nIn order to make use of the PQC, I think it makes sense to use the same encoder as the fully quantum autoencoder. This way we can upload a larger image to the PQC and reduce the dimension down to the number of qubits. For a first implementation, I e.g. reduce the dimension of the image with convolutional layers down to 9x9. I then upload the image in patches like in the fully quantum case. However this time I use a kernel size of 3 and a stride of 2 to upload the data in 16 patches. This way I can measure all 16 qubits to obtain the latent space. A smaller latent space would be too small to fully reconstruct the image.\nUnfortunately training this model took very long and I was not able to train it until convergence. However, I achieved an AUC of $57$% without the model converging.\nImplementation details When I started the project I implemented the fully quantum autoencoder in TensorFlow-quantum together with cirq. However, I soon moved to pennylane due to the flexibility when building quantum models and their great plugin system. When you write your code in pennylane you specify a device on which to run your quantum circuits. When simulating the circuits you can e.g. use the lightning simulator, which is a fast simulation framework.\ndev = qml.device(\u0026#39;lightning.qubit\u0026#39;, wires=TOTAL_QBITS) The wires thereby specifies the number of qubits to simulate. When the code is developed in pennylane you can always switch the backend for the simulations without making any changes to the code. In principle, you can also use a real quantum computer e.g. with the pennylane qiskit plugin. One plugin that is especially useful is the lightning.gpu. It enables the simulation of the circuits on GPUs using NVIDIAs cuQuantum framework, which can considerably speed up the simulation when using a larger amount of qubits.\nIn pennylane a circuit is built by successively applying gates to different wires, e.g. the function building the circuit for the fully quantum autoencoder:\ndef circuit(self, params, data): encoder(params, data) qml.Hadamard(wires=total_qbits-1) for i in range(trash_qbits): qml.CSWAP(wires=[total_qbits - 1, latent_qbits + i, data_qbits + i]) qml.Hadamard(wires=total_qbits-1) return qml.expval(qml.PauliZ(total_qbits-1)) This function takes parameters, which are trainable, and data, which is not trainable as arguments. In the encoder function, more gates are applied to upload the data with trainable weights using e.g. Pauli X rotations qml.RX. Thereby total_qubits denotes the total number of qubits the circuit contains, data_qubits the number of qubits the encoder uses, latent_qbits the size of the latent and trash_qubits the number of non-latent space and there also the number of reference qubits. The circuit function can be turned into a Qnode, which is a pennylane object associated with a function that returns an expectation value. Here we measure the qubit containing the result of the swap test in the $\\sigma_z$ basis as described in the QAE post. When a Qnode is created you specify the differentiation method for it.\ncircuit_node = qml.QNode(circuit, dev, diff_method=\u0026#34;adjoint\u0026#34;) The differentiation method describes how the gradients of the parameters are calculated. On a quantum computer, the gradients of a circuit can e.g. be obtained with the parameter shift rule. When simulating however I would usually use the adjoint differentiation, as it is a very fast method.\nYou can combine the circuit simulation and differentiation of pennylane with your machine learning framework of choice, e.g. TensorFlow, Pytorch, or Jax. When using Keras e.g. pennylane already provides a class for turning a Qnode into a Keras layer with\nweight_shapes = {\u0026#34;weights\u0026#34;: (num_params,)} qlayer = qml.qnn.KerasLayer(circuit_node, weight_shapes, output_dim=num_outputs) Note that to my knowledge, this currently only works if the data passed to the qlayer is onedimensional.\nOutlook The next step is to scale the quantum models to more parameters and longer training on more sophisticated hardware. The development and first experiments were performed on retail hardware which is not suitable for larger simulations. I am curious to see if the quantum models can achieve the same or even a better AUC when simulating the quantum models with the same number of parameters and training for the same amount of epochs as the classical reference model. Furthermore, we should try to train the models on larger images as there is of course a large loss of information when scaling the images down to 12x12.\nApart from the computational bottleneck, there are still a couple of questions that are not fully answered yet. I mainly want to investigate the effects of different initializations of the reference qubits and of different latent space sizes. Furthermore one could also try other entangling schemes than the simple circular entangling topology.\nSince we have tried vision transformer-based architectures for supervised tasks on jet images I also implemented a quantum vision transformer. To do so I replaced the self-attention layer in a simple ViT with the quantum self-attention proposed in ( Citation: Li,\u0026#32;Zhao \u0026amp; al.,\u0026#32;2022 Li,\u0026#32; G.,\u0026#32; Zhao,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; X. \u0026#32; (2022). \u0026#32;Quantum Self-Attention Neural Networks for Text Classification.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2205.05625 ) . By now I was not able to train the model, again, due to a lack of resources, however, I hope to be able to run it in the future.\nAll in all, it was a fun experience and I\u0026rsquo;m looking forward to seeing what QML will be able to achieve in HEP in the future.\nReferences Rivas,\u0026#32; Zhao\u0026#32;\u0026amp;\u0026#32;Orduz (2021) Rivas,\u0026#32; P.,\u0026#32; Zhao,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Orduz,\u0026#32; J. \u0026#32; (2021). \u0026#32; Hybrid Quantum Variational Autoencoders for Representation Learning. https://doi.org/10.1109/CSCI54926.2021.00085 Andrews,\u0026#32; Alison,\u0026#32; An,\u0026#32; Bryant,\u0026#32; Burkle,\u0026#32; Gleyzer,\u0026#32; Narain,\u0026#32; Paulini,\u0026#32; Poczos\u0026#32;\u0026amp;\u0026#32;Usai (2019) Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 Finke,\u0026#32; Krämer,\u0026#32; Morandini,\u0026#32; Mück\u0026#32;\u0026amp;\u0026#32;Oleksiyuk (2021) Finke,\u0026#32; T.,\u0026#32; Krämer,\u0026#32; M.,\u0026#32; Morandini,\u0026#32; A.,\u0026#32; Mück,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Oleksiyuk,\u0026#32; I. \u0026#32; (2021). \u0026#32;Autoencoders for unsupervised anomaly detection in high energy physics. JHEP 06 (2021) 161. https://doi.org/10.1007/JHEP06(2021)161 Fraser,\u0026#32; Homiller,\u0026#32; Mishra,\u0026#32; Ostdiek\u0026#32;\u0026amp;\u0026#32;Schwartz (2021) Fraser,\u0026#32; K.,\u0026#32; Homiller,\u0026#32; S.,\u0026#32; Mishra,\u0026#32; R.,\u0026#32; Ostdiek,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Schwartz,\u0026#32; M. \u0026#32; (2021). \u0026#32;Challenges for Unsupervised Anomaly Detection in Particle Physics. https://doi.org/10.1007/JHEP03(2022)066 Kasieczka,\u0026#32; Nachman\u0026#32;\u0026amp;\u0026#32;al. (2021) Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 Komiske,\u0026#32; Metodiev\u0026#32;\u0026amp;\u0026#32;Thaler (2019) Komiske,\u0026#32; P.,\u0026#32; Metodiev,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Thaler,\u0026#32; J. \u0026#32; (2019). \u0026#32;The Metric Space of Collider Events. Phys. Rev. Lett. 123, 041801 (2019). https://doi.org/10.1103/PhysRevLett.123.041801 Li,\u0026#32; Zhao\u0026#32;\u0026amp;\u0026#32;Wang (2022) Li,\u0026#32; G.,\u0026#32; Zhao,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; X. \u0026#32; (2022). \u0026#32;Quantum Self-Attention Neural Networks for Text Classification.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2205.05625 Ngairangbam,\u0026#32; Spannowsky\u0026#32;\u0026amp;\u0026#32;Takeuchi (2021) Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 Romero,\u0026#32; Olson\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik (2016) Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ","permalink":"https://tommago.com/posts/gsoc/","summary":"This is a summary of my 2022 GSoC project with ML4SCI. The ML4SCI organization accustoms different projects of machine learning applied to scientific problems, many connected to high-energy physics. A big thank you to Sergei Gleyzer for the supervision and support.\nAbstract The Standard Model of particle physics is a theory that describes the fundamental particles and the interactions between them. While it has extensively been tested and was able to correctly predict experiments to an impressive degree, there are multiple reasons to believe that it cannot be a complete description of nature.","title":"GSoC 22 | Quantum Autoencoders for HEP Analysis at the LHC"},{"content":"An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.\nUniversal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions. Considering a classification problem, we might have functions $f: I_m \\to \\Reals$, where $I_m = [0,1]^m$. The output of a neural network with a single hidden layer may be written as $$h(\\vec{x}) = \\sum^{N}_{i=1}\\alpha_i\\sigma(\\vec{w}_i\\vec{x} + b_i), \\tag{1}$$ where $\\vec{w}_i$ and $b_i$ are the weights and biases of the hidden layer and $\\alpha_i$ the weights of the output layer. The function $\\sigma$ is the non-linear activation function. The function $h$ being dense in the continuous functions $f$ means, that for every $\\epsilon$ we can choose the parameters in Eq. $(1)$ so that $$|h(\\vec{x}) - f(\\vec{x})| \u0026lt; \\epsilon \\ \\ \\text{for all} \\ \\ \\vec{x}.$$ This is a very powerful statement and enables neural networks to tackle very complex problems.\nA proof for this theorem if $\\sigma$ is a sigmoidal function, which means $\\lim_{x\\to\\infty}\\sigma(x)=1$ and $\\lim_{x\\to -\\infty}\\sigma(x)=0$, can be found in ( Citation: Cybenko,\u0026#32;1989 Cybenko,\u0026#32; G. \u0026#32; (1989). \u0026#32;Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems,\u0026#32;2(4).\u0026#32;303–314. https://doi.org/10.1007/BF02551274 ) . The proof basically works by contradiction: If the space of all functions $h$ denoted by $S$ is not all of the continuous functions $f: I_m \\to \\Reals$ denoted by $C(I_m)$, then there is a linear functional $L: C(I_m) \\to \\Reals$ with $L(S)=0$ (or more accurately the closure of $S$). The functional can be written as integral over a function $h$ with respect to some measure $\\mu$. Since $L(S)=0$, which follows from the Hanh-Banach theorem, the integral over our neural network function $h$ with respect to the measure $\\mu$ would vanish. However one can show that for a sigmoidal function $\\sigma$ integrals over terms of the form Eq. $(1)$ are non-zero for all non-zero measures, leading to the contradiction.\nThere are many variations of this theorem, especially ( Citation: Hornik,\u0026#32;1991 Hornik,\u0026#32; K. \u0026#32; (1991). \u0026#32;Approximation capabilities of multilayer feedforward networks. Neural Networks,\u0026#32;4(2).\u0026#32;251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T ) provides a proof dropping the sigmoidal requirement. This generalization applies to any nonconstant continuous activation function, which is bounded.\nUniversal Quantum Circuit Approximation In their paper ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) the authors show that a similar proof can be done for the approximation capabilities of a PQC with a single qubit. Let\u0026rsquo;s consider some data $\\vec{x}\\in\\Reals^n$ we want to classify. The data follows some classification function $f: \\Reals^n \\to O$ we want to approximate. In the simple case of binary classification, we might have $O=\\{0,1\\}$. The idea proposed in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) is to subsequently apply parametrized gates with trainable weights and data uploads, effectively uploading the data many times. Hence the name data re-uploading. In their paper, the authors describe the motivation for the data re-uploading to be that classical neural networks effectively copy the input data when processing it. An example of a neural network with a single hidden layer is shown below.\nWhen e.g. passing the input data to the first hidden layer, the data is effectively passed to every unit of the hidden layer separately, thus \u0026ldquo;copied\u0026rdquo;. In Quantum Mechanics, however, there is the No-Cloning-Theorem, which states that there is no unitarity $U$ that clones arbitrary input states. This can be seen when assuming two states $\\ket{\\phi}$ and $\\ket{\\psi}$ which should be copied independently to a state $\\ket{c}$. The unitarity $U$ should therefore fulfill $$ \\begin{align*} \u0026amp;\\ket{S_1} = U(\\ket{\\phi,c}) = \\ket{\\phi,\\phi},\\\\ \u0026amp;\\ket{S_2} = U(\\ket{\\psi,c}) = \\ket{\\psi,\\psi}, \\end{align*} $$ in order to universally clone the input states. Here we denote $\\ket{\\phi}\\otimes\\ket{c} = \\ket{\\phi,c}$. The scalar product of the cloned states $\\ket{S_1}$ and $\\ket{S_2}$ can be written as $$ \\begin{align*} \\braket{S_1|S_2} =\u0026amp; \\bra{\\phi,c}U^\\dagger U \\ket{\\psi,c} = \\braket{\\phi,c|\\psi,c} = \\braket{\\phi,\\phi|\\psi,\\psi} \\\\ =\u0026amp; \\braket{\\phi|\\psi}\\braket{\\phi|\\psi} = \\braket{\\phi|\\psi}\\braket{k|k}. \\end{align*} $$ Since $\\braket{k|k}=1$ we have $$ \\begin{align*} \\braket{\\phi|\\psi}^2 = \\braket{\\phi|\\psi}, \\end{align*} $$ which is solved by $\\braket{\\phi|\\psi}=1$ and $\\braket{\\phi|\\psi}=0$. In the first case, the two states $\\phi$ and $\\psi$ are identical, which we don\u0026rsquo;t want, since we want to clone different states with the same unitarity. In the second case, the two states are orthogonal. The unitarity $U$ is therefore only able to clone orthogonal states. Non-orthogonal states can\u0026rsquo;t be copied without some information loss.\nTo mimic the copying of input data to hidden notes, as it happens in classical neural networks, the authors, therefore, propose to upload the input data multiple times to a single qubit. An example of a DRC is sketched below.\nA single gate can be understood as a single perceptron, with the unitarity as activation function.\nTo investigate the capabilities of this circuit, we consider a general unitary transformation $U(\\phi_1, \\phi_2, \\phi_3)\\in\\text{SU}(2)$. We can use this unitarity to upload data with $U(\\vec{x})$ or apply transformations with trainable parameters $\\vec{\\phi}$. In the case of data with only three features $\\vec{x}\\in\\Reals^3$, we would construct the data re-uploading circuit (DRC) with depth $N$ as $$\\ket{m} = U(\\vec{\\phi}_N)U(\\vec{x}) \\cdots U(\\vec{\\phi}_1)U(\\vec{x})\\ket{0}.$$ After applying the gates, the qubit can be measured to access the state which arises from the PQC. To reduce the number of gates and thus the depth of the circuit, we can incorporate the data upload and the parameters in a single gate. A single processing unit as analogy to a single unit in a neural network is then written as $$L_i = U(\\vec{b}_i + \\vec{w}_i\\odot\\vec{x}),$$ where $w$ are the weights, $b$ the biases and $\\odot$ denotes the elementwise Hadamard product. This already looks like the output of a single neuron! The classifier then becomes $$\\ket{m} = L_N \\cdots L_1\\ket{0}.$$\nData with an arbitrary number of features can be treated by padding the data with zeros, so that the number of features is a multiple of three and then uploading the three-dimensional feature vectors $\\vec{x}_j$ successively. In this case, a single processing unit $L_i$ is given as $$L_i = U(\\vec{b}^{(k)}_i + \\vec{w}^{(k)}_i \\odot \\vec{x}^{(k)}) \\cdots U(\\vec{b}^{(1)}_i + \\vec{w}^{(1)}_i \\odot \\vec{x}^{(1)}).$$\nTo see that this expression can approximate any function, we insert an explicit representation of $U(\\vec{\\phi})$ and summarize all transformations. For the general unitarity, we use $$U(\\vec{\\phi}) = \\mathrm{e}^{i\\phi_2\\sigma_z} \\mathrm{e}^{i\\phi_1\\sigma_y} \\mathrm{e}^{i\\phi_3\\sigma_z},$$ where we abbreviate $\\vec{\\phi} = \\vec{b} + \\vec{w}\\odot\\vec{x}$. This is equal to $$U(\\vec{\\phi}) = \\mathrm{e}^{i(w_1(\\vec{\\phi})\\sigma_x + w_2(\\vec{\\phi})\\sigma_y + w_3(\\vec{\\phi})\\sigma_z)},$$ with $$ \\begin{align} w_1(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_2(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\cos\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_3(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right), \\end{align} $$ where $d = \\arccos\\left(\\cos\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right)\\right)$.\nA DRC with $N$ processing units can now be written as $$\\mathcal{U} := \\prod^N_{j=1} \\mathrm{e}^{i(w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z)}.$$ Here the index of the $\\phi$ denotes the index of the different weights and biases. The product of Pauli-matrix exponentials can be simplified using the Baker-Campbell-Hausdorff formula $$\\mathcal{U} = \\exp\\left(i\\sum^N_{j_1}\\left[w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z\\right] + \\mathcal{O}_\\text{corr}\\right).$$\nThe correction term $\\mathcal{O}_\\text{corr}$ proportional to commutators of Pauli-matrices. The sum of the $w(\\vec{\\phi})$ terms can now be rewritten. Since all $w_i(\\vec{\\phi})$ are trigonometric functions, which are bounded to $[-1,1]$ and continuous, we can use the general version of the Universal Approximation Theorem and use the sum over $w_i(\\vec{b}_i + \\vec{w}_i \\odot \\vec{x})$ to approximate some continuous function $f_i(\\vec{x})$ just like in the classical case $$\\sum^N_{j=1}w_i(\\vec{b}_j + \\vec{w}_j \\odot \\vec{x}) = f_i(\\vec{x}).$$\nSince the correction terms are proportional to pauli matrices, ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) argues, that they can be absorbed into the functions $f(\\vec{x})$.\nBy optimizing the parameters with a classical optimization scheme, we can therefore approximate any function in terms of the final state (theoretically only with an infinite number of data re-uploads of course). To perform the optimization and classification, we can measure the qubit at the end of the circuit and compare the outcome with states which are predefined for the different classes. This way, it is possible to perform binary classification, but also multiclass problems can be treated by defining a label state for every class.\nThis approach can be extended to multi-qubit classifiers by entangling the different qubits. I think, the data re-uploading approach definitely looks quite promising, since it proves the capabilities of QML. In addition, it enables us to upload larger amounts of data on fewer qubits. Furthermore, it seems to improve the robustness against noise see e.g. ( Citation: Easom-Mccaldin,\u0026#32;Bouridane \u0026amp; al.,\u0026#32;2021 Easom-Mccaldin,\u0026#32; P.,\u0026#32; Bouridane,\u0026#32; A.,\u0026#32; Belatreche,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Jiang,\u0026#32; R. \u0026#32; (2021). \u0026#32;On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access,\u0026#32;9.\u0026#32;65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 ) . Currently, I am using DRCs in my autoencoders. In the future, I aim to explore different entangle schemes for multi-qubit DRCs.\nEasom-Mccaldin,\u0026#32; Bouridane,\u0026#32; Belatreche\u0026#32;\u0026amp;\u0026#32;Jiang (2021) Easom-Mccaldin,\u0026#32; P.,\u0026#32; Bouridane,\u0026#32; A.,\u0026#32; Belatreche,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Jiang,\u0026#32; R. \u0026#32; (2021). \u0026#32;On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access,\u0026#32;9.\u0026#32;65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 Cybenko (1989) Cybenko,\u0026#32; G. \u0026#32; (1989). \u0026#32;Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems,\u0026#32;2(4).\u0026#32;303–314. https://doi.org/10.1007/BF02551274 Hornik (1991) Hornik,\u0026#32; K. \u0026#32; (1991). \u0026#32;Approximation capabilities of multilayer feedforward networks. Neural Networks,\u0026#32;4(2).\u0026#32;251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ","permalink":"https://tommago.com/posts/drc/","summary":"An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.\nUniversal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions.","title":"Data re-uploading"},{"content":"When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).\nNatural Gradient Descent When minimizing a cost function $\\mathcal{L}(\\Theta)$ the well-known gradient descent iteratively updates the parameters $\\Theta$ by descending into the direction of the gradient $$\\Theta_{t+1} := \\Theta_t - \\eta \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}.$$ Here and in the following all gradients are calculated with respect to $\\Theta$ In Stochastic Gradient Descent (SGD) specifically the gradient $\\nabla \\mathcal{L}(\\Theta)$ is approximated by the gradient of the cost function over a subset of the training data. With this update rule, gradient descent implicitly assumes a euclidean geometry of the parameter space. This can be seen when writing the update rule as $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta} \\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_2 \\right],$$ where a proximity term is added, just like in the lagrangian of a spring mass. The equivalence to the gradient descent update rule can immediately be seen when solving the $\\argmin$ by setting the derivative equal to zero.\nThe choice of euclidean geometry does however not necessarily reflect the actual parameter space. Since it gives equal weight to all parameters $\\Theta_i$ ill-conditioned situations can arise as e.g. shown below.\nThe algorithm bounces over the valley and only slowly approaches the minimum. In the shown example the large step size aggravates the problem. For SGD a careful tuning of the learning rate is therefore especially important. Optimizers like Adam can address this problem by adjusting the step size based on previous gradients. A reparameterization of the parameters space on the other hand could lead to a problem way better suited for SGD.\nSo instead of using the euclidean metric $||\\Theta||_2$ a distance measure for an infinitesimal vector $\\text{d}\\Theta$ on a curved manifold is given by $$||\\Theta||_{g} = \\sum_{ij}g_{ij}(\\Theta)\\text{d}\\Theta_i\\text{d}\\Theta_j,$$ where $g_{ij}$ is the Riemannian metric tensor.\nFor every physicist, this seems very familiar. Of course, the euclidean metric is the special case of $g_{ij}=\\delta_{ij}$. Using this general metric for the method of steepest descent S. Amari shows in ( Citation: Amari,\u0026#32;1998 Amari,\u0026#32; S. \u0026#32; (1998). \u0026#32;Natural Gradient Works Efficiently in Learning. Neural Computation,\u0026#32;10(2).\u0026#32;251–276. https://doi.org/10.1162/089976698300017746 ) that the gradient descent update rule becomes $$\\Theta_{t+1} := \\Theta_t - \\eta G^{-1}\\nabla\\mathcal{L}(\\Theta)\\big|_{\\Theta_{t+1}}\\tag{1},$$ where $G^{-1}$ is the inverse of the metric $G = (g_{ij})$.\nThe question remains on how to determine the metric. In the framework of Information Geometry, instead of considering the parameter space, the optimization is performed on the so-called statistical manifold. A statistical manifold is a Riemannian manifold, where every point corresponds to a probability function.\nIn our case, we may consider the manifold of likelihoods $p(x|\\Theta)$ for the different possible parameters $\\Theta$. To measure the similarity between two probability distributions there exist different divergences, the most known one being the Kullback–Leibler (KL) divergence. For two distribution $p(x)$ and $q(x)$ it is defined as $$D_{KL}(p(x)||q(x)) = \\sum_x p(x)\\log\\left(\\frac{p(x)}{q(x)}\\right)\\tag{2}.$$ Note that formally the KL-divergence is not symmetric and thus is not a proper distance measure. However, things work out for infinitesimal distance and thus it can be used to describe the manifold locally ( Citation: Martens,\u0026#32;2014 Martens,\u0026#32; J. \u0026#32; (2014). \u0026#32;New insights and perspectives on the natural gradient method.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1412.1193 ) .\nLet\u0026rsquo;s try to rewrite our gradient update from SGD with the KL-divergence instead of the euclidean metric: $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta}D_{KL}(q(x|\\Theta)||q(x|\\Theta_t)) \\right]$$ To minimize this expression we set the derivative to zero $$\\nabla \\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t} + \\frac{1}{\\eta}\\nabla D_{KL}\\left(q(x|\\Theta)||q(x|\\Theta_t)\\right)\\bigg|_{\\Theta_{t+1}} = 0\\tag{3}.$$ So to solve this we need the gradient of the KL-divergence, which we will approximate by Taylor expanding the $D_{KL}$ around $\\Theta_t$. In the following we denote $D_{KL}(\\Theta||\\Theta_t) := D_{KL}(q(x|\\Theta)||q(x|\\Theta_t))$ for brevity. In second order we obtain $$ \\begin{align*} D_{KL}(\\Theta||\\Theta_t)\\approx D_{KL}(\\Theta_t||\\Theta_t) + \\nabla D_{KL}(\\Theta||\\Theta_t)\\big|_{\\Theta_t}(\\Theta-\\Theta_t)\\\\ +\\frac{1}{2}(\\Theta - \\Theta_t)^T H_{D_{KL}}\\big|_{\\Theta_t} (\\Theta - \\Theta_t), \\end{align*} $$ where $H_{D_{KL}}$ denotes the Hessian with respect to $\\Theta$. The first term obviously vanishes since the divergence for identical distributions is zero. The second becomes zero as well, which we can see if we insert the definition from Eq. $(2)$: $$ \\begin{align*} \\nabla D_{KL}(\\Theta||\\Theta_t)\\big|_{\\Theta_t}=\u0026amp;\\sum_x \\nabla p(\\Theta)\\big|_{\\Theta_t}\\log\\left(\\frac{p(\\Theta_t)}{p(\\Theta_t)}\\right) + p(\\Theta)\\nabla\\log\\left(\\frac{p(\\Theta)}{p(\\Theta_t)}\\right) \\\\ \u0026amp; =\\sum_x \\nabla p(\\Theta) - p(\\Theta) \\nabla\\log\\left(p(\\Theta_t)\\right) = \\nabla 1 = 0. \\end{align*} $$ We can now insert the Taylor expression for the KL-divergence in Eq. $(3)$ to obtain $$\\nabla\\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t}+\\frac{1}{\\eta}H_{D_{KL}}\\bigg|_{\\Theta_t}(\\Theta - \\Theta_t)=0,$$ which leads to our update-rule $$\\Theta_{t+1} := \\Theta_t - \\eta H^{-1}_{D_{KL}}\\bigg|_{\\Theta_t}\\nabla\\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t} \\tag{4}.$$ Comparing this with Eq. $(1)$ we can identify the metric $G$ with the hessian of the KL-divergence $H_{D_{KL}}$. Rearranging the terms we can bring the hessian of the KL-divergence in the familiar form of the fisher information matrix $${H_{D_{KL}}}_{ij} = g_{ij} = \\sum_x p(x|\\Theta)\\frac{\\partial \\log p(x|\\Theta)}{\\partial \\Theta_i}\\frac{\\partial \\log p(x|\\Theta)}{\\partial \\Theta_j}.$$ The fisher information matrix thus describes the local curvature of the statistical manifold. With Eq. $(4)$ it constitutes the classical natural gradient descent.\nQuantum natural gradient descent The optimization of PQCs is very similar to classical deep learning. We may have a quantum circuit with parameters $\\Theta$. The resulting states of the circuit for fixed input data define a parametrized Hilbert space $\\mathcal{H}(\\Theta)$. We can define a distance measure $d$ between two states with an infinitesimal distance between the parameters $$d\\left(\\ket{\\psi(\\Theta)}, \\ket{\\psi(\\Theta + \\text{d}\\Theta)}\\right) = \\sum_{ij} g_{ij}(\\Theta)\\text{d}\\Theta_i\\text{d}\\Theta_j,$$ where $g_{ij}$ is the Fubini-Study metric ( Citation: Yamamoto,\u0026#32;2019 Yamamoto,\u0026#32; N. \u0026#32; (2019). \u0026#32;On the natural gradient for variational quantum eigensolver.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1909.05074 ) $$\\text{Re}\\left[\\braket{\\partial_i\\psi|\\partial_j\\psi}-\\braket{\\partial_i\\psi|\\psi}\\braket{\\psi|\\partial_j\\psi}\\right],$$ where $\\ket{\\partial_i\\psi}=\\partial\\ket{\\psi(\\Theta)}\\big/\\partial\\Theta_i$. With this metric, we can again write out and simplify our formulation of steepest descent to obtain an update rule for the quantum natural gradient descent proposed in ( Citation: Stokes,\u0026#32;Izaac \u0026amp; al.,\u0026#32;2019 Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 ) $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta} \\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_{g(\\Theta_t)} \\right],$$ where using our metric $g(\\Theta)$ we have the norm as the scalar product $$\\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_{g(\\Theta_t)} = \\braket{\\Theta - \\Theta_t, g(\\Theta_t)(\\Theta - \\Theta_t)}.$$ Setting the derivative to zero like before directly leads to $$\\Theta_{t+1} = \\Theta_t - \\eta g^+(\\Theta_t)\\nabla\\mathcal{L}(\\Theta)\\big|_{\\Theta_t}.$$ Here $g^+$ denotes the pseudo-inverse of the metric tensor which is usually calculated as the Moore-Penrose-Inverse.\nComputing the metric tensor can be very expensive, which is why ( Citation: Stokes,\u0026#32;Izaac \u0026amp; al.,\u0026#32;2019 Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 ) proposes to compute a diagonal or block-diagonal approximation of it.\nImplementation Fortunately, the computation of the diagonal and block diagonal approximations of the metric tensor are already implemented in Pennylane\nI want to show a little example of the usage of QNG on a real dataset as I struggled a bit with the implementation of the iteration over data. Suppose you have some circuit that takes the parameters and data as arguments. If you want to train the parameters you define some cost function e.g. a simple MSE\ndef cost(params, x, y): return (y - circuit(params, x)) ** 2 After initializing the parameters params we optimize them by iterating over the training data x_train, y_train and applying steps to the optimizer QNGOptimizer which is implemented in pennylane. To compute the step, however, we need the metric tensor function, which is also implemented in pennylane. As the metric tensor function can only be obtained for function with a single argument, namely the parameters to be trained, we need to define a lambda function for every data sample that only depends on the parameters. The same goes for the cost function.\nopt = qml.QNGOptimizer(learning_rate) for it in range(epochs): for j, sample in enumerate(x_train): cost_fn = lambda p: cost_sample(p, sample, y[j]) metric_fn = lambda p: qml.metric_tensor(circuit, approx=\u0026#34;block-diag\u0026#34;)(p, sample) params = opt.step(cost_fn, params, metric_tensor_fn=metric_fn) print(j, end=\u0026#34;\\r\u0026#34;) loss = cost(params) print(f\u0026#34;Epoch: {it} | Loss: {loss} |\u0026#34;) Note that the data needs to be defined in pennylane with requires_grad=False.\nThe QNG can be quite useful in avoiding Barren Plateaus in training. However of course computing the metric tensor takes time, which makes the QNG especially useful for models with a smaller number of parameters.\nAmari (1998) Amari,\u0026#32; S. \u0026#32; (1998). \u0026#32;Natural Gradient Works Efficiently in Learning. Neural Computation,\u0026#32;10(2).\u0026#32;251–276. https://doi.org/10.1162/089976698300017746 Martens (2014) Martens,\u0026#32; J. \u0026#32; (2014). \u0026#32;New insights and perspectives on the natural gradient method.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1412.1193 Stokes,\u0026#32; Izaac,\u0026#32; Killoran\u0026#32;\u0026amp;\u0026#32;Carleo (2019) Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 Yamamoto (2019) Yamamoto,\u0026#32; N. \u0026#32; (2019). \u0026#32;On the natural gradient for variational quantum eigensolver.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1909.05074 ","permalink":"https://tommago.com/posts/qng/","summary":"When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).","title":"Quantum Natural Gradient Descent"},{"content":"In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analog of a classical autoencoder equally aims to learn a smaller representation of data.\nA naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure. Here the Autoencoder has an input dimension of three and compresses the data to a single qbit.\nEach line represents a single qbit and reading from left to right, consecutive transformations are applied to them. The classical training data is first encoded into a quantum state indicated by $|\\psi\\big\u0026gt;$. This can be done with different encodings e.g. the angle encoding ( Citation: Weigold,\u0026#32;Barzen \u0026amp; al.,\u0026#32;2021 Weigold,\u0026#32; M.,\u0026#32; Barzen,\u0026#32; J.,\u0026#32; Leymann,\u0026#32; F.\u0026#32;\u0026amp;\u0026#32;Salm,\u0026#32; M. \u0026#32; (2021). \u0026#32; Expanding Data Encoding Patterns For Quantum Algorithms. https://doi.org/10.1109/ICSA-C52384.2021.00025 ) . After feeding the data into the first three qbits, an encoding block is applied. The encoder is a parameterized unitary transformation $U(\\Theta)$. It consists of rotations on the Bloch sphere and entanglement using CNOT gates. The parameters $\\Theta$ are angles for the rotations which will eventually be learned when training the autoencoder. After applying the encoder, a second parametrized circuit follows, which acts as the decoder. The decoder only overlaps with the encoder at a subset of its qbits, in this example a single one. This qbit acts as the smaller latent space of the autoencoder. The rest of the decoder circuit acts on qbits that were initialized as $|0\\big\u0026gt;$. The goal of an Autoencoder is to learn to reconstruct the input data after compressing it to the latent space. To verify this, we can use a SWAP-test with three reference bits. The reference bits were initialized to the input state of the data as well. The SWAP-test can then measure the similarity of the input data state $|\\psi\\big\u0026gt;$ with the output of the decoder $|\\phi\\big\u0026gt;$. This similarity $F$ is called fidelity $$F(\\ket{\\psi},\\ket{\\phi})=\\big|\\big\u0026lt;\\psi|\\phi\\big\u0026gt;\\big|^2.$$ It can be measured at the readout bit.\nSince we want to learn parameters that enable the autoencoder to reconstruct the input, we can then use the fidelity to construct the loss function $$\\mathcal{L} = 1-\\big|\\big\u0026lt;\\psi|\\phi\\big\u0026gt;\\big|^2$$ to minimize. The loss function can be used to optimize the parameters by gradient descent similar to classical learning.\nAn example for a circuit of a $3\\rightarrow 1 \\rightarrow 3$ autoencoder is shown below.\nIn this example for simplicity, the Encoder and Decoder consist of only a single layer using $R_y(\\Theta_i)$ gates and entanglement by CNOT. The SWAP-test is carried out using controlled SWAP-gates on the output of the decoder and the reference bits. The controlling bit is the last qbit which is used to read out the result of the SWAP-test. It is initialized as $|0\\big\u0026gt;$ and prepared with a Hadamard gate, which leads to the state $\\frac{1}{\\sqrt{2}}|0\\big\u0026gt; + \\frac{1}{\\sqrt{2}}|1\\big\u0026gt;$. The controlled SWAP operation on two states $|\\psi\\big\u0026gt;$ and $|\\phi\\big\u0026gt;$ transfers the three qbit system into the state $\\frac{1}{\\sqrt{2}}|0,\\psi,\\phi\\big\u0026gt; + \\frac{1}{\\sqrt{2}}|1,\\phi,\\psi\\big\u0026gt;$. The $|\\psi\\big\u0026gt;$ could thereby be the state of an output qbit of the decoder, e.g. $(0,2)$ and $|\\phi\\big\u0026gt;$ the state of the respective reference bit, e.g. $(0,5)$. Applying another Hadamard gate to the SWAP-qbit transfers the state to $\\frac{1}{2}\\big(|0,\\psi,\\phi\\big\u0026gt; + |1,\\psi,\\phi\\big\u0026gt; + |0,\\phi,\\psi\\big\u0026gt; - |1,\\phi,\\psi\\big\u0026gt;\\big)$. When we now measure the first qbit, which is the auxiliary SWAP-qbit $(0,8)$ in the circuit shown above, it will turn out to be $|0\\big\u0026gt;$ with the probability $$P(|0\\big\u0026gt;)=\\frac{1}{4}(\\big\u0026lt;\\psi,\\phi| + \\big\u0026lt;\\phi,\\psi|)(|\\psi,\\phi\\big\u0026gt; + |\\phi,\\psi\\big\u0026gt;) = \\frac{1}{2} + \\frac{1}{2}|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2.$$ The probability of measuring $|1\\big\u0026gt;$ is therefore $$P(|1\\big\u0026gt;) = 1 - P(|0\\big\u0026gt;) = \\frac{1}{2}-\\frac{1}{2}|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2.$$ To obtain the fidelity $|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2$ we measure the SWAP-qbit in the $Z$-basis. Since the eigenvalues of $\\sigma_z$ are $1$ and $-1$ the expectation value of the measurement calculates to $$\\big\u0026lt;q_8|\\sigma_z|q_8\\big\u0026gt; = 1\\cdot P(|0\\big\u0026gt;) + (-1)\\cdot P(|1\\big\u0026gt;) = |\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2,$$ where $|q_8\\big\u0026gt;$ denotes the SWAP-qbit. The output of measuring the last qbit of the shown circuit in the $Z$-basis, therefore, corresponds to the fidelity between the output of the decoder and the reference qbits.\nA simpler Quantum Autoencoder The naive implementation discussed above can be simplified as shown in ( Citation: Ngairangbam,\u0026#32;Spannowsky \u0026amp; al.,\u0026#32;2021 Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 ) based on ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) . A $3\\rightarrow 1 \\rightarrow 3$ autoencoder circuit without the SWAP test for measuring the fidelity of the output of the decoder is displayed below.\nThe Hilbert space $\\mathcal{H}=\\mathcal{H}_T\\otimes\\mathcal{H}_A\\otimes\\mathcal{H}_L$ can be written as the product of the subspaces for the so-called trash qubits $\\mathcal{H}_T$, the data qubits which are not part of the latent space $\\mathcal{H}_A$ and the compressed state at the latent bits $\\mathcal{H}_L$. In this depiction, the trash bits are fed into the decoder by a SWAP operation ${V_T}_A$ between $\\mathcal{H}_A$ and $\\mathcal{H}_T$. The input state to the autoencoder can be written as $$\\ket{\\Psi} = \\ket{\\psi}_{LA} \\otimes \\ket{t}_T,$$ where $\\ket{\\psi}$ is the actual input data and $\\ket{t}$ are the trash states which are initialized as $\\ket{0}$. The output state $\\ket{\\omicron}$ is the result of the unitary transformations and the SWAP applied to the input $$\\ket{\\omicron} = U_{LA}^\\dagger V_{AT} U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T.$$ The Fidelity between the input and the output $F(\\ket{\\Psi},\\ket{\\omicron})$ can be simplified $$ \\begin{align} \\begin{split} F(\\ket{\\Psi},\\ket{\\omicron}) \u0026amp;= \\big|\\braket{\\Psi|\\omicron}\\big|^2 \\\\ \u0026amp;= \\big|\\braket{\\Psi|U^\\dagger_{LA}V_{AT}U_{LA}|\\Psi}\\big|^2 \\\\ \u0026amp;= F\\big(U_{LA}\\ket{\\psi}_{LA} \\otimes \\ket{t}_T, V_{AT} U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T\\big)\\\\ \u0026amp;= F\\big(U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T, U_{LT} \\ket{\\psi}_{LT} \\otimes \\ket{t}_A\\big)\\\\ \\end{split} \\end{align} $$ where in the last line the SWAP $V_{AT}$ exchanged $\\mathcal{H}_A$ and $\\mathcal{H}_T$. We can now see that if $$U_{LZ} \\ket{\\psi}_{LZ} = \\ket{\\psi_c}_L\\otimes\\ket{t}_Z$$ the fidelity reduces to one. Here $\\ket{\\psi_c}$ denotes a compressed version of the input state. It is therefore sufficient to train the unitarity $U(\\Theta)$ to separate the trash state $\\ket{t}$. To do so, the decoder, which is the adjoint $U^\\dagger$ is not needed. An example circuit is shown below.\nIn this case, the SWAP test measures the fidelity between the trash qubits $\\ket{t}$ and the output bits of the encoder which are not latent bits. As we can see the main advantage of this method is that it needs way fewer qbits and works with a shallower circuit. Nevertheless, it can still be used for compression, as the lower dimensional representation of the input data could be extracted from the qbit $(0,0)$. Moreover, the fidelity used for training can equally be used for anomaly tagging. Another useful advantage is the fact that we can upload more than one feature per qubit as shown in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) , Because we don\u0026rsquo;t have to compare the output of the decoder with the input data.\nIt should be noted, that the two fidelities for the first and the second approach are not identical. According to ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) the fidelity of the naive QAE is always less or equal to the simpler version.\nCurrently, I am using this architecture and experimenting with different encoder circuits, as I am not aware of other fully quantum autoencoder architectures in the literature. It would be interesting to explore to what extent a potential quantum (variational ?) autoencoder could be used for generative purposes, which I will leave for the future.\nNgairangbam,\u0026#32; Spannowsky\u0026#32;\u0026amp;\u0026#32;Takeuchi (2021) Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 Romero,\u0026#32; Olson\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik (2016) Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 Weigold,\u0026#32; Barzen,\u0026#32; Leymann\u0026#32;\u0026amp;\u0026#32;Salm (2021) Weigold,\u0026#32; M.,\u0026#32; Barzen,\u0026#32; J.,\u0026#32; Leymann,\u0026#32; F.\u0026#32;\u0026amp;\u0026#32;Salm,\u0026#32; M. \u0026#32; (2021). \u0026#32; Expanding Data Encoding Patterns For Quantum Algorithms. https://doi.org/10.1109/ICSA-C52384.2021.00025 ","permalink":"https://tommago.com/posts/qae/","summary":"In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analog of a classical autoencoder equally aims to learn a smaller representation of data.\nA naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure.","title":"Quantum Autoencoder"},{"content":"This year I\u0026rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 ) . These models are thereby trained on Standard Model data and search for anomalous events that deviate from the known physics. With the rise of NISQ-devices ( Citation: Preskill,\u0026#32;2018 Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ) the question comes up if quantum machine learning can enhance classical machine learning applications to hep problems.\nSince we are encouraged by Google to publicly share our work, I set up this blog to document the project and share some of my scientific interests.\nKasieczka,\u0026#32; Nachman\u0026#32;\u0026amp;\u0026#32;al. (2021) Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 Preskill (2018) Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ","permalink":"https://tommago.com/posts/hello/","summary":"This year I\u0026rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics.","title":"Hello World! | Hello GSoC!"}]