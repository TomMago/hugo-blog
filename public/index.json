[{"content":"An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically aproximate any function. When it comes to quantum machine learning, a similar statement can be made. Suprisingly a single qubit is sufficient, to perform classification of arbitrary data distributions.\nUniversial Approximation Theorem The Universial Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continous functions. Considering a classification problem, we might have functions $f: I_m \\to \\Reals$, where $I_m = [0,1]^m$. The output of a neural network with a single hidden layer may be written as $$h(\\vec{x}) = \\sum^{N}_{i=1}\\alpha_i\\sigma(\\vec{w}_i\\vec{x} + b_i), \\tag{1}$$ where $\\vec{w}_i$ and $b_i$ are the weights and biases of the hidden layer and $\\alpha_i$ the weights of the output layer. The function $\\sigma$ is the non-linear activation function. The function $h$ beeing dense in the continous functions $f$ means, that for every $\\epsilon$ we can choose the parameters in Eq. $(1)$ so that $$|h(\\vec{x}) - f(\\vec{x})| \u0026lt; \\epsilon \\ \\ \\text{for all} \\ \\ \\vec{x}.$$ This is a very powerful statement and enables neural networks to tackle very complex problems.\nA proof for this theorem if $\\sigma$ is a sigmodial function, which means $\\lim_{x\\to\\infty}\\sigma(x)=1$ and $\\lim_{x\\to -\\infty}\\sigma(x)=0$, can be found in ( Citation: Cybenko,\u0026#32;1989 Cybenko,\u0026#32; G. \u0026#32; (1989). \u0026#32;Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems,\u0026#32;2(4).\u0026#32;303–314. https://doi.org/10.1007/BF02551274 ) . The prove basically works by contradiction: If the space of all functions $h$ denoted by $S$ is not all of the continous functions $f: I_m \\to \\Reals$ denoted by $C(I_m)$, then there is a linear functional $L: C(I_m) \\to \\Reals$ with $L(S)=0$ (or more accurately the closure of $S$). The functional can be written as integral over a function $h$ with respect to some measure $\\mu$. Since $L(S)=0$, which follows from the Hanh-Banach theorem, the integral over our neural network function $h$ with respect to the measure $\\mu$ would vanish. However one can show that for a sigmoidal function $\\sigma$ integrals over terms of the form Eq. $(1)$ are non-zero for all non-zero measures, leading to the contradiction.\nThere are many variations of this theorem, especially ( Citation: Hornik,\u0026#32;1991 Hornik,\u0026#32; K. \u0026#32; (1991). \u0026#32;Approximation capabilities of multilayer feedforward networks. Neural Networks,\u0026#32;4(2).\u0026#32;251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T ) provides a proof dropping the sigmodial requirement. This generelization applies to any nonconstant continous activation function, which is bounded.\nUniversal Quantum Circuit Approximation In their paper ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) the authors show that a similar proof can be done for the approximation capabilities of a PQC with a single qubit. Lets consider some data $\\vec{x}\\in\\Reals^n$ we want to classify. The data follows some classification function $f: \\Reals^n \\to O$ we want to approximate. In the simple case of binary classification we might have $O=\\{0,1\\}$. The idea proposed in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) is to subsequently apply parametrized gates with trainable weights and datauploads, effectively uploading the data many times. Hence the name data re-uploading. In their paper, the authors describe the motivation for the data re-uploading, that classical neural networks effectively copy the input data when processing it. An example for a neural network with a single hidden layer is shown below.\nWhen e.g. passing the input data to the first hidden layer, the data is effectively passed to every unit of the hidden layer seperately, thus \u0026ldquo;copied\u0026rdquo;. In Quantum Mechanics however there is the No-Cloning-Theorem, which states that there is no unitarity $U$ which clones arbitrary input states. This can be seen when assuming two states $\\ket{\\phi}$ and $\\ket{\\psi}$ which should be copied independetly to a state $\\ket{c}$. The unitarity $U$ should therefore fulfill $$ \\begin{align*} \u0026amp;\\ket{S_1} = U(\\ket{\\phi,c}) = \\ket{\\phi,\\phi},\\\\ \u0026amp;\\ket{S_2} = U(\\ket{\\psi,c}) = \\ket{\\psi,\\psi}, \\end{align*} $$ in order to universally clone the input states. Here we denote $\\ket{\\phi}\\otimes\\ket{c} = \\ket{\\phi,c}$. The scalar product of the cloned states $\\ket{S_1}$ and $\\ket{S_2}$ can be written as $$ \\begin{align*} \\braket{S_1|S_2} =\u0026amp; \\bra{\\phi,c}U^\\dagger U \\ket{\\psi,c} = \\braket{\\phi,c|\\psi,c} = \\braket{\\phi,\\phi|\\psi,\\psi} \\\\ =\u0026amp; \\braket{\\phi|\\psi}\\braket{\\phi|\\psi} = \\braket{\\phi|\\psi}\\braket{k|k}. \\end{align*} $$ Since $\\braket{k|k}=1$ we have $$ \\begin{align*} \\braket{\\phi|\\psi}^2 = \\braket{\\phi|\\psi}, \\end{align*} $$ which is solved by $\\braket{\\phi|\\psi}=1$ and $\\braket{\\phi|\\psi}=0$. In the first case the two states $\\phi$ and $\\psi$ are identical, which we don\u0026rsquo;t want, since we want to clone different states with the same unitarity. In the second case the two states are orthogonal. The unitarity $U$ is therefore only able to clone orthogonal states. Non-orthogonal states can\u0026rsquo;t be copied without some information loss.\nTo mimic the copying of input data to hidden notes, as it happens in classical neural networks, the authors therfore propose to upload the input data multiple times to the single qubit. An exmaple for a DRC is sketched below.\nA single gate can be understood as a single perceptron, with the unitarity as activation function.\nTo investigate the capabilites of this circuit, we consider a general unitary transformation $U(\\phi_1, \\phi_2, \\phi_3)\\in\\text{SU}(2)$. We can use this unitarity to upload data with $U(\\vec{x})$ or apply transformations with trainable parameters $\\vec{\\phi}$. In the case of data with only three features $\\vec{x}\\in\\Reals^3$, we would construct the data re-uploading circuit (DRC) with depth $N$ as $$\\ket{m} = U(\\vec{\\phi}_N)U(\\vec{x}) \\cdots U(\\vec{\\phi}_1)U(\\vec{x})\\ket{0}.$$ After applying the gates, the qubit can be measured to access the state which arises from the PQC. To reduce the number of gates and thus the depth of the circuit, we can encorporate the data upload and the parameters in a single gate. A single processing unit as analogy to a single unit in a neural network is then written as $$L_i = U(\\vec{b}_i + \\vec{w}_i\\odot\\vec{x}),$$ where $w$ are the weights, $b$ the biases and $\\odot$ denotes the elementwise Hadamard product. This already looks like the output of a single neuron! The classifier then becomes $$\\ket{m} = L_N \\cdots L_1\\ket{0}.$$\nData with an arbitrary number of features can be treated by padding the data with zeros, so that the number of features is a multiple of three and then uploading the three dimensional feature vectors $\\vec{x}_j$ successively. In this case a single processing unit $L_i$ is given as $$L_i = U(\\vec{b}^{(k)}_i + \\vec{w}^{(k)}_i \\odot \\vec{x}^{(k)}) \\cdots U(\\vec{b}^{(1)}_i + \\vec{w}^{(1)}_i \\odot \\vec{x}^{(1)}).$$\nTo see that this expression can approximate any function, we insert an explicit representaiton of $U(\\vec{\\phi})$ and summarize all transformations. For the general unitarity we use $$U(\\vec{\\phi}) = \\mathrm{e}^{i\\phi_2\\sigma_z} \\mathrm{e}^{i\\phi_1\\sigma_y} \\mathrm{e}^{i\\phi_3\\sigma_z},$$ where we abbreveate $\\vec{\\phi} = \\vec{b} + \\vec{w}\\odot\\vec{x}$. This is equal to $$U(\\vec{\\phi}) = \\mathrm{e}^{i(w_1(\\vec{\\phi})\\sigma_x + w_2(\\vec{\\phi})\\sigma_y + w_3(\\vec{\\phi})\\sigma_z)},$$ with $$ \\begin{align} w_1(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_2(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\cos\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_3(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right), \\end{align} $$ where $d = \\arccos\\left(\\cos\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right)\\right)$.\nA DRC with $N$ processing units can now be written as $$\\mathcal{U} := \\prod^N_{j=1} \\mathrm{e}^{i(w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z)}.$$ Here the index of the $\\phi$ denotes the index of the different weights and biases. The product of pauli-matrix exponentials can be simplified using the Baker-Campbell-Hausdorff formula $$\\mathcal{U} = \\exp\\left(i\\sum^N_{j_1}\\left[w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z\\right] + \\mathcal{O}_\\text{corr}\\right).$$\nThe correction term $\\mathcal{O}_\\text{corr}$ proportional to commutators of pauli matrices. The sum of the $w(\\vec{\\phi})$ terms can now be rewritten. Since all $w_i(\\vec{\\phi})$ are trigonometric functions, which are bounded to $[-1,1]$ and continous, we can use the general version of the Universial Approximation Theorem and use the sum over $w_i(\\vec{b}_i + \\vec{w}_i \\odot \\vec{x})$ to approximate some continous function $f_i(\\vec{x})$ just like in the classical case $$\\sum^N_{j=1}w_i(\\vec{b}_j + \\vec{w}_j \\odot \\vec{x}) = f_i(\\vec{x}).$$\nSince the correction terms are proportional to pauli matrices, ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) argues, that they can be absorbed into the functions $f(\\vec{x})$.\nBy optimizing the parameters with a classical optimization scheme, we can therefore approximate any function in terms of the final state (theoretically only with an infinate nuber of data re-uploads of course). To perform the optimization and classification, we can measure the qubit at the end of the circuit and compare the outcome with states which are predefined for the different classes. This way, it is possible to perform binary classification, but also multiclass problems can be treated by defining a label state for every class.\nThis approach can be extended to multi qubit classifiers by entangling the different qubits. I think, the data re-uploading approach definitely looks quite promising, since it proofs the capabilites of QML. In addition it enables us to upload larger amounts of data on less qubits. Furthermore it seems to improve the robustness agains noise see e.g. ( Citation: Easom-Mccaldin,\u0026#32;Bouridane \u0026amp; al.,\u0026#32;2021 Easom-Mccaldin,\u0026#32; P.,\u0026#32; Bouridane,\u0026#32; A.,\u0026#32; Belatreche,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Jiang,\u0026#32; R. \u0026#32; (2021). \u0026#32;On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access,\u0026#32;9.\u0026#32;65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 ) . Currently I am using DRCs in my autoencoders. In the future I aim to explore different entangle schemes for multi qubit DRCs.\nEasom-Mccaldin,\u0026#32; Bouridane,\u0026#32; Belatreche\u0026#32;\u0026amp;\u0026#32;Jiang (2021) Easom-Mccaldin,\u0026#32; P.,\u0026#32; Bouridane,\u0026#32; A.,\u0026#32; Belatreche,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Jiang,\u0026#32; R. \u0026#32; (2021). \u0026#32;On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access,\u0026#32;9.\u0026#32;65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 Cybenko (1989) Cybenko,\u0026#32; G. \u0026#32; (1989). \u0026#32;Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems,\u0026#32;2(4).\u0026#32;303–314. https://doi.org/10.1007/BF02551274 Hornik (1991) Hornik,\u0026#32; K. \u0026#32; (1991). \u0026#32;Approximation capabilities of multilayer feedforward networks. Neural Networks,\u0026#32;4(2).\u0026#32;251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ","permalink":"https://tommago.com/posts/drc/","summary":"An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically aproximate any function. When it comes to quantum machine learning, a similar statement can be made. Suprisingly a single qubit is sufficient, to perform classification of arbitrary data distributions.\nUniversial Approximation Theorem The Universial Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continous functions.","title":"Data re-uploading"},{"content":"When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).\nNatural Gradient Descent When minimizing a cost function $\\mathcal{L}(\\Theta)$ the well known gradient descent iteratively updates the parameters $\\Theta$ by descenting into the direction of the gradient $$\\Theta_{t+1} := \\Theta_t - \\eta \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}.$$ Here and in the following all gradients are calculated with respect to $\\Theta$ In Stochastic Gradient Descent (SGD) specifically the gradient $\\nabla \\mathcal{L}(\\Theta)$ is approximated by the gradient of the cost funciton over a subset of the training data. With this update rule, gradient descent implicitly assumes a euclidean geometry of the parameter space. This can be seen when writing the update rule as $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta} \\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_2 \\right],$$ where a proximity term is added, just like in the lagrangian of a spring mass. The equivalence to the gradient descent update rule can immediately be seen when solving the $\\argmin$ by setting the derivative equal to zero.\nThe choice of euclidean geometry does however not necessarily reflect the actual parameter space. Since it gives equal weight to all parameters $\\Theta_i$ ill conditioned situations can arise as e.g. shown below.\nThe algorithm bounces over the valley and only slowly approaches the minimum. In the shown example the large step size aggrevates the problem. For SGD a careful tuning of the learning rate is therefore especially important. Optimizers like Adam can adress this problem by adjusting the step size based on previous gradients. A reparameterization of the parameters space on the other hand could lead to problem way better suited for SGD.\nSo instead of using the euclidean metric $||\\Theta||_2$ a distance measure for an infinitesimal vector $\\text{d}\\Theta$ on a curved manifold is given by $$||\\Theta||_{g} = \\sum_{ij}g_{ij}(\\Theta)\\text{d}\\Theta_i\\text{d}\\Theta_j,$$ where $g_{ij}$ is the Riemannian metric tensor.\nFor every physicist this seems very familiar. Of course the euclidean metric is the special case of $g_{ij}=\\delta_{ij}$. Using this general metric for the method of steepest descent S. Amari shows in ( Citation: Amari,\u0026#32;1998 Amari,\u0026#32; S. \u0026#32; (1998). \u0026#32;Natural Gradient Works Efficiently in Learning. Neural Computation,\u0026#32;10(2).\u0026#32;251–276. https://doi.org/10.1162/089976698300017746 ) that the gradient descent update rule becomes $$\\Theta_{t+1} := \\Theta_t - \\eta G^{-1}\\nabla\\mathcal{L}(\\Theta)\\big|_{\\Theta_{t+1}}\\tag{1},$$ where $G^{-1}$ is the inverse of the metric $G = (g_{ij})$.\nThe question remains how to determin the metric. In the framework of Information Geometry, instead of considering the parameter space, the optimization is performed on the so called statistical manifold. A statistical manifold is a riemannian manifold, where every point corresponds to a probability function.\nIn our case, we may consider the manifold of likelihoods $p(x|\\Theta)$ for the different possible parameters $\\Theta$. To measure the similarity between two probability distributions there exist different divergences, the most known one beeing the Kullback–Leibler (KL) divergence. For two distribution $p(x)$ and $q(x)$ it is defined as $$D_{KL}(p(x)||q(x)) = \\sum_x p(x)\\log\\left(\\frac{p(x)}{q(x)}\\right)\\tag{2}.$$ Note that formally the KL-divergence is not symmetric an thus is not a proper distance measure. However things work out for infinitesimal distance and thus it can be used to describe the manifold locally ( Citation: Martens,\u0026#32;2014 Martens,\u0026#32; J. \u0026#32; (2014). \u0026#32;New insights and perspectives on the natural gradient method.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1412.1193 ) .\nLet\u0026rsquo;s try to rewrite our gradient update from SGD with the KL-divergence as instead of the euclidean metric: $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta}D_{KL}(q(x|\\Theta)||q(x|\\Theta_t)) \\right]$$ To minimize this expression we set the derivative to zero $$\\nabla \\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t} + \\frac{1}{\\eta}\\nabla D_{KL}\\left(q(x|\\Theta)||q(x|\\Theta_t)\\right)\\bigg|_{\\Theta_{t+1}} = 0\\tag{3}.$$ So to solve this we need the gradient of the KL-divergence, which we will approximate by taylor expanding the $D_{KL}$ around $\\Theta_t$. In the following we denote $D_{KL}(\\Theta||\\Theta_t) := D_{KL}(q(x|\\Theta)||q(x|\\Theta_t))$ for brevity. In second order we obtain $$ \\begin{align*} D_{KL}(\\Theta||\\Theta_t)\\approx D_{KL}(\\Theta_t||\\Theta_t) + \\nabla D_{KL}(\\Theta||\\Theta_t)\\big|_{\\Theta_t}(\\Theta-\\Theta_t)\\\\ +\\frac{1}{2}(\\Theta - \\Theta_t)^T H_{D_{KL}}\\big|_{\\Theta_t} (\\Theta - \\Theta_t), \\end{align*} $$ where $H_{D_{KL}}$ denotes the Hessian with respect to $\\Theta$. The first term obviously vanishes since the divergance for identical distributions is zero. The second becomes zero as well, which we can see if we insert the definition from Eq. $(2)$: $$ \\begin{align*} \\nabla D_{KL}(\\Theta||\\Theta_t)\\big|_{\\Theta_t}=\u0026amp;\\sum_x \\nabla p(\\Theta)\\big|_{\\Theta_t}\\log\\left(\\frac{p(\\Theta_t)}{p(\\Theta_t)}\\right) + p(\\Theta)\\nabla\\log\\left(\\frac{p(\\Theta)}{p(\\Theta_t)}\\right) \\\\ \u0026amp; =\\sum_x \\nabla p(\\Theta) - p(\\Theta) \\nabla\\log\\left(p(\\Theta_t)\\right) = \\nabla 1 = 0. \\end{align*} $$ We can now insert the taylor expression for the KL-divergence in Eq. $(3)$ to obtain $$\\nabla\\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t}+\\frac{1}{\\eta}H_{D_{KL}}\\bigg|_{\\Theta_t}(\\Theta - \\Theta_t)=0,$$ which leads to our update rule $$\\Theta_{t+1} := \\Theta_t - \\eta H^{-1}_{D_{KL}}\\bigg|_{\\Theta_t}\\nabla\\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t} \\tag{4}.$$ Comparing this with Eq. $(1)$ we can identify the metric $G$ with the hessian of the KL-divergence $H_{D_{KL}}$. Rearranging the terms we can bring the hessian of the KL-divergence in the familiar form of the fisher information matrix $${H_{D_{KL}}}_{ij} = g_{ij} = \\sum_x p(x|\\Theta)\\frac{\\partial \\log p(x|\\Theta)}{\\partial \\Theta_i}\\frac{\\partial \\log p(x|\\Theta)}{\\partial \\Theta_j}.$$ The fisher information matrix thus describes the local curvature of the statistical manifold. With Eq. $(4)$ it constitutes the classical natural gradient descent.\nQuantum natural gradient descent The optimization of PQCs is very similar to classical deep learning. We may have a quantum circuit with parameters $\\Theta$. The resulting states for of the circuit for fixed input data define a parametrized hilbert space $\\mathcal{H}(\\Theta)$. We can define a distance measure $d$ between two states with an infinitesimal distance between the parameters $$d\\left(\\ket{\\psi(\\Theta)}, \\ket{\\psi(\\Theta + \\text{d}\\Theta)}\\right) = \\sum_{ij} g_{ij}(\\Theta)\\text{d}\\Theta_i\\text{d}\\Theta_j,$$ where $g_{ij}$ is the Fubini-Study metric ( Citation: Yamamoto,\u0026#32;2019 Yamamoto,\u0026#32; N. \u0026#32; (2019). \u0026#32;On the natural gradient for variational quantum eigensolver.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1909.05074 ) $$\\text{Re}\\left[\\braket{\\partial_i\\psi|\\partial_j\\psi}-\\braket{\\partial_i\\psi|\\psi}\\braket{\\psi|\\partial_j\\psi}\\right],$$ where $\\ket{\\partial_i\\psi}=\\partial\\ket{\\psi(\\Theta)}\\big/\\partial\\Theta_i$. With this metric we can again write out and simplify our formulation of steepest descent to obtain an update rule for the quantum natural gradient descent proposed in ( Citation: Stokes,\u0026#32;Izaac \u0026amp; al.,\u0026#32;2019 Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 ) $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta} \\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_{g(\\Theta_t)} \\right],$$ where using our metric $g(\\Theta)$ we have the norm as the scalar product $$\\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_{g(\\Theta_t)} = \\braket{\\Theta - \\Theta_t, g(\\Theta_t)(\\Theta - \\Theta_t)}.$$ Setting the derivative to zero like before directly leads to $$\\Theta_{t+1} = \\Theta_t - \\eta g^+(\\Theta_t)\\nabla\\mathcal{L}(\\Theta)\\big|_{\\Theta_t}.$$ Here $g^+$ denotes the pseudo-inverso of the metric tensor which is usually calculated as the Moore-Penrose-Inverse.\nComputing the metric tensor can be very expensive, which is why ( Citation: Stokes,\u0026#32;Izaac \u0026amp; al.,\u0026#32;2019 Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 ) proposes to compute a diagonal or block-diagonal approximation of it.\nImplementation Fortunately the computation of the diagonal and block diagonal approximations of the metric tensor are already implemented in Pennylane\nI want to show a little example of the usage of QNG on a real dataset as I struggeled a bit with the implementation of the iteration over data. Suppose you have some circuit which takes the parameters and data as arguments. If you want to train the parameters you define some cost function e.g. a simple MSE\ndef cost(params, x, y): return (y - circuit(params, x)) ** 2 After initializing the parameters params we optimize them by iterating over the training data x_train, y_train and appling steps to the optimizer QNGOptimizer which is implemented in pennylane. To compute the step however, we need the metric tensor function, which is also implemented in pennylane. As the metric tensor function can only be obtained for function with a single argument, namely the parameters to be trained, we need to define a lambda function for every data sample that only depends on the parameters. Same goes for the cost function.\nopt = qml.QNGOptimizer(learning_rate) for it in range(epochs): for j, sample in enumerate(x_train): cost_fn = lambda p: cost_sample(p, sample, y[j]) metric_fn = lambda p: qml.metric_tensor(circuit, approx=\u0026#34;block-diag\u0026#34;)(p, sample) params = opt.step(cost_fn, params, metric_tensor_fn=metric_fn) print(j, end=\u0026#34;\\r\u0026#34;) loss = cost(params) print(f\u0026#34;Epoch: {it} | Loss: {loss} |\u0026#34;) Note that the data needs to be defined in pennylane with requires_grad=False.\nThe QNG can be quite useful in avoiding Barren Plateaus in training. However of course computing the metric tensor takes time, which makes the QNG especially useful for models with a smaller number of parameters.\nAmari (1998) Amari,\u0026#32; S. \u0026#32; (1998). \u0026#32;Natural Gradient Works Efficiently in Learning. Neural Computation,\u0026#32;10(2).\u0026#32;251–276. https://doi.org/10.1162/089976698300017746 Martens (2014) Martens,\u0026#32; J. \u0026#32; (2014). \u0026#32;New insights and perspectives on the natural gradient method.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1412.1193 Stokes,\u0026#32; Izaac,\u0026#32; Killoran\u0026#32;\u0026amp;\u0026#32;Carleo (2019) Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 Yamamoto (2019) Yamamoto,\u0026#32; N. \u0026#32; (2019). \u0026#32;On the natural gradient for variational quantum eigensolver.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1909.05074 ","permalink":"https://tommago.com/posts/qng/","summary":"When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).","title":"Quantum Natural Gradient Descent"},{"content":"In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analogue of a classical autoencoder equally aims to learn a smaller representation of data.\nA naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure. Here the Autoencoder has an input dimension of three and compresses the data to a single qbit.\nEach line represents a single qbit and reading from left to right, consecutive transformations are applied to them. The classical training data is first encoded into a quantum state indicated by $|\\psi\\big\u0026gt;$. This can be done with different encodings e.g. the angle encoding ( Citation: Weigold,\u0026#32;Barzen \u0026amp; al.,\u0026#32;2021 Weigold,\u0026#32; M.,\u0026#32; Barzen,\u0026#32; J.,\u0026#32; Leymann,\u0026#32; F.\u0026#32;\u0026amp;\u0026#32;Salm,\u0026#32; M. \u0026#32; (2021). \u0026#32; Expanding Data Encoding Patterns For Quantum Algorithms. https://doi.org/10.1109/ICSA-C52384.2021.00025 ) . After feeding the data into the first three qbits, an encoding block is applied. The encoder is a parameterized unitary transformation $U(\\Theta)$. It consists of rotations on the bloch sphere and entanglement using CNOT gates. The parameters $\\Theta$ are angles for the rotations which will eventually be learned when training the autoencoder. After applying the encoder, a second parametrized circuit follows, which acts as the decoder. The decoder only overlaps with the encoder at a subset of its qbits, in this example a single one. This qbit acts as the smaller latent space of the autoencoder. The rest of the decoder circuit acts on qbits which were initialized as $|0\\big\u0026gt;$. The goal of an Autoencoder is to learn to reconstruct the input data after compressing it to the latent space. To verify this, we can use a SWAP-test with three reference bits. The reference bits were initialized to the input state of the data as well. The SWAP-test can then measure the similarity of the input data state $|\\psi\\big\u0026gt;$ with the output of the decoder $|\\phi\\big\u0026gt;$. This similarity $F$ is called fidelity $$F(\\ket{\\psi},\\ket{\\phi})=\\big|\\big\u0026lt;\\psi|\\phi\\big\u0026gt;\\big|^2.$$ It can be measured at the readout bit.\nSince we want to learn parameters that enable the autoencoder to reconstruct the input, we can then use the fidelity to construct the loss function $$\\mathcal{L} = 1-\\big|\\big\u0026lt;\\psi|\\phi\\big\u0026gt;\\big|^2$$ to minimize. The loss function can be used to optimize the parameters by gradient descent similar to classical learning.\nAn example for a circuit of a $3\\rightarrow 1 \\rightarrow 3$ autoencoder is shown below.\nIn this example for simplicity the Encoder and Decoder consist of only a single layer using $R_y(\\Theta_i)$ gates and entaglement by CNOT. The SWAP-test is carried out using controlled SWAP-gates on the output of the decoder and the reference bits. The controlling bit is the last qbit which is used to readout the result of the SWAP-test. It is initialized as $|0\\big\u0026gt;$ and prepared with a Hadamard gate, which leads to the state $\\frac{1}{\\sqrt{2}}|0\\big\u0026gt; + \\frac{1}{\\sqrt{2}}|1\\big\u0026gt;$. The controlled SWAP operation on two states $|\\psi\\big\u0026gt;$ and $|\\phi\\big\u0026gt;$ transfers the three qbit system into the state $\\frac{1}{\\sqrt{2}}|0,\\psi,\\phi\\big\u0026gt; + \\frac{1}{\\sqrt{2}}|1,\\phi,\\psi\\big\u0026gt;$. The $|\\psi\\big\u0026gt;$ could thereby be the state of an output qbit of the decoder, e.g. $(0,2)$ and $|\\phi\\big\u0026gt;$ the state of the respective reference bit, e.g. $(0,5)$. Applying another Hadamard gate to the SWAP-qbit transfers the state to $\\frac{1}{2}\\big(|0,\\psi,\\phi\\big\u0026gt; + |1,\\psi,\\phi\\big\u0026gt; + |0,\\phi,\\psi\\big\u0026gt; - |1,\\phi,\\psi\\big\u0026gt;\\big)$. When we now measure the first qbit, which is the auxillary SWAP-qbit $(0,8)$ in the circuit shown above, it will turn out to be $|0\\big\u0026gt;$ with the probability $$P(|0\\big\u0026gt;)=\\frac{1}{4}(\\big\u0026lt;\\psi,\\phi| + \\big\u0026lt;\\phi,\\psi|)(|\\psi,\\phi\\big\u0026gt; + |\\phi,\\psi\\big\u0026gt;) = \\frac{1}{2} + \\frac{1}{2}|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2.$$ The probability of measuring $|1\\big\u0026gt;$ is therefore $$P(|1\\big\u0026gt;) = 1 - P(|0\\big\u0026gt;) = \\frac{1}{2}-\\frac{1}{2}|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2.$$ To obtain the fidelity $|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2$ we measure the SWAP-qbit in the $Z$-basis. Since the eigenvalues of $\\sigma_z$ are $1$ and $-1$ the expectation value of the measurement calculates to $$\\big\u0026lt;q_8|\\sigma_z|q_8\\big\u0026gt; = 1\\cdot P(|0\\big\u0026gt;) + (-1)\\cdot P(|1\\big\u0026gt;) = |\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2,$$ where $|q_8\\big\u0026gt;$ denotes the SWAP-qbit. The output of measuring the last qbit of the shown circuit in the $Z$-basis therefore corresponds to the fidelity between the output of the decoder and the reference qbits.\nA simpler Quantum Autoencoder The naive implementation discussed above can be simplified as shown in ( Citation: Ngairangbam,\u0026#32;Spannowsky \u0026amp; al.,\u0026#32;2021 Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 ) based on ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) . A $3\\rightarrow 1 \\rightarrow 3$ autoencoder circuit without the SWAP test for measuring the fidelity of the output of the decoder is displayed below.\nThe Hilbertspace $\\mathcal{H}=\\mathcal{H}_T\\otimes\\mathcal{H}_A\\otimes\\mathcal{H}_L$ can be written as the product of the subspaces for the so called trash qubits $\\mathcal{H}_T$, the data qubits which are not part of the latent space $\\mathcal{H}_A$ and the compessed state at the latent bits $\\mathcal{H}_L$. In this depiction the trash bits are fed into the decoder by a SWAP operation ${V_T}_A$ between $\\mathcal{H}_A$ and $\\mathcal{H}_T$. The input state to the autoencoder can be written as $$\\ket{\\Psi} = \\ket{\\psi}_{LA} \\otimes \\ket{t}_T,$$ where $\\ket{\\psi}$ is the actual input data and $\\ket{t}$ are the trash states which are initialized as $\\ket{0}$. The output state $\\ket{\\omicron}$ is the result of the unitary transformations and the SWAP applied to the input $$\\ket{\\omicron} = U_{LA}^\\dagger V_{AT} U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T.$$ The Fidelity between the input and the output $F(\\ket{\\Psi},\\ket{\\omicron})$ can be simplified $$ \\begin{align} \\begin{split} F(\\ket{\\Psi},\\ket{\\omicron}) \u0026amp;= \\big|\\braket{\\Psi|\\omicron}\\big|^2 \\\\ \u0026amp;= \\big|\\braket{\\Psi|U^\\dagger_{LA}V_{AT}U_{LA}|\\Psi}\\big|^2 \\\\ \u0026amp;= F\\big(U_{LA}\\ket{\\psi}_{LA} \\otimes \\ket{t}_T, V_{AT} U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T\\big)\\\\ \u0026amp;= F\\big(U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T, U_{LT} \\ket{\\psi}_{LT} \\otimes \\ket{t}_A\\big)\\\\ \\end{split} \\end{align} $$ where in the last line the SWAP $V_{AT}$ exchanged $\\mathcal{H}_A$ and $\\mathcal{H}_T$. We can now see that, if $$U_{LZ} \\ket{\\psi}_{LZ} = \\ket{\\psi_c}_L\\otimes\\ket{t}_Z$$ the fidelity reduces to one. Here $\\ket{\\psi_c}$ denotes a compressed version of the input state. It is therefore sufficient to train the unitarity $U(\\Theta)$ to seperate the trash state $\\ket{t}$. To do so, the decoder, which is the adjoint $U^\\dagger$ is not needed. An example circuit is shown below.\nIn this case the SWAP test measures the fidelity between the trash qubits $\\ket{t}$ and the output bits of the encoder which are not latent bits. As we can see the main advantage of this method is that it needs way fewer qbits and works with a shallower circuit. Nevertheless, it can still be used for compression, as the lower dimensional representation of the input data could be extracted from the qbit $(0,0)$. Moreover, the fidelity used for training can equally be used for anomaly tagging. Another useful advantage is the fact that we can upload more than one feature per qubit as shown in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) , Because we don\u0026rsquo;t have to compre the output of the decoder with the input data.\nIt should be noted, that the two fidelities for the first and the second approach are not identical. According to ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) the fidelity of the naive QAE is always less or equalt to the simpler version.\nCurrently I am using this architecture and experimenting with different encoder circuits, as I am not aware of other fully quantum autoencoder architectures in the literature. It would be interesting to explore to what extend a potential quantum (variational ?) autoencoder could be used for generative purposes, which I will leave for the future.\nNgairangbam,\u0026#32; Spannowsky\u0026#32;\u0026amp;\u0026#32;Takeuchi (2021) Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 Romero,\u0026#32; Olson\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik (2016) Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 Weigold,\u0026#32; Barzen,\u0026#32; Leymann\u0026#32;\u0026amp;\u0026#32;Salm (2021) Weigold,\u0026#32; M.,\u0026#32; Barzen,\u0026#32; J.,\u0026#32; Leymann,\u0026#32; F.\u0026#32;\u0026amp;\u0026#32;Salm,\u0026#32; M. \u0026#32; (2021). \u0026#32; Expanding Data Encoding Patterns For Quantum Algorithms. https://doi.org/10.1109/ICSA-C52384.2021.00025 ","permalink":"https://tommago.com/posts/qae/","summary":"In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analogue of a classical autoencoder equally aims to learn a smaller representation of data.\nA naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure.","title":"Quantum Autoencoder"},{"content":"This year I\u0026rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.,\u0026#32; Shih,\u0026#32; D.,\u0026#32; Amram,\u0026#32; O.,\u0026#32; Andreassen,\u0026#32; A.,\u0026#32; Benkendorfer,\u0026#32; K.,\u0026#32; Bortolato,\u0026#32; B.,\u0026#32; Brooijmans,\u0026#32; G.,\u0026#32; Canelli,\u0026#32; F.,\u0026#32; Collins,\u0026#32; J.,\u0026#32; Dai,\u0026#32; B.,\u0026#32; Freitas,\u0026#32; F.,\u0026#32; Dillon,\u0026#32; B.,\u0026#32; Dinu,\u0026#32; I.,\u0026#32; Dong,\u0026#32; Z.,\u0026#32; Donini,\u0026#32; J.,\u0026#32; Duarte,\u0026#32; J.,\u0026#32; Faroughy,\u0026#32; D.,\u0026#32; Gonski,\u0026#32; J.,\u0026#32; Harris,\u0026#32; P.,\u0026#32; Kahn,\u0026#32; A.,\u0026#32; Kamenik,\u0026#32; J.,\u0026#32; Khosa,\u0026#32; C.,\u0026#32; Komiske,\u0026#32; P.,\u0026#32; Pottier,\u0026#32; L.,\u0026#32; Martín-Ramiro,\u0026#32; P.,\u0026#32; Matevc,\u0026#32; A.,\u0026#32; Metodiev,\u0026#32; E.,\u0026#32; Mikuni,\u0026#32; V.,\u0026#32; Ochoa,\u0026#32; I.,\u0026#32; Park,\u0026#32; S.,\u0026#32; Pierini,\u0026#32; M.,\u0026#32; Rankin,\u0026#32; D.,\u0026#32; Sanz,\u0026#32; V.,\u0026#32; Sarda,\u0026#32; N.,\u0026#32; Seljak,\u0026#32; U.,\u0026#32; Smolkovic,\u0026#32; A.,\u0026#32; Stein,\u0026#32; G.,\u0026#32; Suarez,\u0026#32; C.,\u0026#32; Szewc,\u0026#32; M.,\u0026#32; Thaler,\u0026#32; J.,\u0026#32; Tsan,\u0026#32; S.,\u0026#32; Udrescu,\u0026#32; S.,\u0026#32; Vaslin,\u0026#32; L.,\u0026#32; Vlimant,\u0026#32; J.,\u0026#32; Williams,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Yunus,\u0026#32; M. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 ) . These models are thereby trained on Standard Model data and search for anomalous events that deviate from the known physics. With the rise of NISQ-devices ( Citation: Preskill,\u0026#32;2018 Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ) the question comes up if quantum machine learning can enhance classical machine learning applications to hep problems.\nSince we are encouraged by Google to publicly share our work, I set up this blog to document the project and share some of my scientific interests.\nKasieczka,\u0026#32; Nachman,\u0026#32; Shih,\u0026#32; Amram,\u0026#32; Andreassen,\u0026#32; Benkendorfer,\u0026#32; Bortolato,\u0026#32; Brooijmans,\u0026#32; Canelli,\u0026#32; Collins,\u0026#32; Dai,\u0026#32; Freitas,\u0026#32; Dillon,\u0026#32; Dinu,\u0026#32; Dong,\u0026#32; Donini,\u0026#32; Duarte,\u0026#32; Faroughy,\u0026#32; Gonski,\u0026#32; Harris,\u0026#32; Kahn,\u0026#32; Kamenik,\u0026#32; Khosa,\u0026#32; Komiske,\u0026#32; Pottier,\u0026#32; Martín-Ramiro,\u0026#32; Matevc,\u0026#32; Metodiev,\u0026#32; Mikuni,\u0026#32; Ochoa,\u0026#32; Park,\u0026#32; Pierini,\u0026#32; Rankin,\u0026#32; Sanz,\u0026#32; Sarda,\u0026#32; Seljak,\u0026#32; Smolkovic,\u0026#32; Stein,\u0026#32; Suarez,\u0026#32; Szewc,\u0026#32; Thaler,\u0026#32; Tsan,\u0026#32; Udrescu,\u0026#32; Vaslin,\u0026#32; Vlimant,\u0026#32; Williams\u0026#32;\u0026amp;\u0026#32;Yunus (2021) Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.,\u0026#32; Shih,\u0026#32; D.,\u0026#32; Amram,\u0026#32; O.,\u0026#32; Andreassen,\u0026#32; A.,\u0026#32; Benkendorfer,\u0026#32; K.,\u0026#32; Bortolato,\u0026#32; B.,\u0026#32; Brooijmans,\u0026#32; G.,\u0026#32; Canelli,\u0026#32; F.,\u0026#32; Collins,\u0026#32; J.,\u0026#32; Dai,\u0026#32; B.,\u0026#32; Freitas,\u0026#32; F.,\u0026#32; Dillon,\u0026#32; B.,\u0026#32; Dinu,\u0026#32; I.,\u0026#32; Dong,\u0026#32; Z.,\u0026#32; Donini,\u0026#32; J.,\u0026#32; Duarte,\u0026#32; J.,\u0026#32; Faroughy,\u0026#32; D.,\u0026#32; Gonski,\u0026#32; J.,\u0026#32; Harris,\u0026#32; P.,\u0026#32; Kahn,\u0026#32; A.,\u0026#32; Kamenik,\u0026#32; J.,\u0026#32; Khosa,\u0026#32; C.,\u0026#32; Komiske,\u0026#32; P.,\u0026#32; Pottier,\u0026#32; L.,\u0026#32; Martín-Ramiro,\u0026#32; P.,\u0026#32; Matevc,\u0026#32; A.,\u0026#32; Metodiev,\u0026#32; E.,\u0026#32; Mikuni,\u0026#32; V.,\u0026#32; Ochoa,\u0026#32; I.,\u0026#32; Park,\u0026#32; S.,\u0026#32; Pierini,\u0026#32; M.,\u0026#32; Rankin,\u0026#32; D.,\u0026#32; Sanz,\u0026#32; V.,\u0026#32; Sarda,\u0026#32; N.,\u0026#32; Seljak,\u0026#32; U.,\u0026#32; Smolkovic,\u0026#32; A.,\u0026#32; Stein,\u0026#32; G.,\u0026#32; Suarez,\u0026#32; C.,\u0026#32; Szewc,\u0026#32; M.,\u0026#32; Thaler,\u0026#32; J.,\u0026#32; Tsan,\u0026#32; S.,\u0026#32; Udrescu,\u0026#32; S.,\u0026#32; Vaslin,\u0026#32; L.,\u0026#32; Vlimant,\u0026#32; J.,\u0026#32; Williams,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Yunus,\u0026#32; M. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 Preskill (2018) Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ","permalink":"https://tommago.com/posts/hello/","summary":"This year I\u0026rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.,\u0026#32; Shih,\u0026#32; D.,\u0026#32; Amram,\u0026#32; O.,\u0026#32; Andreassen,\u0026#32; A.,\u0026#32; Benkendorfer,\u0026#32; K.,\u0026#32; Bortolato,\u0026#32; B.,\u0026#32; Brooijmans,\u0026#32; G.,\u0026#32; Canelli,\u0026#32; F.,\u0026#32; Collins,\u0026#32; J.","title":"Hello World! | Hello GSoC!"}]