[{"content":"We recently put out a paper on bottomonium suppression in the quark gluon plasma 2403.15545. This is a project I\u0026rsquo;ve been working on for some time now and I want to show real quick what we have been doing.\nQuarkonium suppression Heavy ion collisions are experiments where two heavy nuclei are collided. Such experiments are conducted e.g. at CERN. In these collisions a state of matter called the quark gluon plasma is created. The quark gluon plasma is a hot liquid like state of matter, where light quarks, the fundamental building blocks of matter do bind to composite particles. Such a condition is assumed to have existed in the early universe shortly after the big bang, where matter was condensed to a tight space.\nThe quark gluon plasma (QGP) only exists for a short amount of time. It quickly cools down and hadronizes, dissolving into bound hadrons. It is therefore hard to access the QGP experimentally as it cant be detected.\nA possible probe was famously proposed by Matui and Satz. The idea is to use heavy quarkonium as a test particle. Heavy quarkonium like charmonium (charm anti-charm quark bound states) or bottomonium (bottom anti-bottom bound states) are produced in the initial stages of the collision. They afterwards travel through the QGP and are recorded in the detector of the experiments after hadronization (the quarkonium decays and the decay prodcuts can be used to reconstruct it).\nNow the idea is that inside the hot QGP the quarkonium \u0026ldquo;melts\u0026rdquo; leading to a lower number of quarkonia measured. One can then see the effect of the QGP by comparing the number of quarkonia in heavy ion collisions to the number in proton-proton collisions. Since no QGP is created in proton-proton collisions, one expects there to be less quarkonia recorded in heavy ion experiemnts due to the melting effect in the QGP.\nThis phenomenon is called quarkonium suppression and we aim to describe it theoretically.\nOpen quantum systems To describe quarkonium suppression we use the formalism of open quantum systems. Here the idea is that usually a quantum system is not perfectly isolated, but instead it interacts with an environment. This interaction of course affects the evolution of the system.\nOne can formalize this by considering the evolution of the full system, including the system $S$ and the environment $E$. The hamiltonian of the joint system is given by $$H = H_S\\otimes 1_E + 1_S\\otimes H_E + H_\\mathrm{int},$$ where $H_\\mathrm{int}$ contains the interaction between system $S$ and environment $E$.\nUsually the evolution of a closed quantum system is given by the von Neumann equation $$\\frac{d}{dt}\\rho(t) = -i[H,\\rho(t)].$$ We apply the von Neumann equation to the full hamiltonian $H$, which acts on the Hilbert space $\\mathcal{H}_S\\otimes \\mathcal{H}_E$. Since we only care about the evolution of our system $\\rho_S$, we afterwards trace out the environmental degrees of freedom to obtain an evolution equation for the reduced density matrix $\\rho_S(t) = \\text{Tr}_E[\\rho(t)]$.\nThis leads to the infamous Lindblad equation $$\\frac{d}{dt}\\rho_S(t) = -i[H_S,\\rho_S(t)] + \\sum_n \\left[C_n\\rho_S(t)C^\\dagger_n - \\frac{1}{2}\\{C^\\dagger_nC_n,\\rho_S(t)\\}\\right].$$ Here the $C_n$ are so called Lindblad or jump operators. They contain contributions from interactions with the environment. They need to be derived for a given problem.\nSolving this equation provides the time evolution of our system under the influence of the environment.\nQuarkonium suppression as an open quantum system In the case of quarkonium suppression, the system $S$ is the quarkonium particle and the environment $E$ is the quark gluon plasma. When the quarkonium travels through the quark gluon plasma, they interact through the strong force, quantum chromodynamics (QCD).\nOne can greatly simplify calculations in QCD by using effective field theories. They exploit the fact that in many problems only certain energy scales are relevant. Using effective field theories of QCD it is possible to derive a Lindblad equation for quarkonium suppression. By solving it, we obtain the time evolution of the quarkonium state. To obtain the survival probability of the quarkonium we then calculate the overlap with the vacuum quarkonium wave function.\nFrome the survival probability one can calculate the nuclear modification factor $R_{AA}$, which can be compared to experimental results.\nThe quarkonium potential A central part of the theoretical description is the quarkonium potential. The quarkonium potential is the potential which confines the quark and the anti-quark together. In a quantum mechanical description one can calculate the mass of quarkonium states by solving the schrödinger equation with the respective quarkonium potential. This gives the so called spectrum, which can be compared to experimental measurements. Now in previous works the Coulomb potential has been used as quarkonium potential. The Coulomb potential is proportional to $1/r$, where $r$ is the distance, and it is well known from e.g. electromagnetism. For quarks, we know however that this potential only gives a very limited description. In QCD, we know that quarks are confined, that means when you try to pull them apart, the attraction becomes stronger and stronger. This is not the case for the Coulomb potential since it vanishes for $r\\to\\infty$. In the latest paper, we therefore investigate the inclusion of a more realistic potential derived from QCD.\nIn the above plot the dashed lines are the Coulomb potentials (attractive and repulsive meaning negative and positive factors) and the solid lines are the new potentials we implemented. One can see that the solid blue line increases for large $r$ hinting at the onset of confinement, which is not present in the Coulomb potential.\nQuarkonium suppression predictions We solve the Lindblad equation by using a Monte Carlo method, which can be scaled to an HPC system. The results are shown below. The bands represent the predicts for the supression factor we obtain.\nOn the $x$-axis we have the number of participating partions in the collision, which is essentially the amount of overlap the two nuclei have during the collission. On the $y$-axis we plot the nuclear modification factor, which gives the suppression compared to proton-proton collisions. As can be seen, we describe the experimental data quite well in this case.\n","permalink":"https://tommago.com/posts/vpert/","summary":"We recently put out a paper on bottomonium suppression in the quark gluon plasma 2403.15545. This is a project I\u0026rsquo;ve been working on for some time now and I want to show real quick what we have been doing.\nQuarkonium suppression Heavy ion collisions are experiments where two heavy nuclei are collided. Such experiments are conducted e.g. at CERN. In these collisions a state of matter called the quark gluon plasma is created.","title":"Bottomonium suppression as an open quantum system"},{"content":"This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data. The full code for all my work can be found on Github. In the following post I will outline my work and describe some parts of the implementation and the results\nEvent generation in HEP experiments In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics. Generating these predictions is usually done by Monte Carlo simulations. Since these simulations are very demaning, there has been a vast amount of work on generative machine learning models e.g. ( Citation: Oliveira,\u0026#32;Paganini \u0026amp; al.,\u0026#32;2017 Oliveira,\u0026#32; L.,\u0026#32; Paganini,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Nachman,\u0026#32; B. \u0026#32; (2017). \u0026#32;Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis. Comput Softw Big Sci (2017) 1: 4. https://doi.org/10.1007/s41781-017-0004-6 ;\u0026#32; Citation: Butter,\u0026#32;Plehn \u0026amp; al.,\u0026#32;2019 Butter,\u0026#32; A.,\u0026#32; Plehn,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Winterhalder,\u0026#32; R. \u0026#32; (2019). \u0026#32;How to GAN LHC Events. SciPost Phys. 7, 075 (2019). https://doi.org/10.21468/SciPostPhys.7.6.075 ;\u0026#32; Citation: Hariri,\u0026#32;Dyachkova \u0026amp; al.,\u0026#32;2021 Hariri,\u0026#32; A.,\u0026#32; Dyachkova,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Gleyzer,\u0026#32; S. \u0026#32; (2021). \u0026#32;Graph Generative Models for Fast Detector Simulations in High Energy Physics.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2104.01725 ) . The main incentive is to speed up the simulation process by training a generative model like a GAN, from which one can then cheaply sample from.\nClassical GANs Generative Adversarial Networks (GANs) are a class of unsupervised machine learning models proposed in ( Citation: Goodfellow,\u0026#32;Pouget-Abadie \u0026amp; al.,\u0026#32;2014 Goodfellow,\u0026#32; I.,\u0026#32; Pouget-Abadie,\u0026#32; J.,\u0026#32; Mirza,\u0026#32; M.,\u0026#32; Xu,\u0026#32; B.,\u0026#32; Warde-Farley,\u0026#32; D.,\u0026#32; Ozair,\u0026#32; S.,\u0026#32; Courville,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Bengio,\u0026#32; Y. \u0026#32; (2014). \u0026#32;Generative Adversarial Networks.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1406.2661 ) . GANs aim to train a generator $G(z,\\Theta_g)$ with a latent space $z$ and parameters $\\Theta_g$ to replicate a reference probability distribution when sampling from the latent space $z$.\nA GAN consists of two networks, the generator $G$ and a discriminator $D$. The networks are trained by playing a zero sum game, where the generator tries to generate samples which are as realistic as possible, while the discriminator tries to classify real data samples and tag samples generated by the generator as fake.\nA more detailed description of classical GANs is given in this post.\nA Quantum GAN for event generation There have been several different proposals for Quantum GANs ( Citation: Lloyd\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32;2018 Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32; C. \u0026#32; (2018). \u0026#32;Quantum generative adversarial learning. Phys. Rev. Lett. 121, 040502 (2018). https://doi.org/10.1103/PhysRevLett.121.040502 ;\u0026#32; Citation: Dallaire-Demers\u0026#32;\u0026amp;\u0026#32;Killoran,\u0026#32;2018 Dallaire-Demers,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Killoran,\u0026#32; N. \u0026#32; (2018). \u0026#32;Quantum generative adversarial networks. Phys. Rev. A 98, 012324 (2018). https://doi.org/10.1103/PhysRevA.98.012324 ;\u0026#32; Citation: Niu,\u0026#32;Zlokapa \u0026amp; al.,\u0026#32;2021 Niu,\u0026#32; M.,\u0026#32; Zlokapa,\u0026#32; A.,\u0026#32; Broughton,\u0026#32; M.,\u0026#32; Boixo,\u0026#32; S.,\u0026#32; Mohseni,\u0026#32; M.,\u0026#32; Smelyanskyi,\u0026#32; V.\u0026#32;\u0026amp;\u0026#32;Neven,\u0026#32; H. \u0026#32; (2021). \u0026#32;Entangling Quantum Generative Adversarial Networks. https://doi.org/10.48550/ARXIV.2105.00080 ) including proposals for applications in HEP simulation ( Citation: Rehm,\u0026#32;Vallecorsa \u0026amp; al.,\u0026#32;2023 Rehm,\u0026#32; F.,\u0026#32; Vallecorsa,\u0026#32; S.,\u0026#32; Grossi,\u0026#32; M.,\u0026#32; Borras,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Krücker,\u0026#32; D. \u0026#32; (2023). \u0026#32;A Full Quantum Generative Adversarial Network Model for High Energy Physics Simulations.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2305.07284 ;\u0026#32; Citation: Rehm,\u0026#32;Vallecorsa \u0026amp; al.,\u0026#32;2023 Rehm,\u0026#32; F.,\u0026#32; Vallecorsa,\u0026#32; S.,\u0026#32; Borras,\u0026#32; K.,\u0026#32; Grossi,\u0026#32; M.,\u0026#32; Krücker,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Varo,\u0026#32; V. \u0026#32; (2023). \u0026#32;Precise Image Generation on Current Noisy Quantum Computing Devices.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2307.05253 ) .\nIn this project I focused on a Hybrid quantum classical model based on ( Citation: Zoufal,\u0026#32;Lucchi \u0026amp; al.,\u0026#32;2019 Zoufal,\u0026#32; C.,\u0026#32; Lucchi,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Woerner,\u0026#32; S. \u0026#32; (2019). \u0026#32;Quantum Generative Adversarial Networks for Learning and Loading Random Distributions. npj Quantum Inf 5, 103 (2019). https://doi.org/10.1038/s41534-019-0223-2 ) . It consists of a quantum circuit acting as generator and a classical discriminator, which recives measured samples as input. Notably, this approach differs from many other approches in the literature, as it does not try to embed the continous data into quantum states through an embedding. We rather try to systematically discretize the data and develop a mapping to basis states of quantum system. The generator can then learn to prepare a quantum state whose distribution of basis states when measured resembles the discretized data.\nAs a toy example, I take a dataset of detector images originating from Quark initiated Jets ( Citation: Andrews,\u0026#32;Alison \u0026amp; al.,\u0026#32;2019 Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 ) and stepwise scale this down to the size of $3\\times 3$ with $M=2$ possible values per pixel. Below I show exemplary, how the average of all data samples scales down from a full $125\\times 125$ detector image.\nAs every possible state needs to be represented by a unique basis state, I represent every pixel with $\\log_2M$ qubits, where $M$ is the number of discrete values a single pixel can take. The total qubits needed to represent an image of size $N\\times N$ with $M$ discrete values per pixel is then $n = N^2\\log_2M$.\nFor our toy example we therefore have $n=9$ pixels. Of course this is a very simplified example, however it can work as a simple benchmark to test our model.\nSampling from the generator we will obtain a measured basis stat which is a list of $0$s and $1$s representing the measured value for every qubit.\nTo map such a basis state $\\ket{q_1q_2\\dots}, q_i = 0,1$ to an image and vice versa, we proceed as following:\nReshape the list $[q_1,q_2,\\dots]$ to $(N,N,\\log_2 M)$ Convert the third dimension from an binary list to an integer Divide by $2^M - 1$ to normalize These images can then be passed into a classical deep neural network of your choice, meaning it is possible to utilize convolutional or graph neural networks as discriminator, as has been done in jet phyics before. Since the generator is trained on adversarially against the discriminator, we expect the generator to be able to generalize, given that the discriminator generalizes well on the data.\nThe training of the hybrid QGAN proceeds as following, for every epoch:\nWe draw $N$ samples $s_i$ of basis states We evaluate the discriminator $D$ at theses samples giving $D(s_i)$ We get the probabilites for every sample beeing drawn by the generator $G$ as $p_i = G(s_i)$ We calculate the generator Loss $\\mathcal{L}_G=\\sum_i p_i \\log D(s_i)$ We perform a step updating the generator parameters using $\\mathcal{L}_G$ We evaluate the discriminator on a batch of real data samples $D(x_i)$ We calculate the discriminator loss as $\\mathcal{L}_D=\\frac{1}{N}\\sum^N_i\\log D(x_i) - \\sum_i p_i \\log D(s_i)$. We perform a step updating the discriminator parameters using $\\mathcal{L}_D$. In the discriminator loss $\\mathcal{L}_D$ the first term corresponds to the discriminator learning the real samples and the second term to learning to spot the fake samples generated by the generator.\nI implemented this learning procedure using pennylane and pytorch. For that, I use a qnode with the pennylane pytorch interface\ndev = qml.device(\u0026#34;default.qubit.torch\u0026#34;, wires=num_qubits) @qml.qnode(dev, interface=\u0026#34;torch\u0026#34;, diff_method=\u0026#34;backprop\u0026#34;, cachesize=1000000) def circuit(inputs, weights): for wire in range(num_qubits): qml.Hadamard(wires=wire) qml.StronglyEntanglingLayers(weights=weights, wires=list(range(num_qubits))) return qml.probs() In this example, I return as measurement the probabilites of the basis states to use for the generator loss function. Note that I have to increase the cachesize for larger ciruits. As a unitarity in this example I use pennylane StronglyEntanglingLayer, though a more hardware efficient ansatz would be advantageous for execution on real hardware.\nTo perform the hybrid training with pytorch, we have to convert the qnode to a torch layer\nweight_shapes = {\u0026#34;weights\u0026#34;: (n_layers, num_qubits,3)} qlayer = qml.qnn.TorchLayer(circuit, weight_shapes) As discriminator we can build a feed forward neural network with the pixel number as input size.\nclass Discriminator(nn.Module): def __init__(self, input_size): super(Discriminator, self).__init__() self.linear_input = nn.Linear(input_size, 50) self.leaky_relu = nn.LeakyReLU(0.2) self.linear1 = nn.Linear(50, 20) self.linear2 = nn.Linear(20, 1) self.sigmoid = nn.Sigmoid() self.flatten = nn.Flatten() def forward(self, inputs): x = self.flatten(inputs) x = self.linear_input(x) x = self.leaky_relu(x) x = self.linear1(x) x = self.leaky_relu(x) x = self.linear2(x) x = self.sigmoid(x) return x generator = qlayer discriminator = Discriminator(N**2) We can now optimize the parameters of these two models using pytorchs standard optimizers and the loss functions described above.\nTraining results The training results of the Qgan are shown below\nWe can see, that the KL divergence and the MSE between the average generated and data image decrease and converge in a controlled manner. Therefore, the learned generator can generate the data distribution fairly well. It is worthwile to note that especially, there is no mode collapse here, as we are, in contrast to many other QGAN proposals, sampling from a quantum state, therefore mode collapse would mean learning a unitarity, which creates exactly a basis state. This is very unlikely and especially would not lead to a minimum in the loss function.\nScaling and prospects I think this is an interesting model for generative tasks on classical data. Of course we need more qubits than alternative models that embed continous data, however since the discretization scales with $\\log_2 M$ in the long run this should not be the limiting factor. I tried applying this model to larger images, however on a classical simulator the requried qubits were too many to simulate reasonably.\nThe main thing I would like to understand now is what advantages Quantum assisted machine learning can bring over classical methods. My main motivation for this model was the complexity theory based argument that by measurement quantum computers can efficiently sample from distributions, which are hard to sample for classical algorithms. However, I would like to understand this point better, develop an insight to what kind of data distribtuions this applies and especially make sure that the classical discriminator is able to learn these distribtuions.\nReferences Andrews,\u0026#32; Alison,\u0026#32; An,\u0026#32; Bryant,\u0026#32; Burkle,\u0026#32; Gleyzer,\u0026#32; Narain,\u0026#32; Paulini,\u0026#32; Poczos\u0026#32;\u0026amp;\u0026#32;Usai (2019) Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 Butter,\u0026#32; Plehn\u0026#32;\u0026amp;\u0026#32;Winterhalder (2019) Butter,\u0026#32; A.,\u0026#32; Plehn,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Winterhalder,\u0026#32; R. \u0026#32; (2019). \u0026#32;How to GAN LHC Events. SciPost Phys. 7, 075 (2019). https://doi.org/10.21468/SciPostPhys.7.6.075 Dallaire-Demers\u0026#32;\u0026amp;\u0026#32;Killoran (2018) Dallaire-Demers,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Killoran,\u0026#32; N. \u0026#32; (2018). \u0026#32;Quantum generative adversarial networks. Phys. Rev. A 98, 012324 (2018). https://doi.org/10.1103/PhysRevA.98.012324 Goodfellow,\u0026#32; Pouget-Abadie,\u0026#32; Mirza,\u0026#32; Xu,\u0026#32; Warde-Farley,\u0026#32; Ozair,\u0026#32; Courville\u0026#32;\u0026amp;\u0026#32;Bengio (2014) Goodfellow,\u0026#32; I.,\u0026#32; Pouget-Abadie,\u0026#32; J.,\u0026#32; Mirza,\u0026#32; M.,\u0026#32; Xu,\u0026#32; B.,\u0026#32; Warde-Farley,\u0026#32; D.,\u0026#32; Ozair,\u0026#32; S.,\u0026#32; Courville,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Bengio,\u0026#32; Y. \u0026#32; (2014). \u0026#32;Generative Adversarial Networks.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1406.2661 Hariri,\u0026#32; Dyachkova\u0026#32;\u0026amp;\u0026#32;Gleyzer (2021) Hariri,\u0026#32; A.,\u0026#32; Dyachkova,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Gleyzer,\u0026#32; S. \u0026#32; (2021). \u0026#32;Graph Generative Models for Fast Detector Simulations in High Energy Physics.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2104.01725 Lloyd\u0026#32;\u0026amp;\u0026#32;Weedbrook (2018) Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32; C. \u0026#32; (2018). \u0026#32;Quantum generative adversarial learning. Phys. Rev. Lett. 121, 040502 (2018). https://doi.org/10.1103/PhysRevLett.121.040502 Niu,\u0026#32; Zlokapa,\u0026#32; Broughton,\u0026#32; Boixo,\u0026#32; Mohseni,\u0026#32; Smelyanskyi\u0026#32;\u0026amp;\u0026#32;Neven (2021) Niu,\u0026#32; M.,\u0026#32; Zlokapa,\u0026#32; A.,\u0026#32; Broughton,\u0026#32; M.,\u0026#32; Boixo,\u0026#32; S.,\u0026#32; Mohseni,\u0026#32; M.,\u0026#32; Smelyanskyi,\u0026#32; V.\u0026#32;\u0026amp;\u0026#32;Neven,\u0026#32; H. \u0026#32; (2021). \u0026#32;Entangling Quantum Generative Adversarial Networks. https://doi.org/10.48550/ARXIV.2105.00080 Oliveira,\u0026#32; Paganini\u0026#32;\u0026amp;\u0026#32;Nachman (2017) Oliveira,\u0026#32; L.,\u0026#32; Paganini,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Nachman,\u0026#32; B. \u0026#32; (2017). \u0026#32;Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis. Comput Softw Big Sci (2017) 1: 4. https://doi.org/10.1007/s41781-017-0004-6 Rehm,\u0026#32; Vallecorsa,\u0026#32; Grossi,\u0026#32; Borras\u0026#32;\u0026amp;\u0026#32;Krücker (2023) Rehm,\u0026#32; F.,\u0026#32; Vallecorsa,\u0026#32; S.,\u0026#32; Grossi,\u0026#32; M.,\u0026#32; Borras,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Krücker,\u0026#32; D. \u0026#32; (2023). \u0026#32;A Full Quantum Generative Adversarial Network Model for High Energy Physics Simulations.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2305.07284 Rehm,\u0026#32; Vallecorsa,\u0026#32; Borras,\u0026#32; Grossi,\u0026#32; Krücker\u0026#32;\u0026amp;\u0026#32;Varo (2023) Rehm,\u0026#32; F.,\u0026#32; Vallecorsa,\u0026#32; S.,\u0026#32; Borras,\u0026#32; K.,\u0026#32; Grossi,\u0026#32; M.,\u0026#32; Krücker,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Varo,\u0026#32; V. \u0026#32; (2023). \u0026#32;Precise Image Generation on Current Noisy Quantum Computing Devices.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2307.05253 Zoufal,\u0026#32; Lucchi\u0026#32;\u0026amp;\u0026#32;Woerner (2019) Zoufal,\u0026#32; C.,\u0026#32; Lucchi,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Woerner,\u0026#32; S. \u0026#32; (2019). \u0026#32;Quantum Generative Adversarial Networks for Learning and Loading Random Distributions. npj Quantum Inf 5, 103 (2019). https://doi.org/10.1038/s41534-019-0223-2 ","permalink":"https://tommago.com/posts/gsoc23/","summary":"This is a summary of my 2023 GSoC project with the ML4SCI-organization. In my project I designed and implemented a Quantum Generative Adversarial Network for the generation of HEP experiment data. The full code for all my work can be found on Github. In the following post I will outline my work and describe some parts of the implementation and the results\nEvent generation in HEP experiments In high energy physics experiements like they are conducted at CERN, an integral part of the analysis process is the comparison of measurements with results expected based on predictions from our theory of nature, the Standard Model of particle physics.","title":"GSoC 23 | Quantum Generative Adversarial Networks for HEP event generation the LHC"},{"content":"This year I\u0026rsquo;m participating in the Google Summer of Code again. Just like last year I\u0026rsquo;m working with the ML4SCI organization. In this years project I am working on Quantum Generative Adversarial Networks.\nGANs Generative Adversarial Networks (GANs) are a class of unsupervised machine learning models proposed in ( Citation: Goodfellow,\u0026#32;Pouget-Abadie \u0026amp; al.,\u0026#32;2014 Goodfellow,\u0026#32; I.,\u0026#32; Pouget-Abadie,\u0026#32; J.,\u0026#32; Mirza,\u0026#32; M.,\u0026#32; Xu,\u0026#32; B.,\u0026#32; Warde-Farley,\u0026#32; D.,\u0026#32; Ozair,\u0026#32; S.,\u0026#32; Courville,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Bengio,\u0026#32; Y. \u0026#32; (2014). \u0026#32;Generative Adversarial Networks.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1406.2661 ) . GANs aim to train a generator $G(z,\\Theta_g)$ with a latent space $z$ and parameters $\\Theta_g$ to replicate a reference probability distribution when sampling from the latent space $z$.\nA GAN consists of two networks, the generator $G$ and a discriminator $D$. The networks are trained by playing a zero sum game, where the generator tries to generate samples which are as realistic as possible, while the discriminator tries to classify real data samples and tag samples generated by the generator as fake.\nA schematic sketch of a GAN is shown below.\nBoth the generator and the discriminator are trained independently by the classification results of the discriminator. If the generator $G(z,\\Theta_g)$ is neural network which maps from the latent space to the space $\\Omega$, then the discriminator $D:\\Omega\\to[0,1]$ classifies the data with $D=1$ corresponding to the discriminator tagging a sample as real and $D=0$ as fake. The objective function $\\mathcal{L}(\\Theta_g, \\Theta_d)$ can then be written as $$\\mathcal{L}(\\Theta_g, \\Theta_d) = E_{x\\sim\\mu_{train}}[\\ln D(x)]+E_{z\\sim\\mu_z}[\\ln(1-D(G(z)))],$$ and the training as a min-max optimization of the form $$\\min_{\\Theta_g}\\max_{\\Theta_d}\\mathcal{L}(\\Theta_g,\\Theta_d).$$ The expecation values $E$ run over the training data and the latent space distribution respectively.\nThis min-max loss can be recast into a different form with two distinct loss functions, one for the discriminator $\\mathcal{L}_D$ and one for the generator $\\mathcal{L}_G$ $$\\mathcal{L}_D(\\Theta_g,\\Theta_d) = E_{z\\sim\\mu_z}[\\ln D(G(z))]+E_{x\\sim x_{train}}[\\ln(1-D(x))],$$ $$\\mathcal{L}_G(\\Theta_g,\\Theta_d) = -E_{z\\sim\\mu_z}[\\ln D(1-G(z))].$$ Both of these are minimized, with respect to their parameters, while freezing the parameters of the opponent network. $$\\min_{\\Theta_d} \\mathcal{L}_D(\\Theta_g,\\Theta_d),$$ $$\\min_{\\Theta_g} \\mathcal{L}_G(\\Theta_g,\\Theta_d).$$\nTo check that this make sense, we can think about the terms in the loss function: The discriminator wants to tag the generators samples as fake ($D(G(z))=0$) and data samples as real ($D(x)=1$). Inserting these values in $\\mathcal{L}_D$ would minimize both terms.\nGANs in hep There has been a vast amount of work on generative models applied to high energy physics tasks, e.g. ( Citation: Oliveira,\u0026#32;Paganini \u0026amp; al.,\u0026#32;2017 Oliveira,\u0026#32; L.,\u0026#32; Paganini,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Nachman,\u0026#32; B. \u0026#32; (2017). \u0026#32;Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis. Comput Softw Big Sci (2017) 1: 4. https://doi.org/10.1007/s41781-017-0004-6 ;\u0026#32; Citation: Butter,\u0026#32;Plehn \u0026amp; al.,\u0026#32;2019 Butter,\u0026#32; A.,\u0026#32; Plehn,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Winterhalder,\u0026#32; R. \u0026#32; (2019). \u0026#32;How to GAN LHC Events. SciPost Phys. 7, 075 (2019). https://doi.org/10.21468/SciPostPhys.7.6.075 ;\u0026#32; Citation: Hariri,\u0026#32;Dyachkova \u0026amp; al.,\u0026#32;2021 Hariri,\u0026#32; A.,\u0026#32; Dyachkova,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Gleyzer,\u0026#32; S. \u0026#32; (2021). \u0026#32;Graph Generative Models for Fast Detector Simulations in High Energy Physics.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2104.01725 ) . The main incentive is to speed up the simulation of particle physics processes by training a GAN, which then cheaply be sampled from.\nIn the typical analysis pipline of a high energy physics (HEP) experiment, one of the computationally most demanding steps is the generation of expected reference data from our assumed theory (the Standard Model of particle physics). Classical event generator rely on Monte-Carlo techniques to sample from the respective event distributions, which is a very demanding step. A GAN can in principle learn the structure even of complex events once and then generate events more efficiently.\nQGAN Building on the success of classical GANs in generative tasks, similar models have been proposed to perform generative tasks on quantum computers ( Citation: Lloyd\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32;2018 Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32; C. \u0026#32; (2018). \u0026#32;Quantum generative adversarial learning. Phys. Rev. Lett. 121, 040502 (2018). https://doi.org/10.1103/PhysRevLett.121.040502 ;\u0026#32; Citation: Dallaire-Demers\u0026#32;\u0026amp;\u0026#32;Killoran,\u0026#32;2018 Dallaire-Demers,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Killoran,\u0026#32; N. \u0026#32; (2018). \u0026#32;Quantum generative adversarial networks. Phys. Rev. A 98, 012324 (2018). https://doi.org/10.1103/PhysRevA.98.012324 ) . There are different motivations for a Quantum Generative Adversarial Network (QGAN), what I find particularly intersting is:\nThe measurement of a quantum system can, under certain assumptions, generate classical data, which can not be generted efficiently by a classical model (based on a classical random number generator) ( Citation: Preskill,\u0026#32;2018 Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ) , which implies a quantum advantage in generative tasks of such distributions. The concept of a QRAM ( Citation: Giovannetti,\u0026#32;Lloyd \u0026amp; al.,\u0026#32;2007 Giovannetti,\u0026#32; V.,\u0026#32; Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Maccone,\u0026#32; L. \u0026#32; (2007). \u0026#32;Quantum random access memory. V. Giovannetti, S. Lloyd, L. Maccone, Phys. Rev. Lett. 100, 160501 (2008).. https://doi.org/10.1103/PhysRevLett.100.160501 ) aims to represent a large data vector of size $N$ in $\\log N$ qubits. Together with the ability of quantum computers to perform maniputlations of sparse and low rank $N\\times N$ matrices with a scaling of $\\mathcal{O}(\\text{poly}(\\log N))$ implies that there is a potential advantage in the scaling of sampling in QGANs ( Citation: Lloyd\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32;2018 Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32; C. \u0026#32; (2018). \u0026#32;Quantum generative adversarial learning. Phys. Rev. Lett. 121, 040502 (2018). https://doi.org/10.1103/PhysRevLett.121.040502 ) From a practical point of view I think QGANs offer interesing applications for e.g. state preperation, to learn a shallower circuit to load an approximation of a probability distribution, instead of deeper exact circuits. QGAN circuit There are many different proposals for QGANs, with both fully quantum architectures, or hybrid models with a classical discriminator network, see e.g. ( Citation: Romero\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32;2019 Romero,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2019). \u0026#32;Variational quantum generators: Generative adversarial quantum machine learning for continuous distributions.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1901.00848 ;\u0026#32; Citation: Tian,\u0026#32;Sun \u0026amp; al.,\u0026#32;2022 Tian,\u0026#32; J.,\u0026#32; Sun,\u0026#32; X.,\u0026#32; Du,\u0026#32; Y.,\u0026#32; Zhao,\u0026#32; S.,\u0026#32; Liu,\u0026#32; Q.,\u0026#32; Zhang,\u0026#32; K.,\u0026#32; Yi,\u0026#32; W.,\u0026#32; Huang,\u0026#32; W.,\u0026#32; Wang,\u0026#32; C.,\u0026#32; Wu,\u0026#32; X.,\u0026#32; Hsieh,\u0026#32; M.,\u0026#32; Liu,\u0026#32; T.,\u0026#32; Yang,\u0026#32; W.\u0026#32;\u0026amp;\u0026#32;Tao,\u0026#32; D. \u0026#32; (2022). \u0026#32;Recent Advances for Quantum Neural Networks in Generative Learning.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2206.03066 ) for reviews.\nThe simplest version of a fully quantum QGAN can be build with a SWAP test as discriminator.\nIn this case the discriminator does not have parameters and therefore, we do not have an adversarial min-max training. Also, we do not have a latent space $z$ to sample from. Instead we just want to train the generator unitarity $G(\\Theta_g)$ to produce a state which is a superposition of the input states $\\sigma_i$ $$G(\\Theta_g)\\ket{0} = \\sum P_i \\sigma_i.$$\nThe generator parameters can be trained by maximizing the fidelity $F(\\sigma,G(\\Theta))$ of the generator $G(\\Theta)$ and the input data $\\sigma$ $$F(\\sigma,G(\\Theta))=\\left|\\braket{\\sigma|G(\\Theta)}\\right|^2.$$ In practice I use the following loss function for minimization $$\\mathcal{L}(\\Theta_g) = -\\log\\left(G(\\Theta_g)\\epsilon\\right),$$ with some small regularization $\\epsilon$.\nMeasuring the generetor circuit would then correspond to sampling from the data distribution. While I perfom this simple training on a noiseless simulator, ( Citation: Niu,\u0026#32;Zlokapa \u0026amp; al.,\u0026#32;2021 Niu,\u0026#32; M.,\u0026#32; Zlokapa,\u0026#32; A.,\u0026#32; Broughton,\u0026#32; M.,\u0026#32; Boixo,\u0026#32; S.,\u0026#32; Mohseni,\u0026#32; M.,\u0026#32; Smelyanskyi,\u0026#32; V.\u0026#32;\u0026amp;\u0026#32;Neven,\u0026#32; H. \u0026#32; (2021). \u0026#32;Entangling Quantum Generative Adversarial Networks. https://doi.org/10.48550/ARXIV.2105.00080 ) shows that adding parameters $\\Theta_d$ to the SWAP test, making the fidelity loss \u0026ldquo;imperfect\u0026rdquo;, can make the training more robust to device noise.\nImplementing a simple QGAN (toy data) As a simple toy example I want to train a QGAN to load a gaussian peak. So I start off by generating a toy dataset drawing $N=120$ integers between $0$ and $15$ from a normal distribution with $\\mu=7$ and $\\sigma=1.5$.\nTo load the data, I convert the integers to 4 bit values and encode them in quantum states of a four qubit quantum register. To implement the quantum circuits and the optimization I use the pennylane library. The for the generator circuit, I use a strongly entangled layer. The code for the training circuit is given below.\ndev = qml.device(\u0026#39;lightning.qubit\u0026#39;, wires=9) def num_circuit(num, wires): # Cast numberst to binary bin_str = format(num, \u0026#39;#06b\u0026#39;)[2:] # Apply X to appropiate wires for i, c in enumerate(bin_str): if c == \u0026#39;1\u0026#39;: qml.PauliX(wires=i+wires[0]) def generator(params_g, qubits): qml.StronglyEntanglingLayers(weights=params_g, wires=qubits) @qml.qnode(dev) def training_circ(data, params_g): # Real data num_circuit(data, [1,2,3,4]) # Generator circuit generator(params_g, [5,6,7,8]) # SWAP test qml.Hadamard(wires=0) qml.CSWAP(wires=[0,1,5]) qml.CSWAP(wires=[0,2,6]) qml.CSWAP(wires=[0,3,7]) qml.CSWAP(wires=[0,4,8]) qml.Hadamard(wires=0) return qml.expval(qml.PauliZ(0)) To perform the training, I loop over the data samples and optimize the parameters using Adam.\n# The first dimension corresponds to the number of layers in the generator # Since we need some expressiveness a higher number will give better results params_g = np.random.uniform(0,np.pi, size=(18,4,3), requires_grad=True) epochs = 110 batch_size=16 learning_rate=0.01 def iterate_minibatches(data, batch_size): for start_idx in range(0, data.shape[0] - batch_size + 1, batch_size): idxs = slice(start_idx, start_idx + batch_size) yield data[idxs] def cost_batch(paramsg, paramsd, batch, reg=0.000001): loss = 0.0 for i in batch: f = training_circ(i, paramsg) + reg loss += - np.log(f) return loss / len(batch) # Training loop for it in range(epochs): for j,Xbatch in enumerate(iterate_minibatches(data, batch_size=batch_size)): cost_fn = lambda p: cost_batch(p, Xbatch) params_g = optg.step(cost_fn, params_g) print(j, end=\u0026#34;\\r\u0026#34;) loss = cost_batch(params_g, data) print(f\u0026#34;Epoch: {it} | Loss: {loss:.3} | \u0026#34;) print(\u0026#34;____\u0026#34;) Performing the optimization until convergence sets the generator parameters. We can then define a circuit which only contains the generator and sample in the computational basis.\nsample_dev = qml.device(\u0026#39;lightning.qubit\u0026#39;, wires=4, shots = N) @qml.qnode(sample_dev) def sample_test(): generator(paramsg, [0,1,2,3]) return qml.sample() testresult = [int(\u0026#39;\u0026#39;.join(str(i) for i in a), 2) for a in sample_test()] If we convert the computational basis results back to integers we can check how well the generator aprroximates our data distribution.\nOutlook For further work, I\u0026rsquo;m interested in a couple of things. First I want to see how complex the distributions can be which I can train in this matter. Then I want to implement the same model with a classical discriminator and compare the training with the fully quantum one. Finally, I want to extend the model to a continous value QGAN by using an embedding for the continous input data, and add a latent space $z$ to the generator. In this way it should be possible to sample from a continous distribution, by taking an expectatin value over a number of shots when drawing from the generator.\nReferences Butter,\u0026#32; Plehn\u0026#32;\u0026amp;\u0026#32;Winterhalder (2019) Butter,\u0026#32; A.,\u0026#32; Plehn,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Winterhalder,\u0026#32; R. \u0026#32; (2019). \u0026#32;How to GAN LHC Events. SciPost Phys. 7, 075 (2019). https://doi.org/10.21468/SciPostPhys.7.6.075 Dallaire-Demers\u0026#32;\u0026amp;\u0026#32;Killoran (2018) Dallaire-Demers,\u0026#32; P.\u0026#32;\u0026amp;\u0026#32;Killoran,\u0026#32; N. \u0026#32; (2018). \u0026#32;Quantum generative adversarial networks. Phys. Rev. A 98, 012324 (2018). https://doi.org/10.1103/PhysRevA.98.012324 Giovannetti,\u0026#32; Lloyd\u0026#32;\u0026amp;\u0026#32;Maccone (2007) Giovannetti,\u0026#32; V.,\u0026#32; Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Maccone,\u0026#32; L. \u0026#32; (2007). \u0026#32;Quantum random access memory. V. Giovannetti, S. Lloyd, L. Maccone, Phys. Rev. Lett. 100, 160501 (2008).. https://doi.org/10.1103/PhysRevLett.100.160501 Goodfellow,\u0026#32; Pouget-Abadie,\u0026#32; Mirza,\u0026#32; Xu,\u0026#32; Warde-Farley,\u0026#32; Ozair,\u0026#32; Courville\u0026#32;\u0026amp;\u0026#32;Bengio (2014) Goodfellow,\u0026#32; I.,\u0026#32; Pouget-Abadie,\u0026#32; J.,\u0026#32; Mirza,\u0026#32; M.,\u0026#32; Xu,\u0026#32; B.,\u0026#32; Warde-Farley,\u0026#32; D.,\u0026#32; Ozair,\u0026#32; S.,\u0026#32; Courville,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Bengio,\u0026#32; Y. \u0026#32; (2014). \u0026#32;Generative Adversarial Networks.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1406.2661 Hariri,\u0026#32; Dyachkova\u0026#32;\u0026amp;\u0026#32;Gleyzer (2021) Hariri,\u0026#32; A.,\u0026#32; Dyachkova,\u0026#32; D.\u0026#32;\u0026amp;\u0026#32;Gleyzer,\u0026#32; S. \u0026#32; (2021). \u0026#32;Graph Generative Models for Fast Detector Simulations in High Energy Physics.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2104.01725 Lloyd\u0026#32;\u0026amp;\u0026#32;Weedbrook (2018) Lloyd,\u0026#32; S.\u0026#32;\u0026amp;\u0026#32;Weedbrook,\u0026#32; C. \u0026#32; (2018). \u0026#32;Quantum generative adversarial learning. Phys. Rev. Lett. 121, 040502 (2018). https://doi.org/10.1103/PhysRevLett.121.040502 Niu,\u0026#32; Zlokapa,\u0026#32; Broughton,\u0026#32; Boixo,\u0026#32; Mohseni,\u0026#32; Smelyanskyi\u0026#32;\u0026amp;\u0026#32;Neven (2021) Niu,\u0026#32; M.,\u0026#32; Zlokapa,\u0026#32; A.,\u0026#32; Broughton,\u0026#32; M.,\u0026#32; Boixo,\u0026#32; S.,\u0026#32; Mohseni,\u0026#32; M.,\u0026#32; Smelyanskyi,\u0026#32; V.\u0026#32;\u0026amp;\u0026#32;Neven,\u0026#32; H. \u0026#32; (2021). \u0026#32;Entangling Quantum Generative Adversarial Networks. https://doi.org/10.48550/ARXIV.2105.00080 Oliveira,\u0026#32; Paganini\u0026#32;\u0026amp;\u0026#32;Nachman (2017) Oliveira,\u0026#32; L.,\u0026#32; Paganini,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Nachman,\u0026#32; B. \u0026#32; (2017). \u0026#32;Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis. Comput Softw Big Sci (2017) 1: 4. https://doi.org/10.1007/s41781-017-0004-6 Preskill (2018) Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 Romero\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik (2019) Romero,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2019). \u0026#32;Variational quantum generators: Generative adversarial quantum machine learning for continuous distributions.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1901.00848 Tian,\u0026#32; Sun,\u0026#32; Du,\u0026#32; Zhao,\u0026#32; Liu,\u0026#32; Zhang,\u0026#32; Yi,\u0026#32; Huang,\u0026#32; Wang,\u0026#32; Wu,\u0026#32; Hsieh,\u0026#32; Liu,\u0026#32; Yang\u0026#32;\u0026amp;\u0026#32;Tao (2022) Tian,\u0026#32; J.,\u0026#32; Sun,\u0026#32; X.,\u0026#32; Du,\u0026#32; Y.,\u0026#32; Zhao,\u0026#32; S.,\u0026#32; Liu,\u0026#32; Q.,\u0026#32; Zhang,\u0026#32; K.,\u0026#32; Yi,\u0026#32; W.,\u0026#32; Huang,\u0026#32; W.,\u0026#32; Wang,\u0026#32; C.,\u0026#32; Wu,\u0026#32; X.,\u0026#32; Hsieh,\u0026#32; M.,\u0026#32; Liu,\u0026#32; T.,\u0026#32; Yang,\u0026#32; W.\u0026#32;\u0026amp;\u0026#32;Tao,\u0026#32; D. \u0026#32; (2022). \u0026#32;Recent Advances for Quantum Neural Networks in Generative Learning.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2206.03066 ","permalink":"https://tommago.com/posts/qgan/","summary":"This year I\u0026rsquo;m participating in the Google Summer of Code again. Just like last year I\u0026rsquo;m working with the ML4SCI organization. In this years project I am working on Quantum Generative Adversarial Networks.\nGANs Generative Adversarial Networks (GANs) are a class of unsupervised machine learning models proposed in ( Citation: Goodfellow,\u0026#32;Pouget-Abadie \u0026amp; al.,\u0026#32;2014 Goodfellow,\u0026#32; I.,\u0026#32; Pouget-Abadie,\u0026#32; J.,\u0026#32; Mirza,\u0026#32; M.,\u0026#32; Xu,\u0026#32; B.,\u0026#32; Warde-Farley,\u0026#32; D.,\u0026#32; Ozair,\u0026#32; S.,\u0026#32; Courville,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Bengio,\u0026#32; Y. \u0026#32; (2014). \u0026#32;Generative Adversarial Networks.","title":"Quantum GANs"},{"content":"The past month I have been participating in the NERSC Open Hackathon hosted together with NVIDIA. Throughout the event we had access to the Perlmutter compute system and worked together with mentors on scaling our scientific software projects on GPUs. During the event I worked on scaling the training of VQCs in Pennylane to multiple GPUs. A word of thanks goes to the organizers and all the mentors who helped us throghout the event.\nScaling quantum simulations During my GSoC project I worked on training QML models for HEP analysis tasks. In the process, a major bottleneck in validating current QML models were the long training times and the inability of simulating larger qbit systems. My goal for the hackathon was therefore to\nEnable scaling to larger qbit systems Enable faster training for medium sized qbit systems The simulation of quantum circuits can be performed on GPUs, with a promising implementation beeing the NVIDIA cuQuantum SDK. In previous benchmarks I found cuQuantum outperforming the simulation on comparable CPUs for qbit numbers of approximately $N\\geq 20$. An especially interesting feature is provided by the cuQuantum Appliance, as it is able to handle the splitting of the state vector across gpus. Since the state vector of an $N$ qubit system scales with $2^N$, it quickly supasses the memory of current GPUs when moving beyond $N=30$. Therefore splitting it across multiple gpus becomes mandatory to simulate larger systems.\nSetup: Pennylane + Cirq + cuQuantum Up until now I have been using the PennyLane-Lightning-GPU Plugin. This plugin is build on cuQuantum and conveniently enables to execute the circuits on the GPU. However, lightning.gpu has a limited multi-GPU support, as it only enables the execution of different measurements in a circuit on multiple GPUs. Especially it does not support the splitting of the state vector. Manual methods like circuit cutting can circumvent this, however I was looking for a more generic technical solution.\nThe solution we came up with during the hackathon was to use the cuQuantum Appliance, which supports the scaling to multiple GPUs with cirq, together with the PennyLane-Cirq Plugin to integrate it with pennylane. To set up your code this way you have to:\nRun the cuQuantum Appliance Add pennylane and pennylane-cirq with pip install pennylane pennylane-cirq Modify your code with import qsimcirq import pennylane as qml qs_opts = qsimcirq.QSimOptions(cpu_threads=64, verbosity=0, gpu_mode=4) qs = qsimcirq.QSimSimulator(qs_opts) dev1 = qml.device(\u0026#39;cirq.simulator\u0026#39;, wires=NUMBER_QBITS, simulator=qs) Done! You can run your job. However during our work we encountered a bug which resulted in the consoled beeing flooded, so you might consider redirecting the error output with python yourscript.py 2\u0026gt;/dev/null. Of course you will loose all real errors like that. In this example gpu_mode describes the number of gpus to use. In Perlmutter there are four GPUs in a node. Unfortunately to this date, the current release of the cuQuantum Appliance only supports a single node, however a multi-node version already exists as shown in NVIDIAs Blog and is said to release to the public soon.\nBenchmark To validate this is working I used a simple sample script similar to pennylane\u0026rsquo;s gpu benchmark\nimport qsimcirq import pennylane as qml from timeit import default_timer as timer wires = 28 layers = 1 num_runs = 50 GPUs = 4 qs_opts = qsimcirq.QSimOptions(cpu_threads=64, verbosity=0, gpu_mode=GPUs) qs = qsimcirq.QSimSimulator(qs_opts) dev = qml.device(\u0026#39;cirq.simulator\u0026#39;, wires=wires, simulator=qs) @qml.qnode(dev) def circuit(parameters): qml.StronglyEntanglingLayers(weights=parameters, wires=range(wires)) return [qml.expval(qml.PauliZ(i)) for i in range(wires)] shape = qml.StronglyEntanglingLayers.shape(n_layers=layers, n_wires=wires) weights = qml.numpy.random.random(size=shape) timing = [] for t in range(num_runs): start = timer() val = circuit(weights) end = timer() timing.append(end - start) print(\u0026#39;run: \u0026#39;, t, end=\u0026#34;\\r\u0026#34;) print(qml.numpy.mean(timing)) As you can see I only benchmark the execution of circuits.\nBelow I show the results of the benchmark code for different numbers of GPUs.\nAs we can see, this setup has only has a speedup for system above approximately $28$ qubits. Below that, the overhead from splitting the circuit and distributing it between GPUs is too large. I only show systems up to $32$, since larger systems don\u0026rsquo;t fit into a single GPU anymore anyways.\nWhen training VQCs we don\u0026rsquo;t just want to execute circuits, but rather have to differentiate them with respect to the trainable parameters. The by far fastest method is the adjoint differentiation introduced in ( Citation: Jones\u0026#32;\u0026amp;\u0026#32;Gacon,\u0026#32;2020 Jones,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Gacon,\u0026#32; J. \u0026#32; (2020). \u0026#32;Efficient calculation of gradients in classical simulations of variational quantum algorithms.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2009.02823 ) . This method is only applicable to simulation, where it takes advantage of the fact, that we can access the state vector at any point in the circuit. When calculating the derivative with respect to many different parameters we then don\u0026rsquo;t have to simulate the whole circuit every time, but can \u0026lsquo;undo\u0026rsquo; a small number of gates by applying the adjoint and in this way get the derivative to different parameters with a very small effort.\nUnfortunately this method of differentiation is not implemented in cirq, but only here in Pennylane-Lightning-GPU.\nSince the adjoint differentiation is magnitudes faster for larger amounts of trainable parameters, currently a single GPU with adjoint differentiation method is way faster than multi-GPU with e.g. parameter-shift.\nOutlook With regards to our goals, we were able to\nTrain larger qbits systems by splitting the state vector across gpus However, The training of medium sized qbits system is currently faster on a single due to the lack of adjoint differentiation in pennylane-cirq. Therefore the implementation of adjoint differentiation for cirq would be key to enable the scaling of VQC training in pennylane.\nIn addition, I am curious how the multi-node Appliance will perfom. Since the dirstribution across GPUs already has a sizable overhead, I expect the splitting across nodes to be even more expensive. Nevertheless, NVIDIA already demonstrated that the multi-node scaling still results in a worthwile speedup.\nReferences Jones\u0026#32;\u0026amp;\u0026#32;Gacon (2020) Jones,\u0026#32; T.\u0026#32;\u0026amp;\u0026#32;Gacon,\u0026#32; J. \u0026#32; (2020). \u0026#32;Efficient calculation of gradients in classical simulations of variational quantum algorithms.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2009.02823 ","permalink":"https://tommago.com/posts/nersc/","summary":"The past month I have been participating in the NERSC Open Hackathon hosted together with NVIDIA. Throughout the event we had access to the Perlmutter compute system and worked together with mentors on scaling our scientific software projects on GPUs. During the event I worked on scaling the training of VQCs in Pennylane to multiple GPUs. A word of thanks goes to the organizers and all the mentors who helped us throghout the event.","title":"NERSC Open Hackathon 2022 | Multi-GPU quantum circuit simulation in Pennylane"},{"content":"This is a summary of my 2022 GSoC project with ML4SCI. The ML4SCI organization accustoms different projects of machine learning applied to scientific problems, many connected to high-energy physics. A big thank you to Sergei Gleyzer for the supervision and support.\nAbstract The Standard Model of particle physics is a theory that describes the fundamental particles and the interactions between them. While it has extensively been tested and was able to correctly predict experiments to an impressive degree, there are multiple reasons to believe that it cannot be a complete description of nature. In the search for physics beyond the Standard Model (BSM), even though the large hadron collider (LHC) produced a large amount of data, no conclusive evidence of new physics could be found yet. A promising method to uncover new physics in the large amount of data is the use of anomaly detection techniques, which can be used to tag anomalous events. A well-known method of deep anomaly detection is the use of autoencoders, which have been applied to the task of anomaly tagging in HEP data before. In my project study the use of quantum machine learning models for the task of anomaly tagging, to investigate if quantum computers can enhance these analyses.\nThe project In order to apply to GSoC with ML4SCI you have to complete some preliminary screening tasks, and write a proposal for your project. Solving the tasks took some time, but they were very interesting and already prepared well for the upcoming project. You can see my proposal here, if you are interested. Of course, the project deviated a bit from the initial plan, but all in all, I followed the plan outlined in the proposal.\nAnomaly tagging With the large amounts of data produced by the LHC and potentially produced in the future by the HL-LHC, new analysis techniques pose interesting tools to detect new physics. I think the BSM physics is certainly hiding somewhere in the data, uncovering it is just a question of finding the needle in a haystack, due to its elusive nature. I especially like the idea of unsupervised techniques, since it is a way to conduct a model-independent search. Since there are many different BSM models I think model-independent searches make a lot of sense. There has previously been a good amount of research on the application of unsupervised methods to new physics searches, e.g. ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 ;\u0026#32; Citation: Fraser,\u0026#32;Homiller \u0026amp; al.,\u0026#32;2021 Fraser,\u0026#32; K.,\u0026#32; Homiller,\u0026#32; S.,\u0026#32; Mishra,\u0026#32; R.,\u0026#32; Ostdiek,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Schwartz,\u0026#32; M. \u0026#32; (2021). \u0026#32;Challenges for Unsupervised Anomaly Detection in Particle Physics. https://doi.org/10.1007/JHEP03(2022)066 ) . Many studies apply anomaly detection to detector images. Since Autoencoders are one of the most prominent deep anomaly detection models, they have been applied to these anomaly studies as well. I specifically want to highlight ( Citation: Finke,\u0026#32;Krämer \u0026amp; al.,\u0026#32;2021 Finke,\u0026#32; T.,\u0026#32; Krämer,\u0026#32; M.,\u0026#32; Morandini,\u0026#32; A.,\u0026#32; Mück,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Oleksiyuk,\u0026#32; I. \u0026#32; (2021). \u0026#32;Autoencoders for unsupervised anomaly detection in high energy physics. JHEP 06 (2021) 161. https://doi.org/10.1007/JHEP06(2021)161 ) . In their study, the authors use a convolutional autoencoder to tag top quark-initiated jets as anomalous, in samples of QCD-initiated jets. In particular, they highlight, that there is a complexity bias between the QCD and top samples. This refers to the fact, that a convolutional Autoencoder trained on top jets can not tag QCD jets, since the top jets have a higher intrinsic dimensionality, which enables the Autoencoder to work on the \u0026ldquo;easier to reconstruct\u0026rdquo; QCD samples. I think this can definitely be a problem as it sets constraints on the type of new physics we are able to uncover and thus it\u0026rsquo;s an interesting matter to investigate when working on this type of anomaly detection.\nDatasets In the project, I used different datasets, e.g. MNIST for validating ideas and code samples or ECAL images of electrons and photons. However, my main focus was on a dataset of detector images of quark and gluon-initiated jets, which is described in ( Citation: Andrews,\u0026#32;Alison \u0026amp; al.,\u0026#32;2019 Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 ) . The dataset only contains jets of light quarks ($u$, $d$, $s$). The original dataset contains 125x125 images of the electromagnetic calorimeter, hadronic calorimeter and the tracks for every event respectively. Below I show the average images of the three channels for quarks and gluons respectively. Note that I normalized the images by dividing them by the value on the largest pixel respectively. Now I\u0026rsquo;m not completely sure if this is the best way to do it, but since I need the pixel values to be properly distributed in $[0,1]$ this was the most obvious way for me. The HCAL channel was generated at a lower resolution and upscaled to 125x125 resulting in a coarser pixeling than the other channels.\nWe can see, that the gluon-initiated jets show a slightly wider cone, however, the differences are quite small, which makes the differentiation of these jets a very hard task. It becomes even harder if you take a look at the images of individual example events.\nApparently, the images are very sparse, which means only a couple of pixels are activated. In addition, considering the logarithmic scale, most pixels are activated only very weakly, meaning that the majority of the energy in the calorimeters is deposited only in a hand full of pixels, which gives small room for distinctive features. In addition, convolutional networks can struggle with very sparse structures, which can pose difficulties when building robust Autoencoders. Since the quantum circuit simulations are very demanding and my hardware access was limited, I mainly focus on a very reduces version of the dataset. I produced it by center cropping the ECAL images to about 80% and then rescaling it to 12x12. The code for this preprocessing can be found here.\nClassical methods As a very simple benchmark model, I consider a convolutional autoencoder. To get a feeling for the model and training, I first tried an Autoencoder on the 40x40 ECAL dataset. I build the model similar to the one proposed in ( Citation: Finke,\u0026#32;Krämer \u0026amp; al.,\u0026#32;2021 Finke,\u0026#32; T.,\u0026#32; Krämer,\u0026#32; M.,\u0026#32; Morandini,\u0026#32; A.,\u0026#32; Mück,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Oleksiyuk,\u0026#32; I. \u0026#32; (2021). \u0026#32;Autoencoders for unsupervised anomaly detection in high energy physics. JHEP 06 (2021) 161. https://doi.org/10.1007/JHEP06(2021)161 ) , which resulted in about 900k parameters. I Trained the Autoencoder on the quark jet images using AdamW, minimizing the binary cross-entropy. In general, I found the training to be not as straightforward as with other image datasets. The usage of PReLU activation and AdamW seemed to help stabilize the training. As latent space size, I found everything between 25-35 to be suitable. Below are given some quark jet examples from the test set to demonstrate the reconstruction.\nWe can now use the loss (binary cross-entropy) as a discriminating variable to tag gluon jets in the test set. Since the AE was trained on the quark jets, the average loss of a gluon jet image is expected to be higher, which labels it as an anomaly. We can therefore compute the ROC curve of the anomaly tagging. Here I obtained an AUC of about $71$%. Considering that ( Citation: Andrews,\u0026#32;Alison \u0026amp; al.,\u0026#32;2019 Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 ) achieved an AUC of $76$% when training on the full 125x125 ECAL images in a supervised manner, this performance seems reasonably good. However, if we train the Autoencoder on the Gluon jets instead and try to tag quark jets as anomalies, I only obtain an AUC of $29$%. This reflects the complexity bias mentioned before.\nTo have a realistic comparison with the quantum models, I also trained an Autoencoder on the 12x12 dataset. In this case, I also only used 10k images for training and especially constrained the Autoencoder to 3k parameters. Training this Autoencoder until convergence took a couple of hundret epochs and resulted in an AUC of $60$%. In the process of optimizing the models, I used the EMD from ( Citation: Komiske,\u0026#32;Metodiev \u0026amp; al.,\u0026#32;2019 Komiske,\u0026#32; P.,\u0026#32; Metodiev,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Thaler,\u0026#32; J. \u0026#32; (2019). \u0026#32;The Metric Space of Collider Events. Phys. Rev. Lett. 123, 041801 (2019). https://doi.org/10.1103/PhysRevLett.123.041801 ) to judge the reconstruction performance as it seems to capture it a little better than just the loss. Using the EMD as a loss does not seem possible as the implementation as a loss is not differentiable for TensorFlow.\nModels I implemented different models, mainly focusing on a fully quantum model, only consisting of a single parametrized quantum circuit (PQC) and Hybrid models with several classical and quantum layers.\nFully quantum autoencoder My implementaion of the fully quantum autoencoder is based on ( Citation: Ngairangbam,\u0026#32;Spannowsky \u0026amp; al.,\u0026#32;2021 Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 ) and ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) . The architecture consists of some data encoding and trainable PQC which acts as an encoder, followed by a swap test, which fixes the non-latent qubits to the value of some reference bits. This compresses the quantum state to the latent space. A detailed description of the functionality of the Quantum Autoencoder can be found in my post about it.\nOf course, the main question is, how to upload the data and what structure of circuit to use as a trainable layer. I started with a basic angle embedding, where one feature is encoded per qubit. In this encoding the $j$th feature is embedded, by rotating the $j$th qubit with $e^{-i x_j\\sigma_x/2}\\ket{0}$. While this encoding is simple, it only allows a single feature per qubit, which limits the method to datasets with a small number of features or makes it necessary to apply other dimensionality reduction techniques first. Another alternative would be Amplitude Encoding, however, this requires very deep circuits and in prototype implementations I found its performance in the Autoencoder to be very limited. Therefore I drew inspiration from the data re-uploading technique proposed in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) . I discuss the idea in detail in my data re-uploading post, however, in a nutshell, arbitrary dimensional data is uploaded to a single qubit multiple times to mimic a deep neural network with a hidden layer. In order to upload a whole image to a couple of qubits, I chose to upload the image in patches.\nA patch is uploaded to a single qubit, where the pixels $x_i$ of the patch are uploaded with a parametrized unitarity $U(b_i + w_i + x_i)$ with weights and biases as introduced in the data re-uploading. So a 12x12 image can e.g. be divided into $3\\times 3=9$ patches of the size 4x4. In this case, we would upload 16-pixel values on 9 qubits respectively. In the data reuploading spirit, this circuit can be entangled and repeated multiple times to add parameters and build a deeper circuit.\nI trained the model with Adam, maximizing the fidelity of the swap test. Currently, the maximum AUC I achieved is $56.5$%. This result uses 5 data re-uploads which leads to 1440 parameters. I would expect this to improve with more parameters. One question I\u0026rsquo;m still investigating is the best choice of reference states. In the derivation of the Autoencoder, the specific choice of reference states does not matter. However together with the data reuploading as an encoder I think one needs to be careful, because if the reference states are initialized as $\\ket{0}$ the fidelity is maximized if all parameters are zero, creating a pseudo solution.\nHybrid model The hybrid models I build are basically classical layers reducing the dimension of the data and feeding it into a PQC. The qubits get measured to obtain a classical latent space, which is reconstructed into an image by classical layers. Similar models have been proposed before, e.g. in ( Citation: Rivas,\u0026#32;Zhao \u0026amp; al.,\u0026#32;2021 Rivas,\u0026#32; P.,\u0026#32; Zhao,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Orduz,\u0026#32; J. \u0026#32; (2021). \u0026#32; Hybrid Quantum Variational Autoencoders for Representation Learning. https://doi.org/10.1109/CSCI54926.2021.00085 ) .\nIn order to make use of the PQC, I think it makes sense to use the same encoder as the fully quantum autoencoder. This way we can upload a larger image to the PQC and reduce the dimension down to the number of qubits. For a first implementation, I e.g. reduce the dimension of the image with convolutional layers down to 9x9. I then upload the image in patches like in the fully quantum case. However this time I use a kernel size of 3 and a stride of 2 to upload the data in 16 patches. This way I can measure all 16 qubits to obtain the latent space. A smaller latent space would be too small to fully reconstruct the image.\nUnfortunately training this model took very long and I was not able to train it until convergence. However, I achieved an AUC of $57$% without the model converging.\nImplementation details When I started the project I implemented the fully quantum autoencoder in TensorFlow-quantum together with cirq. However, I soon moved to pennylane due to the flexibility when building quantum models and their great plugin system. When you write your code in pennylane you specify a device on which to run your quantum circuits. When simulating the circuits you can e.g. use the lightning simulator, which is a fast simulation framework.\ndev = qml.device(\u0026#39;lightning.qubit\u0026#39;, wires=TOTAL_QBITS) The wires thereby specifies the number of qubits to simulate. When the code is developed in pennylane you can always switch the backend for the simulations without making any changes to the code. In principle, you can also use a real quantum computer e.g. with the pennylane qiskit plugin. One plugin that is especially useful is the lightning.gpu. It enables the simulation of the circuits on GPUs using NVIDIAs cuQuantum framework, which can considerably speed up the simulation when using a larger amount of qubits.\nIn pennylane a circuit is built by successively applying gates to different wires, e.g. the function building the circuit for the fully quantum autoencoder:\ndef circuit(self, params, data): encoder(params, data) qml.Hadamard(wires=total_qbits-1) for i in range(trash_qbits): qml.CSWAP(wires=[total_qbits - 1, latent_qbits + i, data_qbits + i]) qml.Hadamard(wires=total_qbits-1) return qml.expval(qml.PauliZ(total_qbits-1)) This function takes parameters, which are trainable, and data, which is not trainable as arguments. In the encoder function, more gates are applied to upload the data with trainable weights using e.g. Pauli X rotations qml.RX. Thereby total_qubits denotes the total number of qubits the circuit contains, data_qubits the number of qubits the encoder uses, latent_qbits the size of the latent and trash_qubits the number of non-latent space and there also the number of reference qubits. The circuit function can be turned into a Qnode, which is a pennylane object associated with a function that returns an expectation value. Here we measure the qubit containing the result of the swap test in the $\\sigma_z$ basis as described in the QAE post. When a Qnode is created you specify the differentiation method for it.\ncircuit_node = qml.QNode(circuit, dev, diff_method=\u0026#34;adjoint\u0026#34;) The differentiation method describes how the gradients of the parameters are calculated. On a quantum computer, the gradients of a circuit can e.g. be obtained with the parameter shift rule. When simulating however I would usually use the adjoint differentiation, as it is a very fast method.\nYou can combine the circuit simulation and differentiation of pennylane with your machine learning framework of choice, e.g. TensorFlow, Pytorch, or Jax. When using Keras e.g. pennylane already provides a class for turning a Qnode into a Keras layer with\nweight_shapes = {\u0026#34;weights\u0026#34;: (num_params,)} qlayer = qml.qnn.KerasLayer(circuit_node, weight_shapes, output_dim=num_outputs) Note that to my knowledge, this currently only works if the data passed to the qlayer is onedimensional.\nOutlook The next step is to scale the quantum models to more parameters and longer training on more sophisticated hardware. The development and first experiments were performed on retail hardware which is not suitable for larger simulations. I am curious to see if the quantum models can achieve the same or even a better AUC when simulating the quantum models with the same number of parameters and training for the same amount of epochs as the classical reference model. Furthermore, we should try to train the models on larger images as there is of course a large loss of information when scaling the images down to 12x12.\nApart from the computational bottleneck, there are still a couple of questions that are not fully answered yet. I mainly want to investigate the effects of different initializations of the reference qubits and of different latent space sizes. Furthermore one could also try other entangling schemes than the simple circular entangling topology.\nSince we have tried vision transformer-based architectures for supervised tasks on jet images I also implemented a quantum vision transformer. To do so I replaced the self-attention layer in a simple ViT with the quantum self-attention proposed in ( Citation: Li,\u0026#32;Zhao \u0026amp; al.,\u0026#32;2022 Li,\u0026#32; G.,\u0026#32; Zhao,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; X. \u0026#32; (2022). \u0026#32;Quantum Self-Attention Neural Networks for Text Classification.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2205.05625 ) . By now I was not able to train the model, again, due to a lack of resources, however, I hope to be able to run it in the future.\nAll in all, it was a fun experience and I\u0026rsquo;m looking forward to seeing what QML will be able to achieve in HEP in the future.\nReferences Rivas,\u0026#32; Zhao\u0026#32;\u0026amp;\u0026#32;Orduz (2021) Rivas,\u0026#32; P.,\u0026#32; Zhao,\u0026#32; L.\u0026#32;\u0026amp;\u0026#32;Orduz,\u0026#32; J. \u0026#32; (2021). \u0026#32; Hybrid Quantum Variational Autoencoders for Representation Learning. https://doi.org/10.1109/CSCI54926.2021.00085 Andrews,\u0026#32; Alison,\u0026#32; An,\u0026#32; Bryant,\u0026#32; Burkle,\u0026#32; Gleyzer,\u0026#32; Narain,\u0026#32; Paulini,\u0026#32; Poczos\u0026#32;\u0026amp;\u0026#32;Usai (2019) Andrews,\u0026#32; M.,\u0026#32; Alison,\u0026#32; J.,\u0026#32; An,\u0026#32; S.,\u0026#32; Bryant,\u0026#32; P.,\u0026#32; Burkle,\u0026#32; B.,\u0026#32; Gleyzer,\u0026#32; S.,\u0026#32; Narain,\u0026#32; M.,\u0026#32; Paulini,\u0026#32; M.,\u0026#32; Poczos,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Usai,\u0026#32; E. \u0026#32; (2019). \u0026#32;End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data. Nucl. Instrum. Methods Phys. Res. A 977, 164304 (2020). https://doi.org/10.1016/j.nima.2020.164304 Finke,\u0026#32; Krämer,\u0026#32; Morandini,\u0026#32; Mück\u0026#32;\u0026amp;\u0026#32;Oleksiyuk (2021) Finke,\u0026#32; T.,\u0026#32; Krämer,\u0026#32; M.,\u0026#32; Morandini,\u0026#32; A.,\u0026#32; Mück,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Oleksiyuk,\u0026#32; I. \u0026#32; (2021). \u0026#32;Autoencoders for unsupervised anomaly detection in high energy physics. JHEP 06 (2021) 161. https://doi.org/10.1007/JHEP06(2021)161 Fraser,\u0026#32; Homiller,\u0026#32; Mishra,\u0026#32; Ostdiek\u0026#32;\u0026amp;\u0026#32;Schwartz (2021) Fraser,\u0026#32; K.,\u0026#32; Homiller,\u0026#32; S.,\u0026#32; Mishra,\u0026#32; R.,\u0026#32; Ostdiek,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;Schwartz,\u0026#32; M. \u0026#32; (2021). \u0026#32;Challenges for Unsupervised Anomaly Detection in Particle Physics. https://doi.org/10.1007/JHEP03(2022)066 Kasieczka,\u0026#32; Nachman\u0026#32;\u0026amp;\u0026#32;al. (2021) Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 Komiske,\u0026#32; Metodiev\u0026#32;\u0026amp;\u0026#32;Thaler (2019) Komiske,\u0026#32; P.,\u0026#32; Metodiev,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Thaler,\u0026#32; J. \u0026#32; (2019). \u0026#32;The Metric Space of Collider Events. Phys. Rev. Lett. 123, 041801 (2019). https://doi.org/10.1103/PhysRevLett.123.041801 Li,\u0026#32; Zhao\u0026#32;\u0026amp;\u0026#32;Wang (2022) Li,\u0026#32; G.,\u0026#32; Zhao,\u0026#32; X.\u0026#32;\u0026amp;\u0026#32;Wang,\u0026#32; X. \u0026#32; (2022). \u0026#32;Quantum Self-Attention Neural Networks for Text Classification.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/2205.05625 Ngairangbam,\u0026#32; Spannowsky\u0026#32;\u0026amp;\u0026#32;Takeuchi (2021) Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 Romero,\u0026#32; Olson\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik (2016) Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ","permalink":"https://tommago.com/posts/gsoc/","summary":"This is a summary of my 2022 GSoC project with ML4SCI. The ML4SCI organization accustoms different projects of machine learning applied to scientific problems, many connected to high-energy physics. A big thank you to Sergei Gleyzer for the supervision and support.\nAbstract The Standard Model of particle physics is a theory that describes the fundamental particles and the interactions between them. While it has extensively been tested and was able to correctly predict experiments to an impressive degree, there are multiple reasons to believe that it cannot be a complete description of nature.","title":"GSoC 22 | Quantum Autoencoders for HEP Analysis at the LHC"},{"content":"An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.\nUniversal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions. Considering a classification problem, we might have functions $f: I_m \\to \\Reals$, where $I_m = [0,1]^m$. The output of a neural network with a single hidden layer may be written as $$h(\\vec{x}) = \\sum^{N}_{i=1}\\alpha_i\\sigma(\\vec{w}_i\\vec{x} + b_i), \\tag{1}$$ where $\\vec{w}_i$ and $b_i$ are the weights and biases of the hidden layer and $\\alpha_i$ the weights of the output layer. The function $\\sigma$ is the non-linear activation function. The function $h$ being dense in the continuous functions $f$ means, that for every $\\epsilon$ we can choose the parameters in Eq. $(1)$ so that $$|h(\\vec{x}) - f(\\vec{x})| \u0026lt; \\epsilon \\ \\ \\text{for all} \\ \\ \\vec{x}.$$ This is a very powerful statement and enables neural networks to tackle very complex problems.\nA proof for this theorem if $\\sigma$ is a sigmoidal function, which means $\\lim_{x\\to\\infty}\\sigma(x)=1$ and $\\lim_{x\\to -\\infty}\\sigma(x)=0$, can be found in ( Citation: Cybenko,\u0026#32;1989 Cybenko,\u0026#32; G. \u0026#32; (1989). \u0026#32;Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems,\u0026#32;2(4).\u0026#32;303–314. https://doi.org/10.1007/BF02551274 ) . The proof basically works by contradiction: If the space of all functions $h$ denoted by $S$ is not all of the continuous functions $f: I_m \\to \\Reals$ denoted by $C(I_m)$, then there is a linear functional $L: C(I_m) \\to \\Reals$ with $L(S)=0$ (or more accurately the closure of $S$). The functional can be written as integral over a function $h$ with respect to some measure $\\mu$. Since $L(S)=0$, which follows from the Hanh-Banach theorem, the integral over our neural network function $h$ with respect to the measure $\\mu$ would vanish. However one can show that for a sigmoidal function $\\sigma$ integrals over terms of the form Eq. $(1)$ are non-zero for all non-zero measures, leading to the contradiction.\nThere are many variations of this theorem, especially ( Citation: Hornik,\u0026#32;1991 Hornik,\u0026#32; K. \u0026#32; (1991). \u0026#32;Approximation capabilities of multilayer feedforward networks. Neural Networks,\u0026#32;4(2).\u0026#32;251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T ) provides a proof dropping the sigmoidal requirement. This generalization applies to any nonconstant continuous activation function, which is bounded.\nUniversal Quantum Circuit Approximation In their paper ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) the authors show that a similar proof can be done for the approximation capabilities of a PQC with a single qubit. Let\u0026rsquo;s consider some data $\\vec{x}\\in\\Reals^n$ we want to classify. The data follows some classification function $f: \\Reals^n \\to O$ we want to approximate. In the simple case of binary classification, we might have $O=\\{0,1\\}$. The idea proposed in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) is to subsequently apply parametrized gates with trainable weights and data uploads, effectively uploading the data many times. Hence the name data re-uploading. In their paper, the authors describe the motivation for the data re-uploading to be that classical neural networks effectively copy the input data when processing it. An example of a neural network with a single hidden layer is shown below.\nWhen e.g. passing the input data to the first hidden layer, the data is effectively passed to every unit of the hidden layer separately, thus \u0026ldquo;copied\u0026rdquo;. In Quantum Mechanics, however, there is the No-Cloning-Theorem, which states that there is no unitarity $U$ that clones arbitrary input states. This can be seen when assuming two states $\\ket{\\phi}$ and $\\ket{\\psi}$ which should be copied independently to a state $\\ket{c}$. The unitarity $U$ should therefore fulfill $$ \\begin{align*} \u0026amp;\\ket{S_1} = U(\\ket{\\phi,c}) = \\ket{\\phi,\\phi},\\\\ \u0026amp;\\ket{S_2} = U(\\ket{\\psi,c}) = \\ket{\\psi,\\psi}, \\end{align*} $$ in order to universally clone the input states. Here we denote $\\ket{\\phi}\\otimes\\ket{c} = \\ket{\\phi,c}$. The scalar product of the cloned states $\\ket{S_1}$ and $\\ket{S_2}$ can be written as $$ \\begin{align*} \\braket{S_1|S_2} =\u0026amp; \\bra{\\phi,c}U^\\dagger U \\ket{\\psi,c} = \\braket{\\phi,c|\\psi,c} = \\braket{\\phi,\\phi|\\psi,\\psi} \\\\ =\u0026amp; \\braket{\\phi|\\psi}\\braket{\\phi|\\psi} = \\braket{\\phi|\\psi}\\braket{k|k}. \\end{align*} $$ Since $\\braket{k|k}=1$ we have $$ \\begin{align*} \\braket{\\phi|\\psi}^2 = \\braket{\\phi|\\psi}, \\end{align*} $$ which is solved by $\\braket{\\phi|\\psi}=1$ and $\\braket{\\phi|\\psi}=0$. In the first case, the two states $\\phi$ and $\\psi$ are identical, which we don\u0026rsquo;t want, since we want to clone different states with the same unitarity. In the second case, the two states are orthogonal. The unitarity $U$ is therefore only able to clone orthogonal states. Non-orthogonal states can\u0026rsquo;t be copied without some information loss.\nTo mimic the copying of input data to hidden notes, as it happens in classical neural networks, the authors, therefore, propose to upload the input data multiple times to a single qubit. An example of a DRC is sketched below.\nA single gate can be understood as a single perceptron, with the unitarity as activation function.\nTo investigate the capabilities of this circuit, we consider a general unitary transformation $U(\\phi_1, \\phi_2, \\phi_3)\\in\\text{SU}(2)$. We can use this unitarity to upload data with $U(\\vec{x})$ or apply transformations with trainable parameters $\\vec{\\phi}$. In the case of data with only three features $\\vec{x}\\in\\Reals^3$, we would construct the data re-uploading circuit (DRC) with depth $N$ as $$\\ket{m} = U(\\vec{\\phi}_N)U(\\vec{x}) \\cdots U(\\vec{\\phi}_1)U(\\vec{x})\\ket{0}.$$ After applying the gates, the qubit can be measured to access the state which arises from the PQC. To reduce the number of gates and thus the depth of the circuit, we can incorporate the data upload and the parameters in a single gate. A single processing unit as analogy to a single unit in a neural network is then written as $$L_i = U(\\vec{b}_i + \\vec{w}_i\\odot\\vec{x}),$$ where $w$ are the weights, $b$ the biases and $\\odot$ denotes the elementwise Hadamard product. This already looks like the output of a single neuron! The classifier then becomes $$\\ket{m} = L_N \\cdots L_1\\ket{0}.$$\nData with an arbitrary number of features can be treated by padding the data with zeros, so that the number of features is a multiple of three and then uploading the three-dimensional feature vectors $\\vec{x}_j$ successively. In this case, a single processing unit $L_i$ is given as $$L_i = U(\\vec{b}^{(k)}_i + \\vec{w}^{(k)}_i \\odot \\vec{x}^{(k)}) \\cdots U(\\vec{b}^{(1)}_i + \\vec{w}^{(1)}_i \\odot \\vec{x}^{(1)}).$$\nTo see that this expression can approximate any function, we insert an explicit representation of $U(\\vec{\\phi})$ and summarize all transformations. For the general unitarity, we use $$U(\\vec{\\phi}) = \\mathrm{e}^{i\\phi_2\\sigma_z} \\mathrm{e}^{i\\phi_1\\sigma_y} \\mathrm{e}^{i\\phi_3\\sigma_z},$$ where we abbreviate $\\vec{\\phi} = \\vec{b} + \\vec{w}\\odot\\vec{x}$. This is equal to $$U(\\vec{\\phi}) = \\mathrm{e}^{i(w_1(\\vec{\\phi})\\sigma_x + w_2(\\vec{\\phi})\\sigma_y + w_3(\\vec{\\phi})\\sigma_z)},$$ with $$ \\begin{align} w_1(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_2(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\cos\\left(\\frac{\\phi_2 - \\phi_3}{2}\\right)\\sin\\left(\\frac{\\phi_1}{2}\\right),\\\\ w_3(\\vec{\\phi}) = \\frac{d}{\\sqrt{1 - \\cos^2d}} \\sin\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right), \\end{align} $$ where $d = \\arccos\\left(\\cos\\left(\\frac{\\phi_2 + \\phi_3}{2}\\right)\\cos\\left(\\frac{\\phi_1}{2}\\right)\\right)$.\nA DRC with $N$ processing units can now be written as $$\\mathcal{U} := \\prod^N_{j=1} \\mathrm{e}^{i(w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z)}.$$ Here the index of the $\\phi$ denotes the index of the different weights and biases. The product of Pauli-matrix exponentials can be simplified using the Baker-Campbell-Hausdorff formula $$\\mathcal{U} = \\exp\\left(i\\sum^N_{j_1}\\left[w_1(\\vec{\\phi}_j)\\sigma_x + w_2(\\vec{\\phi}_j)\\sigma_y + w_3(\\vec{\\phi}_j)\\sigma_z\\right] + \\mathcal{O}_\\text{corr}\\right).$$\nThe correction term $\\mathcal{O}_\\text{corr}$ proportional to commutators of Pauli-matrices. The sum of the $w(\\vec{\\phi})$ terms can now be rewritten. Since all $w_i(\\vec{\\phi})$ are trigonometric functions, which are bounded to $[-1,1]$ and continuous, we can use the general version of the Universal Approximation Theorem and use the sum over $w_i(\\vec{b}_i + \\vec{w}_i \\odot \\vec{x})$ to approximate some continuous function $f_i(\\vec{x})$ just like in the classical case $$\\sum^N_{j=1}w_i(\\vec{b}_j + \\vec{w}_j \\odot \\vec{x}) = f_i(\\vec{x}).$$\nSince the correction terms are proportional to pauli matrices, ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) argues, that they can be absorbed into the functions $f(\\vec{x})$.\nBy optimizing the parameters with a classical optimization scheme, we can therefore approximate any function in terms of the final state (theoretically only with an infinite number of data re-uploads of course). To perform the optimization and classification, we can measure the qubit at the end of the circuit and compare the outcome with states which are predefined for the different classes. This way, it is possible to perform binary classification, but also multiclass problems can be treated by defining a label state for every class.\nThis approach can be extended to multi-qubit classifiers by entangling the different qubits. I think, the data re-uploading approach definitely looks quite promising, since it proves the capabilities of QML. In addition, it enables us to upload larger amounts of data on fewer qubits. Furthermore, it seems to improve the robustness against noise see e.g. ( Citation: Easom-Mccaldin,\u0026#32;Bouridane \u0026amp; al.,\u0026#32;2021 Easom-Mccaldin,\u0026#32; P.,\u0026#32; Bouridane,\u0026#32; A.,\u0026#32; Belatreche,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Jiang,\u0026#32; R. \u0026#32; (2021). \u0026#32;On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access,\u0026#32;9.\u0026#32;65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 ) . Currently, I am using DRCs in my autoencoders. In the future, I aim to explore different entangle schemes for multi-qubit DRCs.\nEasom-Mccaldin,\u0026#32; Bouridane,\u0026#32; Belatreche\u0026#32;\u0026amp;\u0026#32;Jiang (2021) Easom-Mccaldin,\u0026#32; P.,\u0026#32; Bouridane,\u0026#32; A.,\u0026#32; Belatreche,\u0026#32; A.\u0026#32;\u0026amp;\u0026#32;Jiang,\u0026#32; R. \u0026#32; (2021). \u0026#32;On Depth, Robustness and Performance Using the Data Re-Uploading Single-Qubit Classifier. IEEE Access,\u0026#32;9.\u0026#32;65127–65139. https://doi.org/10.1109/ACCESS.2021.3075492 Cybenko (1989) Cybenko,\u0026#32; G. \u0026#32; (1989). \u0026#32;Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems,\u0026#32;2(4).\u0026#32;303–314. https://doi.org/10.1007/BF02551274 Hornik (1991) Hornik,\u0026#32; K. \u0026#32; (1991). \u0026#32;Approximation capabilities of multilayer feedforward networks. Neural Networks,\u0026#32;4(2).\u0026#32;251–257. https://doi.org/https://doi.org/10.1016/0893-6080(91)90009-T Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ","permalink":"https://tommago.com/posts/drc/","summary":"An important motivation for deep learning was the Universal Approximation Theorem which shows, that neural networks can theoretically approximate any function. When it comes to quantum machine learning, a similar statement can be made. Surprisingly a single qubit is sufficient, to perform the classification of arbitrary data distributions.\nUniversal Approximation Theorem The Universal Approximation Theorem (there are many versions with different constraints) states that the functions which can be expressed by a neural network with a single hidden layer and arbitrarily many units are dense in the space of continuous functions.","title":"Data re-uploading"},{"content":"When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).\nNatural Gradient Descent When minimizing a cost function $\\mathcal{L}(\\Theta)$ the well-known gradient descent iteratively updates the parameters $\\Theta$ by descending into the direction of the gradient $$\\Theta_{t+1} := \\Theta_t - \\eta \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}.$$ Here and in the following all gradients are calculated with respect to $\\Theta$ In Stochastic Gradient Descent (SGD) specifically the gradient $\\nabla \\mathcal{L}(\\Theta)$ is approximated by the gradient of the cost function over a subset of the training data. With this update rule, gradient descent implicitly assumes a euclidean geometry of the parameter space. This can be seen when writing the update rule as $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta} \\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_2 \\right],$$ where a proximity term is added, just like in the lagrangian of a spring mass. The equivalence to the gradient descent update rule can immediately be seen when solving the $\\argmin$ by setting the derivative equal to zero.\nThe choice of euclidean geometry does however not necessarily reflect the actual parameter space. Since it gives equal weight to all parameters $\\Theta_i$ ill-conditioned situations can arise as e.g. shown below.\nThe algorithm bounces over the valley and only slowly approaches the minimum. In the shown example the large step size aggravates the problem. For SGD a careful tuning of the learning rate is therefore especially important. Optimizers like Adam can address this problem by adjusting the step size based on previous gradients. A reparameterization of the parameters space on the other hand could lead to a problem way better suited for SGD.\nSo instead of using the euclidean metric $||\\Theta||_2$ a distance measure for an infinitesimal vector $\\text{d}\\Theta$ on a curved manifold is given by $$||\\Theta||_{g} = \\sum_{ij}g_{ij}(\\Theta)\\text{d}\\Theta_i\\text{d}\\Theta_j,$$ where $g_{ij}$ is the Riemannian metric tensor.\nFor every physicist, this seems very familiar. Of course, the euclidean metric is the special case of $g_{ij}=\\delta_{ij}$. Using this general metric for the method of steepest descent S. Amari shows in ( Citation: Amari,\u0026#32;1998 Amari,\u0026#32; S. \u0026#32; (1998). \u0026#32;Natural Gradient Works Efficiently in Learning. Neural Computation,\u0026#32;10(2).\u0026#32;251–276. https://doi.org/10.1162/089976698300017746 ) that the gradient descent update rule becomes $$\\Theta_{t+1} := \\Theta_t - \\eta G^{-1}\\nabla\\mathcal{L}(\\Theta)\\big|_{\\Theta_{t+1}}\\tag{1},$$ where $G^{-1}$ is the inverse of the metric $G = (g_{ij})$.\nThe question remains on how to determine the metric. In the framework of Information Geometry, instead of considering the parameter space, the optimization is performed on the so-called statistical manifold. A statistical manifold is a Riemannian manifold, where every point corresponds to a probability function.\nIn our case, we may consider the manifold of likelihoods $p(x|\\Theta)$ for the different possible parameters $\\Theta$. To measure the similarity between two probability distributions there exist different divergences, the most known one being the Kullback–Leibler (KL) divergence. For two distribution $p(x)$ and $q(x)$ it is defined as $$D_{KL}(p(x)||q(x)) = \\sum_x p(x)\\log\\left(\\frac{p(x)}{q(x)}\\right)\\tag{2}.$$ Note that formally the KL-divergence is not symmetric and thus is not a proper distance measure. However, things work out for infinitesimal distance and thus it can be used to describe the manifold locally ( Citation: Martens,\u0026#32;2014 Martens,\u0026#32; J. \u0026#32; (2014). \u0026#32;New insights and perspectives on the natural gradient method.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1412.1193 ) .\nLet\u0026rsquo;s try to rewrite our gradient update from SGD with the KL-divergence instead of the euclidean metric: $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta}D_{KL}(q(x|\\Theta)||q(x|\\Theta_t)) \\right]$$ To minimize this expression we set the derivative to zero $$\\nabla \\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t} + \\frac{1}{\\eta}\\nabla D_{KL}\\left(q(x|\\Theta)||q(x|\\Theta_t)\\right)\\bigg|_{\\Theta_{t+1}} = 0\\tag{3}.$$ So to solve this we need the gradient of the KL-divergence, which we will approximate by Taylor expanding the $D_{KL}$ around $\\Theta_t$. In the following we denote $D_{KL}(\\Theta||\\Theta_t) := D_{KL}(q(x|\\Theta)||q(x|\\Theta_t))$ for brevity. In second order we obtain $$ \\begin{align*} D_{KL}(\\Theta||\\Theta_t)\\approx D_{KL}(\\Theta_t||\\Theta_t) + \\nabla D_{KL}(\\Theta||\\Theta_t)\\big|_{\\Theta_t}(\\Theta-\\Theta_t)\\\\ +\\frac{1}{2}(\\Theta - \\Theta_t)^T H_{D_{KL}}\\big|_{\\Theta_t} (\\Theta - \\Theta_t), \\end{align*} $$ where $H_{D_{KL}}$ denotes the Hessian with respect to $\\Theta$. The first term obviously vanishes since the divergence for identical distributions is zero. The second becomes zero as well, which we can see if we insert the definition from Eq. $(2)$: $$ \\begin{align*} \\nabla D_{KL}(\\Theta||\\Theta_t)\\big|_{\\Theta_t}=\u0026amp;\\sum_x \\nabla p(\\Theta)\\big|_{\\Theta_t}\\log\\left(\\frac{p(\\Theta_t)}{p(\\Theta_t)}\\right) + p(\\Theta)\\nabla\\log\\left(\\frac{p(\\Theta)}{p(\\Theta_t)}\\right) \\\\ \u0026amp; =\\sum_x \\nabla p(\\Theta) - p(\\Theta) \\nabla\\log\\left(p(\\Theta_t)\\right) = \\nabla 1 = 0. \\end{align*} $$ We can now insert the Taylor expression for the KL-divergence in Eq. $(3)$ to obtain $$\\nabla\\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t}+\\frac{1}{\\eta}H_{D_{KL}}\\bigg|_{\\Theta_t}(\\Theta - \\Theta_t)=0,$$ which leads to our update-rule $$\\Theta_{t+1} := \\Theta_t - \\eta H^{-1}_{D_{KL}}\\bigg|_{\\Theta_t}\\nabla\\mathcal{L}(\\Theta)\\bigg|_{\\Theta_t} \\tag{4}.$$ Comparing this with Eq. $(1)$ we can identify the metric $G$ with the hessian of the KL-divergence $H_{D_{KL}}$. Rearranging the terms we can bring the hessian of the KL-divergence in the familiar form of the fisher information matrix $${H_{D_{KL}}}_{ij} = g_{ij} = \\sum_x p(x|\\Theta)\\frac{\\partial \\log p(x|\\Theta)}{\\partial \\Theta_i}\\frac{\\partial \\log p(x|\\Theta)}{\\partial \\Theta_j}.$$ The fisher information matrix thus describes the local curvature of the statistical manifold. With Eq. $(4)$ it constitutes the classical natural gradient descent.\nQuantum natural gradient descent The optimization of PQCs is very similar to classical deep learning. We may have a quantum circuit with parameters $\\Theta$. The resulting states of the circuit for fixed input data define a parametrized Hilbert space $\\mathcal{H}(\\Theta)$. We can define a distance measure $d$ between two states with an infinitesimal distance between the parameters $$d\\left(\\ket{\\psi(\\Theta)}, \\ket{\\psi(\\Theta + \\text{d}\\Theta)}\\right) = \\sum_{ij} g_{ij}(\\Theta)\\text{d}\\Theta_i\\text{d}\\Theta_j,$$ where $g_{ij}$ is the Fubini-Study metric ( Citation: Yamamoto,\u0026#32;2019 Yamamoto,\u0026#32; N. \u0026#32; (2019). \u0026#32;On the natural gradient for variational quantum eigensolver.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1909.05074 ) $$\\text{Re}\\left[\\braket{\\partial_i\\psi|\\partial_j\\psi}-\\braket{\\partial_i\\psi|\\psi}\\braket{\\psi|\\partial_j\\psi}\\right],$$ where $\\ket{\\partial_i\\psi}=\\partial\\ket{\\psi(\\Theta)}\\big/\\partial\\Theta_i$. With this metric, we can again write out and simplify our formulation of steepest descent to obtain an update rule for the quantum natural gradient descent proposed in ( Citation: Stokes,\u0026#32;Izaac \u0026amp; al.,\u0026#32;2019 Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 ) $$\\Theta_{t+1} := \\argmin_\\Theta \\left[\\big\u0026lt;\\Theta - \\Theta_t, \\nabla \\mathcal{L}(\\Theta)\\big|_{\\Theta_t}\\big\u0026gt; + \\frac{1}{2\\eta} \\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_{g(\\Theta_t)} \\right],$$ where using our metric $g(\\Theta)$ we have the norm as the scalar product $$\\big|\\big|\\Theta-\\Theta_t\\big|\\big|^2_{g(\\Theta_t)} = \\braket{\\Theta - \\Theta_t, g(\\Theta_t)(\\Theta - \\Theta_t)}.$$ Setting the derivative to zero like before directly leads to $$\\Theta_{t+1} = \\Theta_t - \\eta g^+(\\Theta_t)\\nabla\\mathcal{L}(\\Theta)\\big|_{\\Theta_t}.$$ Here $g^+$ denotes the pseudo-inverse of the metric tensor which is usually calculated as the Moore-Penrose-Inverse.\nComputing the metric tensor can be very expensive, which is why ( Citation: Stokes,\u0026#32;Izaac \u0026amp; al.,\u0026#32;2019 Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 ) proposes to compute a diagonal or block-diagonal approximation of it.\nImplementation Fortunately, the computation of the diagonal and block diagonal approximations of the metric tensor are already implemented in Pennylane\nI want to show a little example of the usage of QNG on a real dataset as I struggled a bit with the implementation of the iteration over data. Suppose you have some circuit that takes the parameters and data as arguments. If you want to train the parameters you define some cost function e.g. a simple MSE\ndef cost(params, x, y): return (y - circuit(params, x)) ** 2 After initializing the parameters params we optimize them by iterating over the training data x_train, y_train and applying steps to the optimizer QNGOptimizer which is implemented in pennylane. To compute the step, however, we need the metric tensor function, which is also implemented in pennylane. As the metric tensor function can only be obtained for function with a single argument, namely the parameters to be trained, we need to define a lambda function for every data sample that only depends on the parameters. The same goes for the cost function.\nopt = qml.QNGOptimizer(learning_rate) for it in range(epochs): for j, sample in enumerate(x_train): cost_fn = lambda p: cost_sample(p, sample, y[j]) metric_fn = lambda p: qml.metric_tensor(circuit, approx=\u0026#34;block-diag\u0026#34;)(p, sample) params = opt.step(cost_fn, params, metric_tensor_fn=metric_fn) print(j, end=\u0026#34;\\r\u0026#34;) loss = cost(params) print(f\u0026#34;Epoch: {it} | Loss: {loss} |\u0026#34;) Note that the data needs to be defined in pennylane with requires_grad=False.\nThe QNG can be quite useful in avoiding Barren Plateaus in training. However of course computing the metric tensor takes time, which makes the QNG especially useful for models with a smaller number of parameters.\nAmari (1998) Amari,\u0026#32; S. \u0026#32; (1998). \u0026#32;Natural Gradient Works Efficiently in Learning. Neural Computation,\u0026#32;10(2).\u0026#32;251–276. https://doi.org/10.1162/089976698300017746 Martens (2014) Martens,\u0026#32; J. \u0026#32; (2014). \u0026#32;New insights and perspectives on the natural gradient method.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1412.1193 Stokes,\u0026#32; Izaac,\u0026#32; Killoran\u0026#32;\u0026amp;\u0026#32;Carleo (2019) Stokes,\u0026#32; J.,\u0026#32; Izaac,\u0026#32; J.,\u0026#32; Killoran,\u0026#32; N.\u0026#32;\u0026amp;\u0026#32;Carleo,\u0026#32; G. \u0026#32; (2019). \u0026#32;Quantum Natural Gradient. Quantum 4, 269 (2020). https://doi.org/10.22331/q-2020-05-25-269 Yamamoto (2019) Yamamoto,\u0026#32; N. \u0026#32; (2019). \u0026#32;On the natural gradient for variational quantum eigensolver.\u0026#32;Retrieved from\u0026#32; http://arxiv.org/abs/1909.05074 ","permalink":"https://tommago.com/posts/qng/","summary":"When training Variational Quantum Algorithms we aim to find a point in the parameter space that minimizes a particular cost function, just like in the case of classical deep learning. Using the parameter-shift rule, we are able to compute the gradient of a Parametrized Quantum Circuit (PQC) and can therefore use that gradient descent method proven in classical machine learning. However vanilla gradient descent can face difficulties in practical training which can be circumvented with Quantum Natural Gradient Descent (QNG).","title":"Quantum Natural Gradient Descent"},{"content":"In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analog of a classical autoencoder equally aims to learn a smaller representation of data.\nA naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure. Here the Autoencoder has an input dimension of three and compresses the data to a single qbit.\nEach line represents a single qbit and reading from left to right, consecutive transformations are applied to them. The classical training data is first encoded into a quantum state indicated by $|\\psi\\big\u0026gt;$. This can be done with different encodings e.g. the angle encoding ( Citation: Weigold,\u0026#32;Barzen \u0026amp; al.,\u0026#32;2021 Weigold,\u0026#32; M.,\u0026#32; Barzen,\u0026#32; J.,\u0026#32; Leymann,\u0026#32; F.\u0026#32;\u0026amp;\u0026#32;Salm,\u0026#32; M. \u0026#32; (2021). \u0026#32; Expanding Data Encoding Patterns For Quantum Algorithms. https://doi.org/10.1109/ICSA-C52384.2021.00025 ) . After feeding the data into the first three qbits, an encoding block is applied. The encoder is a parameterized unitary transformation $U(\\Theta)$. It consists of rotations on the Bloch sphere and entanglement using CNOT gates. The parameters $\\Theta$ are angles for the rotations which will eventually be learned when training the autoencoder. After applying the encoder, a second parametrized circuit follows, which acts as the decoder. The decoder only overlaps with the encoder at a subset of its qbits, in this example a single one. This qbit acts as the smaller latent space of the autoencoder. The rest of the decoder circuit acts on qbits that were initialized as $|0\\big\u0026gt;$. The goal of an Autoencoder is to learn to reconstruct the input data after compressing it to the latent space. To verify this, we can use a SWAP-test with three reference bits. The reference bits were initialized to the input state of the data as well. The SWAP-test can then measure the similarity of the input data state $|\\psi\\big\u0026gt;$ with the output of the decoder $|\\phi\\big\u0026gt;$. This similarity $F$ is called fidelity $$F(\\ket{\\psi},\\ket{\\phi})=\\big|\\big\u0026lt;\\psi|\\phi\\big\u0026gt;\\big|^2.$$ It can be measured at the readout bit.\nSince we want to learn parameters that enable the autoencoder to reconstruct the input, we can then use the fidelity to construct the loss function $$\\mathcal{L} = 1-\\big|\\big\u0026lt;\\psi|\\phi\\big\u0026gt;\\big|^2$$ to minimize. The loss function can be used to optimize the parameters by gradient descent similar to classical learning.\nAn example for a circuit of a $3\\rightarrow 1 \\rightarrow 3$ autoencoder is shown below.\nIn this example for simplicity, the Encoder and Decoder consist of only a single layer using $R_y(\\Theta_i)$ gates and entanglement by CNOT. The SWAP-test is carried out using controlled SWAP-gates on the output of the decoder and the reference bits. The controlling bit is the last qbit which is used to read out the result of the SWAP-test. It is initialized as $|0\\big\u0026gt;$ and prepared with a Hadamard gate, which leads to the state $\\frac{1}{\\sqrt{2}}|0\\big\u0026gt; + \\frac{1}{\\sqrt{2}}|1\\big\u0026gt;$. The controlled SWAP operation on two states $|\\psi\\big\u0026gt;$ and $|\\phi\\big\u0026gt;$ transfers the three qbit system into the state $\\frac{1}{\\sqrt{2}}|0,\\psi,\\phi\\big\u0026gt; + \\frac{1}{\\sqrt{2}}|1,\\phi,\\psi\\big\u0026gt;$. The $|\\psi\\big\u0026gt;$ could thereby be the state of an output qbit of the decoder, e.g. $(0,2)$ and $|\\phi\\big\u0026gt;$ the state of the respective reference bit, e.g. $(0,5)$. Applying another Hadamard gate to the SWAP-qbit transfers the state to $\\frac{1}{2}\\big(|0,\\psi,\\phi\\big\u0026gt; + |1,\\psi,\\phi\\big\u0026gt; + |0,\\phi,\\psi\\big\u0026gt; - |1,\\phi,\\psi\\big\u0026gt;\\big)$. When we now measure the first qbit, which is the auxiliary SWAP-qbit $(0,8)$ in the circuit shown above, it will turn out to be $|0\\big\u0026gt;$ with the probability $$P(|0\\big\u0026gt;)=\\frac{1}{4}(\\big\u0026lt;\\psi,\\phi| + \\big\u0026lt;\\phi,\\psi|)(|\\psi,\\phi\\big\u0026gt; + |\\phi,\\psi\\big\u0026gt;) = \\frac{1}{2} + \\frac{1}{2}|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2.$$ The probability of measuring $|1\\big\u0026gt;$ is therefore $$P(|1\\big\u0026gt;) = 1 - P(|0\\big\u0026gt;) = \\frac{1}{2}-\\frac{1}{2}|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2.$$ To obtain the fidelity $|\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2$ we measure the SWAP-qbit in the $Z$-basis. Since the eigenvalues of $\\sigma_z$ are $1$ and $-1$ the expectation value of the measurement calculates to $$\\big\u0026lt;q_8|\\sigma_z|q_8\\big\u0026gt; = 1\\cdot P(|0\\big\u0026gt;) + (-1)\\cdot P(|1\\big\u0026gt;) = |\\big\u0026lt;\\phi|\\psi\\big\u0026gt;|^2,$$ where $|q_8\\big\u0026gt;$ denotes the SWAP-qbit. The output of measuring the last qbit of the shown circuit in the $Z$-basis, therefore, corresponds to the fidelity between the output of the decoder and the reference qbits.\nA simpler Quantum Autoencoder The naive implementation discussed above can be simplified as shown in ( Citation: Ngairangbam,\u0026#32;Spannowsky \u0026amp; al.,\u0026#32;2021 Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 ) based on ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) . A $3\\rightarrow 1 \\rightarrow 3$ autoencoder circuit without the SWAP test for measuring the fidelity of the output of the decoder is displayed below.\nThe Hilbert space $\\mathcal{H}=\\mathcal{H}_T\\otimes\\mathcal{H}_A\\otimes\\mathcal{H}_L$ can be written as the product of the subspaces for the so-called trash qubits $\\mathcal{H}_T$, the data qubits which are not part of the latent space $\\mathcal{H}_A$ and the compressed state at the latent bits $\\mathcal{H}_L$. In this depiction, the trash bits are fed into the decoder by a SWAP operation ${V_T}_A$ between $\\mathcal{H}_A$ and $\\mathcal{H}_T$. The input state to the autoencoder can be written as $$\\ket{\\Psi} = \\ket{\\psi}_{LA} \\otimes \\ket{t}_T,$$ where $\\ket{\\psi}$ is the actual input data and $\\ket{t}$ are the trash states which are initialized as $\\ket{0}$. The output state $\\ket{\\omicron}$ is the result of the unitary transformations and the SWAP applied to the input $$\\ket{\\omicron} = U_{LA}^\\dagger V_{AT} U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T.$$ The Fidelity between the input and the output $F(\\ket{\\Psi},\\ket{\\omicron})$ can be simplified $$ \\begin{align} \\begin{split} F(\\ket{\\Psi},\\ket{\\omicron}) \u0026amp;= \\big|\\braket{\\Psi|\\omicron}\\big|^2 \\\\ \u0026amp;= \\big|\\braket{\\Psi|U^\\dagger_{LA}V_{AT}U_{LA}|\\Psi}\\big|^2 \\\\ \u0026amp;= F\\big(U_{LA}\\ket{\\psi}_{LA} \\otimes \\ket{t}_T, V_{AT} U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T\\big)\\\\ \u0026amp;= F\\big(U_{LA} \\ket{\\psi}_{LA} \\otimes \\ket{t}_T, U_{LT} \\ket{\\psi}_{LT} \\otimes \\ket{t}_A\\big)\\\\ \\end{split} \\end{align} $$ where in the last line the SWAP $V_{AT}$ exchanged $\\mathcal{H}_A$ and $\\mathcal{H}_T$. We can now see that if $$U_{LZ} \\ket{\\psi}_{LZ} = \\ket{\\psi_c}_L\\otimes\\ket{t}_Z$$ the fidelity reduces to one. Here $\\ket{\\psi_c}$ denotes a compressed version of the input state. It is therefore sufficient to train the unitarity $U(\\Theta)$ to separate the trash state $\\ket{t}$. To do so, the decoder, which is the adjoint $U^\\dagger$ is not needed. An example circuit is shown below.\nIn this case, the SWAP test measures the fidelity between the trash qubits $\\ket{t}$ and the output bits of the encoder which are not latent bits. As we can see the main advantage of this method is that it needs way fewer qbits and works with a shallower circuit. Nevertheless, it can still be used for compression, as the lower dimensional representation of the input data could be extracted from the qbit $(0,0)$. Moreover, the fidelity used for training can equally be used for anomaly tagging. Another useful advantage is the fact that we can upload more than one feature per qubit as shown in ( Citation: Pérez-Salinas,\u0026#32;Cervera-Lierta \u0026amp; al.,\u0026#32;2019 Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 ) , Because we don\u0026rsquo;t have to compare the output of the decoder with the input data.\nIt should be noted, that the two fidelities for the first and the second approach are not identical. According to ( Citation: Romero,\u0026#32;Olson \u0026amp; al.,\u0026#32;2016 Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 ) the fidelity of the naive QAE is always less or equal to the simpler version.\nCurrently, I am using this architecture and experimenting with different encoder circuits, as I am not aware of other fully quantum autoencoder architectures in the literature. It would be interesting to explore to what extent a potential quantum (variational ?) autoencoder could be used for generative purposes, which I will leave for the future.\nNgairangbam,\u0026#32; Spannowsky\u0026#32;\u0026amp;\u0026#32;Takeuchi (2021) Ngairangbam,\u0026#32; V.,\u0026#32; Spannowsky,\u0026#32; M.\u0026#32;\u0026amp;\u0026#32;Takeuchi,\u0026#32; M. \u0026#32; (2021). \u0026#32;Anomaly detection in high-energy physics using a quantum autoencoder. Phys. Rev. D 105, 095004, 2022. https://doi.org/10.1103/PhysRevD.105.095004 Pérez-Salinas,\u0026#32; Cervera-Lierta,\u0026#32; Gil-Fuster\u0026#32;\u0026amp;\u0026#32;Latorre (2019) Pérez-Salinas,\u0026#32; A.,\u0026#32; Cervera-Lierta,\u0026#32; A.,\u0026#32; Gil-Fuster,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Latorre,\u0026#32; J. \u0026#32; (2019). \u0026#32;Data re-uploading for a universal quantum classifier. Quantum 4, 226 (2020). https://doi.org/10.22331/q-2020-02-06-226 Romero,\u0026#32; Olson\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik (2016) Romero,\u0026#32; J.,\u0026#32; Olson,\u0026#32; J.\u0026#32;\u0026amp;\u0026#32;Aspuru-Guzik,\u0026#32; A. \u0026#32; (2016). \u0026#32;Quantum autoencoders for efficient compression of quantum data. Now published in 2017 Quantum Sci. Technol. 2 045001. https://doi.org/10.1088/2058-9565/aa8072 Weigold,\u0026#32; Barzen,\u0026#32; Leymann\u0026#32;\u0026amp;\u0026#32;Salm (2021) Weigold,\u0026#32; M.,\u0026#32; Barzen,\u0026#32; J.,\u0026#32; Leymann,\u0026#32; F.\u0026#32;\u0026amp;\u0026#32;Salm,\u0026#32; M. \u0026#32; (2021). \u0026#32; Expanding Data Encoding Patterns For Quantum Algorithms. https://doi.org/10.1109/ICSA-C52384.2021.00025 ","permalink":"https://tommago.com/posts/qae/","summary":"In my GSoC project, I explore the use of Quantum Autoencoders for the analysis of LHC data. Autoencoders are an unsupervised learning technique, which learns a smaller latent representation of data. The quantum analog of a classical autoencoder equally aims to learn a smaller representation of data.\nA naive Quantum Autoencoder My first idea for a Quantum circuit closely follows the architecture of a classical autoencoder. The structure of the circuit is conceptually sketched in the following figure.","title":"Quantum Autoencoder"},{"content":"This year I\u0026rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 ) . These models are thereby trained on Standard Model data and search for anomalous events that deviate from the known physics. With the rise of NISQ-devices ( Citation: Preskill,\u0026#32;2018 Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ) the question comes up if quantum machine learning can enhance classical machine learning applications to hep problems.\nSince we are encouraged by Google to publicly share our work, I set up this blog to document the project and share some of my scientific interests.\nKasieczka,\u0026#32; Nachman\u0026#32;\u0026amp;\u0026#32;al. (2021) Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics. https://doi.org/10.1088/1361-6633/ac36b9 Preskill (2018) Preskill,\u0026#32; J. \u0026#32; (2018). \u0026#32;Quantum Computing in the NISQ era and beyond. Quantum 2, 79 (2018). https://doi.org/10.22331/q-2018-08-06-79 ","permalink":"https://tommago.com/posts/hello/","summary":"This year I\u0026rsquo;m participating in the Google Summer of Code with the ML4SCI organization. My project proposal deals with a quantum variational autoencoder (QVAE) for the anaysis of particle physics data. Such unsupervised learning paradigms can be used to search for new physics in a model-agnostic way ( Citation: Kasieczka,\u0026#32;Nachman \u0026amp; al.,\u0026#32;2021 Kasieczka,\u0026#32; G.,\u0026#32; Nachman,\u0026#32; B.\u0026#32;\u0026amp;\u0026#32;al. \u0026#32; (2021). \u0026#32;The LHC Olympics 2020: A Community Challenge for Anomaly Detection in High Energy Physics.","title":"Hello World! | Hello GSoC!"},{"content":"A record of our regular wine fuled update talks.\n09.11.24 Tom: Quarkonium Thermalization\nMara: Bakteria Canniblism\nWine: Regional semi-dry red wine\nConclusion: We will stay with dry wines\n","permalink":"https://tommago.com/decanting/","summary":"A record of our regular wine fuled update talks.\n09.11.24 Tom: Quarkonium Thermalization\nMara: Bakteria Canniblism\nWine: Regional semi-dry red wine\nConclusion: We will stay with dry wines","title":"Decanting the Universe"},{"content":"","permalink":"https://tommago.com/news/","summary":"","title":"News"}]